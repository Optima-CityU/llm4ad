[
    {
        "sample_order": 1,
        "algorithm": "{Optimized softmax implementation using warp-level reductions and shared memory for max/exp-sum calculations, with improved thread utilization and reduced synchronization overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__device__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    }\n    return val;\n}\n\n__device__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float s_max[WARP_SIZE];\n    __shared__ float s_sum[WARP_SIZE];\n\n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    \n    thread_max = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        s_max[threadIdx.x / WARP_SIZE] = thread_max;\n    }\n    __syncthreads();\n\n    float max_val = (threadIdx.x < blockDim.x / WARP_SIZE) ? s_max[threadIdx.x] : -FLT_MAX;\n    max_val = warp_reduce_max(max_val);\n\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        s_sum[threadIdx.x / WARP_SIZE] = thread_sum;\n    }\n    __syncthreads();\n\n    float sum_exp = (threadIdx.x < blockDim.x / WARP_SIZE) ? s_sum[threadIdx.x] : 0.0f;\n    sum_exp = warp_reduce_sum(sum_exp);\n\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimized softmax implementation using warp-level reductions and loop unrolling for better parallelism and reduced shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n#define WARP_SIZE 32\n#define UNROLL_FACTOR 4\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float s_max[WARP_SIZE];\n    __shared__ float s_sum[WARP_SIZE];\n    \n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x * UNROLL_FACTOR) {\n        #pragma unroll\n        for (int i = 0; i < UNROLL_FACTOR && j + i * blockDim.x < num_features; ++i) {\n            int idx = j + i * blockDim.x;\n            thread_max = fmaxf(thread_max, input[idx]);\n        }\n    }\n    \n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) s_max[threadIdx.x / WARP_SIZE] = thread_max;\n    __syncthreads();\n    \n    float max_val = (threadIdx.x < blockDim.x / WARP_SIZE) ? s_max[threadIdx.x] : -FLT_MAX;\n    max_val = warpReduceMax(max_val);\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x * UNROLL_FACTOR) {\n        #pragma unroll\n        for (int i = 0; i < UNROLL_FACTOR && j + i * blockDim.x < num_features; ++i) {\n            int idx = j + i * blockDim.x;\n            thread_sum += expf(input[idx] - max_val);\n        }\n    }\n    \n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) s_sum[threadIdx.x / WARP_SIZE] = thread_sum;\n    __syncthreads();\n    \n    float sum_exp = (threadIdx.x < blockDim.x / WARP_SIZE) ? s_sum[threadIdx.x] : 0.0f;\n    sum_exp = warpReduceSum(sum_exp);\n    \n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x * UNROLL_FACTOR) {\n        #pragma unroll\n        for (int i = 0; i < UNROLL_FACTOR && j + i * blockDim.x < num_features; ++i) {\n            int idx = j + i * blockDim.x;\n            output[idx] = expf(input[idx] - max_val) / sum_exp;\n        }\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimized algorithm uses parallel reduction for max and sum computations with improved memory access patterns and loop unrolling for better performance while maintaining numerical stability.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n#define UNROLL_FACTOR 4\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    extern __shared__ float sdata[];\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n\n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x * UNROLL_FACTOR) {\n        #pragma unroll\n        for (int i = 0; i < UNROLL_FACTOR && (j + i * blockDim.x) < num_features; ++i) {\n            thread_max = fmaxf(thread_max, input[j + i * blockDim.x]);\n        }\n    }\n    \n    sdata[threadIdx.x] = thread_max;\n    __syncthreads();\n\n    for (int s = blockDim.x / 2; s > 32; s >>= 1) {\n        if (threadIdx.x < s) {\n            sdata[threadIdx.x] = fmaxf(sdata[threadIdx.x], sdata[threadIdx.x + s]);\n        }\n        __syncthreads();\n    }\n    if (threadIdx.x < 32) {\n        volatile float* vsdata = sdata;\n        vsdata[threadIdx.x] = fmaxf(vsdata[threadIdx.x], vsdata[threadIdx.x + 32]);\n        vsdata[threadIdx.x] = fmaxf(vsdata[threadIdx.x], vsdata[threadIdx.x + 16]);\n        vsdata[threadIdx.x] = fmaxf(vsdata[threadIdx.x], vsdata[threadIdx.x + 8]);\n        vsdata[threadIdx.x] = fmaxf(vsdata[threadIdx.x], vsdata[threadIdx.x + 4]);\n        vsdata[threadIdx.x] = fmaxf(vsdata[threadIdx.x], vsdata[threadIdx.x + 2]);\n        vsdata[threadIdx.x] = fmaxf(vsdata[threadIdx.x], vsdata[threadIdx.x + 1]);\n    }\n    float max_val = sdata[0];\n    __syncthreads();\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x * UNROLL_FACTOR) {\n        #pragma unroll\n        for (int i = 0; i < UNROLL_FACTOR && (j + i * blockDim.x) < num_features; ++i) {\n            thread_sum += expf(input[j + i * blockDim.x] - max_val);\n        }\n    }\n    \n    sdata[threadIdx.x] = thread_sum;\n    __syncthreads();\n\n    for (int s = blockDim.x / 2; s > 32; s >>= 1) {\n        if (threadIdx.x < s) {\n            sdata[threadIdx.x] += sdata[threadIdx.x + s];\n        }\n        __syncthreads();\n    }\n    if (threadIdx.x < 32) {\n        volatile float* vsdata = sdata;\n        vsdata[threadIdx.x] += vsdata[threadIdx.x + 32];\n        vsdata[threadIdx.x] += vsdata[threadIdx.x + 16];\n        vsdata[threadIdx.x] += vsdata[threadIdx.x + 8];\n        vsdata[threadIdx.x] += vsdata[threadIdx.x + 4];\n        vsdata[threadIdx.x] += vsdata[threadIdx.x + 2];\n        vsdata[threadIdx.x] += vsdata[threadIdx.x + 1];\n    }\n    float sum_exp = sdata[0];\n    __syncthreads();\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x * UNROLL_FACTOR) {\n        #pragma unroll\n        for (int i = 0; i < UNROLL_FACTOR && (j + i * blockDim.x) < num_features; ++i) {\n            output[j + i * blockDim.x] = expf(input[j + i * blockDim.x] - max_val) / sum_exp;\n        }\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + UNROLL_FACTOR - 1) / UNROLL_FACTOR));\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads, threads * sizeof(float)>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimized implementation using parallel reduction for max and sum computations, and coalesced memory access for improved performance while maintaining numerical stability.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n#define BLOCK_SIZE 256\n\ntemplate <unsigned blockSize>\n__device__ void warpReduceMax(volatile float* sdata, int tid) {\n    if (blockSize >= 64) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 32]);\n    if (blockSize >= 32) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 16]);\n    if (blockSize >= 16) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 8]);\n    if (blockSize >= 8) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 4]);\n    if (blockSize >= 4) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 2]);\n    if (blockSize >= 2) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 1]);\n}\n\ntemplate <unsigned blockSize>\n__device__ void warpReduceSum(volatile float* sdata, int tid) {\n    if (blockSize >= 64) sdata[tid] += sdata[tid + 32];\n    if (blockSize >= 32) sdata[tid] += sdata[tid + 16];\n    if (blockSize >= 16) sdata[tid] += sdata[tid + 8];\n    if (blockSize >= 8) sdata[tid] += sdata[tid + 4];\n    if (blockSize >= 4) sdata[tid] += sdata[tid + 2];\n    if (blockSize >= 2) sdata[tid] += sdata[tid + 1];\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    extern __shared__ float sdata[];\n    int row = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    const float* row_input = input + row * num_features;\n    float* row_output = output + row * num_features;\n\n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < num_features; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, row_input[i]);\n    }\n    sdata[tid] = thread_max;\n    __syncthreads();\n\n    if (blockDim.x >= 1024) { if (tid < 512) { sdata[tid] = fmaxf(sdata[tid], sdata[tid + 512]); } __syncthreads(); }\n    if (blockDim.x >= 512) { if (tid < 256) { sdata[tid] = fmaxf(sdata[tid], sdata[tid + 256]); } __syncthreads(); }\n    if (blockDim.x >= 256) { if (tid < 128) { sdata[tid] = fmaxf(sdata[tid], sdata[tid + 128]); } __syncthreads(); }\n    if (blockDim.x >= 128) { if (tid < 64) { sdata[tid] = fmaxf(sdata[tid], sdata[tid + 64]); } __syncthreads(); }\n    if (tid < 32) warpReduceMax<BLOCK_SIZE>(sdata, tid);\n    __syncthreads();\n    float max_val = sdata[0];\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int i = tid; i < num_features; i += blockDim.x) {\n        thread_sum += expf(row_input[i] - max_val);\n    }\n    sdata[tid] = thread_sum;\n    __syncthreads();\n\n    if (blockDim.x >= 1024) { if (tid < 512) { sdata[tid] += sdata[tid + 512]; } __syncthreads(); }\n    if (blockDim.x >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }\n    if (blockDim.x >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }\n    if (blockDim.x >= 128) { if (tid < 64) { sdata[tid] += sdata[tid + 64]; } __syncthreads(); }\n    if (tid < 32) warpReduceSum<BLOCK_SIZE>(sdata, tid);\n    __syncthreads();\n    float sum_exp = sdata[0];\n\n    // Step 3: Compute softmax\n    for (int i = tid; i < num_features; i += blockDim.x) {\n        row_output[i] = expf(row_input[i] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    dim3 blocks(batch_size);\n    dim3 threads(BLOCK_SIZE);\n    \n    softmax_kernel<<<blocks, threads, BLOCK_SIZE * sizeof(float)>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{The optimized kernel uses warp-level reductions for max and sum calculations, processes elements in a coalesced manner, and minimizes shared memory usage by leveraging warp shuffle instructions.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value in the row\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if ((threadIdx.x & 0x1F) == 0) {\n        __shared__ float smax_val;\n        if (threadIdx.x == 0) smax_val = -FLT_MAX;\n        __syncthreads();\n        atomicMaxf(&smax_val, thread_max);\n        __syncthreads();\n        thread_max = smax_val;\n    }\n    thread_max = __shfl_sync(0xFFFFFFFF, thread_max, 0);\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - thread_max);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if ((threadIdx.x & 0x1F) == 0) {\n        __shared__ float ssum_val;\n        if (threadIdx.x == 0) ssum_val = 0.0f;\n        __syncthreads();\n        atomicAdd(&ssum_val, thread_sum);\n        __syncthreads();\n        thread_sum = ssum_val;\n    }\n    thread_sum = __shfl_sync(0xFFFFFFFF, thread_sum, 0);\n    \n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - thread_max) / thread_sum;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimized softmax implementation using parallel reduction for max and sum computations, with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    // Each block processes one row\n    int row = blockIdx.x;\n    const float* row_input = input + row * num_features;\n    float* row_output = output + row * num_features;\n\n    // Step 1: Find max value in the row\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, row_input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if ((threadIdx.x & 0x1F) == 0) {\n        __shared__ float smax_val;\n        if (threadIdx.x == 0) smax_val = -FLT_MAX;\n        __syncthreads();\n        atomicMaxf(&smax_val, thread_max);\n        __syncthreads();\n        thread_max = smax_val;\n    }\n    float max_val = warpReduceMax(thread_max);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(row_input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if ((threadIdx.x & 0x1F) == 0) {\n        __shared__ float ssum_val;\n        if (threadIdx.x == 0) ssum_val = 0.0f;\n        __syncthreads();\n        atomicAdd(&ssum_val, thread_sum);\n        __syncthreads();\n        thread_sum = ssum_val;\n    }\n    float sum_exp = warpReduceSum(thread_sum);\n\n    // Step 3: Compute softmax values\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        row_output[j] = expf(row_input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    // Use 256 threads per block (optimal for RTX 4090)\n    const int threads = 256;\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimized softmax implementation using parallel reduction for max and sum computations, with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    extern __shared__ float sdata[];\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[i]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) {\n        sdata[threadIdx.x / 32] = thread_max;\n    }\n    __syncthreads();\n    \n    float max_val = (threadIdx.x < 32) ? warpReduceMax(sdata[threadIdx.x]) : 0.0f;\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        thread_sum += expf(input[i] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) {\n        sdata[threadIdx.x / 32] = thread_sum;\n    }\n    __syncthreads();\n    \n    float sum_exp = (threadIdx.x < 32) ? warpReduceSum(sdata[threadIdx.x]) : 0.0f;\n    \n    // Step 3: Compute softmax\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        output[i] = expf(input[i] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(512, num_features);\n    int blocks = batch_size;\n    int smem_size = ((threads + 31) / 32) * sizeof(float);\n    \n    softmax_kernel<<<blocks, threads, smem_size>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimized algorithm using warp-level reductions for max and sum calculations, and unrolled loops for better parallelism and reduced shared memory usage.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__device__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float s_max[WARP_SIZE];\n    __shared__ float s_sum[WARP_SIZE];\n\n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        s_max[threadIdx.x / WARP_SIZE] = thread_max;\n    }\n    __syncthreads();\n\n    float max_val = threadIdx.x < blockDim.x / WARP_SIZE ? s_max[threadIdx.x] : -FLT_MAX;\n    max_val = warp_reduce_max(max_val);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        s_sum[threadIdx.x / WARP_SIZE] = thread_sum;\n    }\n    __syncthreads();\n\n    float sum_exp = threadIdx.x < blockDim.x / WARP_SIZE ? s_sum[threadIdx.x] : 0.0f;\n    sum_exp = warp_reduce_sum(sum_exp);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimized algorithm: Process each row in parallel with warp-level reductions for max and sum, followed by warp-wide broadcasts to compute softmax values, minimizing shared memory usage and synchronization overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__device__ __inline__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ __inline__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Compute max per warp\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    float max_val = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicMax_block((int*)&max_val, __float_as_int(max_val));\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n\n    // Step 2: Compute sum per warp\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    float sum_exp = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicAdd_block(&sum_exp, sum_exp);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimized algorithm: Process each row in parallel with warp-level reductions for max and sum, followed by coalesced memory access for exp and division operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float s_max[32];\n    __shared__ float s_sum[32];\n    \n    // Step 1: Find max value in row\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) s_max[threadIdx.x / 32] = thread_max;\n    __syncthreads();\n    \n    float max_val = (threadIdx.x < 32) ? s_max[threadIdx.x] : -FLT_MAX;\n    max_val = warpReduceMax(max_val);\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) s_sum[threadIdx.x / 32] = thread_sum;\n    __syncthreads();\n    \n    float sum_exp = (threadIdx.x < 32) ? s_sum[threadIdx.x] : 0.0f;\n    sum_exp = warpReduceSum(sum_exp);\n    \n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimized kernel uses warp-level reductions for max and sum operations, reduces shared memory usage, and employs loop unrolling for better efficiency.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n#define WARP_SIZE 32\n\n__inline__ __device__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float sdata[WARP_SIZE];\n\n    // Step 1: Compute max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        sdata[threadIdx.x / WARP_SIZE] = thread_max;\n    }\n    __syncthreads();\n    float max_val = threadIdx.x < blockDim.x / WARP_SIZE ? sdata[threadIdx.x] : -FLT_MAX;\n    max_val = warp_reduce_max(max_val);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        sdata[threadIdx.x / WARP_SIZE] = thread_sum;\n    }\n    __syncthreads();\n    float sum_exp = threadIdx.x < blockDim.x / WARP_SIZE ? sdata[threadIdx.x] : 0.0f;\n    sum_exp = warp_reduce_sum(sum_exp);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimized implementation using parallel reduction for max and sum calculations with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <unsigned blockSize>\n__device__ void warpReduceMax(volatile float* sdata, int tid) {\n    if (blockSize >= 64) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 32]);\n    if (blockSize >= 32) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 16]);\n    if (blockSize >= 16) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 8]);\n    if (blockSize >= 8) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 4]);\n    if (blockSize >= 4) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 2]);\n    if (blockSize >= 2) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 1]);\n}\n\ntemplate <unsigned blockSize>\n__device__ void warpReduceSum(volatile float* sdata, int tid) {\n    if (blockSize >= 64) sdata[tid] += sdata[tid + 32];\n    if (blockSize >= 32) sdata[tid] += sdata[tid + 16];\n    if (blockSize >= 16) sdata[tid] += sdata[tid + 8];\n    if (blockSize >= 8) sdata[tid] += sdata[tid + 4];\n    if (blockSize >= 4) sdata[tid] += sdata[tid + 2];\n    if (blockSize >= 2) sdata[tid] += sdata[tid + 1];\n}\n\ntemplate <unsigned blockSize>\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    extern __shared__ float sdata[];\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    int tid = threadIdx.x;\n    float thread_max = -FLT_MAX;\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    sdata[tid] = thread_max;\n    __syncthreads();\n\n    if (blockSize >= 512) { if (tid < 256) { sdata[tid] = fmaxf(sdata[tid], sdata[tid + 256]); } __syncthreads(); }\n    if (blockSize >= 256) { if (tid < 128) { sdata[tid] = fmaxf(sdata[tid], sdata[tid + 128]); } __syncthreads(); }\n    if (blockSize >= 128) { if (tid < 64) { sdata[tid] = fmaxf(sdata[tid], sdata[tid + 64]); } __syncthreads(); }\n    if (tid < 32) warpReduceMax<blockSize>(sdata, tid);\n    float max_val = sdata[0];\n    __syncthreads();\n\n    float thread_sum = 0.0f;\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    sdata[tid] = thread_sum;\n    __syncthreads();\n\n    if (blockSize >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }\n    if (blockSize >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }\n    if (blockSize >= 128) { if (tid < 64) { sdata[tid] += sdata[tid + 64]; } __syncthreads(); }\n    if (tid < 32) warpReduceSum<blockSize>(sdata, tid);\n    float sum_exp = sdata[0];\n    __syncthreads();\n\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(512, num_features);\n    int blocks = batch_size;\n    size_t shmem_size = threads * sizeof(float);\n\n    switch (threads) {\n        case 512: softmax_kernel<512><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 256: softmax_kernel<256><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 128: softmax_kernel<128><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 64: softmax_kernel<64><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 32: softmax_kernel<32><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 16: softmax_kernel<16><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 8: softmax_kernel<8><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 4: softmax_kernel<4><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 2: softmax_kernel<2><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 1: softmax_kernel<1><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        default: softmax_kernel<512><<<blocks, threads, shmem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    }\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimized softmax implementation using warp-level reductions and vectorized memory access to minimize shared memory usage and bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value in row\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) {\n        __shared__ float smax;\n        if (threadIdx.x == 0) smax = -FLT_MAX;\n        __syncthreads();\n        atomicMaxFloat(&smax, thread_max);\n        __syncthreads();\n        thread_max = smax;\n    }\n    thread_max = __shfl_sync(0xFFFFFFFF, thread_max, 0);\n    \n    // Step 2: Compute sum of exp(x - max)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - thread_max);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) {\n        __shared__ float ssum;\n        if (threadIdx.x == 0) ssum = 0.0f;\n        __syncthreads();\n        atomicAdd(&ssum, thread_sum);\n        __syncthreads();\n        thread_sum = ssum;\n    }\n    thread_sum = __shfl_sync(0xFFFFFFFF, thread_sum, 0);\n    \n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - thread_max) / thread_sum;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimized algorithm: Process each row in parallel with warp-level reductions for max and sum, followed by coalesced memory access for the final softmax computation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n    \n    // Process elements with stride of blockDim.x * gridDim.x\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[i]);\n    }\n    \n    // Warp-level max reduction\n    float max_val = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) {\n        ((volatile float*)&thread_max) = max_val;\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n    \n    // Compute sum of exp(x - max_val)\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        thread_sum += expf(input[i] - max_val);\n    }\n    \n    // Warp-level sum reduction\n    float sum_exp = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) {\n        ((volatile float*)&thread_sum) = sum_exp;\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n    \n    // Write results\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        output[i] = expf(input[i] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimized softmax kernel using warp-level reductions and shared memory to minimize synchronization overhead while maintaining numerical stability.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    }\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float s_max;\n    __shared__ float s_sum;\n\n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    \n    float max_val = warpReduceMax(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicMaxFloat(&s_max, max_val);\n    }\n    __syncthreads();\n    max_val = s_max;\n\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    \n    float sum_val = warpReduceSum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicAdd(&s_sum, sum_val);\n    }\n    __syncthreads();\n    float sum_exp = s_sum;\n\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\n__device__ void atomicMaxFloat(float* address, float val) {\n    int* address_as_i = (int*) address;\n    int old = *address_as_i, assumed;\n    do {\n        assumed = old;\n        old = atomicCAS(address_as_i, assumed, __float_as_int(fmaxf(val, __int_as_float(assumed))));\n    } while (assumed != old);\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads, 2 * sizeof(float)>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimized softmax implementation using warp-level reductions and vectorized memory access to minimize shared memory usage and maximize parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    const int row = blockIdx.x;\n    const int tid = threadIdx.x;\n    const int lane_id = tid % 32;\n    const int warp_id = tid / 32;\n\n    input += row * num_features;\n    output += row * num_features;\n\n    // Step 1: Find max value in row\n    float thread_max = -FLT_MAX;\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    float max_val = warpReduceMax(thread_max);\n    if (lane_id == 0) {\n        __shared__ float smax_val;\n        if (warp_id == 0) smax_val = -FLT_MAX;\n        __syncthreads();\n        atomicMaxFloat(&smax_val, max_val);\n        __syncthreads();\n        max_val = smax_val;\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    float sum_exp = warpReduceSum(thread_sum);\n    if (lane_id == 0) {\n        __shared__ float ssum;\n        if (warp_id == 0) ssum = 0.0f;\n        __syncthreads();\n        atomicAdd(&ssum, sum_exp);\n        __syncthreads();\n        sum_exp = ssum;\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n\n    // Step 3: Compute softmax\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\n__device__ void atomicMaxFloat(float* address, float val) {\n    int* address_as_i = (int*) address;\n    int old = *address_as_i, assumed;\n    do {\n        assumed = old;\n        old = atomicCAS(address_as_i, assumed, __float_as_int(fmaxf(val, __int_as_float(assumed))));\n    } while (assumed != old);\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{The optimized kernel uses parallel reduction for max and sum calculations, processes elements in a coalesced manner, and employs warp-level primitives for efficient intra-warp reductions.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n\n    // Step 1: Compute max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if ((threadIdx.x % 32) == 0) {\n        atomicMax_block((int*)&thread_max, __float_as_int(thread_max));\n    }\n    __syncthreads();\n    float max_val = thread_max;\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if ((threadIdx.x % 32) == 0) {\n        atomicAdd_block(&thread_sum, thread_sum);\n    }\n    __syncthreads();\n    float sum_exp = thread_sum;\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimized softmax implementation using parallel reduction for max and sum computations, with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    extern __shared__ float sdata[];\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n\n    // Step 1: Compute max per thread and reduce\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    \n    float max_val = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) sdata[threadIdx.x / 32] = max_val;\n    __syncthreads();\n    \n    if (threadIdx.x < 32) max_val = (threadIdx.x < blockDim.x / 32) ? sdata[threadIdx.x] : -FLT_MAX;\n    max_val = warpReduceMax(max_val);\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    \n    float sum_exp = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) sdata[threadIdx.x / 32] = sum_exp;\n    __syncthreads();\n    \n    if (threadIdx.x < 32) sum_exp = (threadIdx.x < blockDim.x / 32) ? sdata[threadIdx.x] : 0.0f;\n    sum_exp = warpReduceSum(sum_exp);\n    \n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    int smem_size = ((threads + 31) / 32) * sizeof(float);\n\n    softmax_kernel<<<blocks, threads, smem_size>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimized softmax kernel using warp-level reductions and loop unrolling for better parallelism and reduced shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__device__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float s_max[WARP_SIZE];\n    __shared__ float s_sum[WARP_SIZE];\n\n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        s_max[threadIdx.x / WARP_SIZE] = thread_max;\n    }\n    __syncthreads();\n    \n    float max_val = (threadIdx.x < blockDim.x / WARP_SIZE) ? s_max[threadIdx.x] : -FLT_MAX;\n    max_val = warp_reduce_max(max_val);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        s_sum[threadIdx.x / WARP_SIZE] = thread_sum;\n    }\n    __syncthreads();\n    \n    float sum_exp = (threadIdx.x < blockDim.x / WARP_SIZE) ? s_sum[threadIdx.x] : 0.0f;\n    sum_exp = warp_reduce_sum(sum_exp);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimized implementation using parallel reduction for max and sum computations, warp-level primitives for efficient intra-warp reduction, and improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    extern __shared__ float sdata[];\n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n\n    // First pass: compute max and sum with better memory access pattern\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        float val = input[j];\n        thread_max = fmaxf(thread_max, val);\n    }\n\n    // Warp-level reduction for max\n    float max_val = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) {\n        sdata[threadIdx.x / 32] = max_val;\n    }\n    __syncthreads();\n\n    // Block-level reduction for max\n    if (threadIdx.x < 32) {\n        max_val = (threadIdx.x < blockDim.x / 32) ? sdata[threadIdx.x] : -FLT_MAX;\n        max_val = warpReduceMax(max_val);\n    }\n    __syncthreads();\n\n    // Second pass: compute sum\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n\n    // Warp-level reduction for sum\n    float sum_val = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) {\n        sdata[threadIdx.x / 32] = sum_val;\n    }\n    __syncthreads();\n\n    // Block-level reduction for sum\n    if (threadIdx.x < 32) {\n        sum_val = (threadIdx.x < blockDim.x / 32) ? sdata[threadIdx.x] : 0.0f;\n        sum_val = warpReduceSum(sum_val);\n    }\n    __syncthreads();\n\n    // Final pass: compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_val;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    int smem_size = ((threads + 31) / 32) * sizeof(float);\n\n    softmax_kernel<<<blocks, threads, smem_size>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimized softmax implementation using warp-level reductions for max and sum computations, and unrolling the final division loop for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Compute max per warp then across warps\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if ((threadIdx.x % WARP_SIZE) == 0) {\n        atomicMax_block(&thread_max);\n    }\n    __syncthreads();\n    float max_val = thread_max;\n\n    // Step 2: Compute sum per warp then across warps\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if ((threadIdx.x % WARP_SIZE) == 0) {\n        atomicAdd_block(&thread_sum);\n    }\n    __syncthreads();\n    float sum_exp = thread_sum;\n\n    // Step 3: Compute softmax with unrolled writes\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the softmax computation by using warp-level reductions for max and sum operations, reducing shared memory usage and synchronization overhead, while maintaining numerical stability.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Compute max per warp then across warps\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    float max_val = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) {\n        atomicMax_block((int*)&max_val, __float_as_int(max_val));\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    float sum_exp = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) {\n        atomicAdd_block(&sum_exp, sum_exp);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimized softmax implementation using warp-level reductions and vectorized memory access to minimize shared memory usage and maximize parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value in row using warp reductions\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicMax_block((int*)&thread_max, __float_as_int(thread_max));\n    }\n    __syncthreads();\n    float max_val = thread_max;\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicAdd_block(&thread_sum, thread_sum);\n    }\n    __syncthreads();\n    float sum_exp = thread_sum;\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimized softmax kernel using warp-level reductions for max and sum computations, and unrolling the final softmax calculation for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__device__ __forceinline__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n    \n    // Compute max and sum in one pass\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        float val = input[j];\n        thread_max = fmaxf(thread_max, val);\n    }\n    \n    // Warp-level reduction for max\n    float max_val = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicMax_block((int*)&max_val, __float_as_int(max_val));\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n    \n    // Compute sum of exp(x - max_val)\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    \n    // Warp-level reduction for sum\n    float sum_exp = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicAdd_block(&sum_exp, sum_exp);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n    \n    // Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimized softmax kernel using warp-level reductions for max and sum computations, and unrolling the final write loop for better memory efficiency.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Compute max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicMax_block((int*)&thread_max, __float_as_int(thread_max));\n    }\n    __syncthreads();\n    float max_val = __int_as_float(*((int*)&thread_max));\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicAdd_block(&thread_sum, thread_sum);\n    }\n    __syncthreads();\n    float sum_exp = thread_sum;\n\n    // Step 3: Compute softmax output\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimized algorithm uses warp-level reductions for max and sum calculations, and unrolls the final softmax computation for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <unsigned blockSize>\n__device__ void warpReduceMax(volatile float* sdata, unsigned int tid) {\n    if (blockSize >= 64) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 32]);\n    if (blockSize >= 32) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 16]);\n    if (blockSize >= 16) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 8]);\n    if (blockSize >= 8) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 4]);\n    if (blockSize >= 4) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 2]);\n    if (blockSize >= 2) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 1]);\n}\n\ntemplate <unsigned blockSize>\n__device__ void warpReduceSum(volatile float* sdata, unsigned int tid) {\n    if (blockSize >= 64) sdata[tid] += sdata[tid + 32];\n    if (blockSize >= 32) sdata[tid] += sdata[tid + 16];\n    if (blockSize >= 16) sdata[tid] += sdata[tid + 8];\n    if (blockSize >= 8) sdata[tid] += sdata[tid + 4];\n    if (blockSize >= 4) sdata[tid] += sdata[tid + 2];\n    if (blockSize >= 2) sdata[tid] += sdata[tid + 1];\n}\n\ntemplate <unsigned blockSize>\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    extern __shared__ float sdata[];\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n\n    // Step 1: Compute max\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    sdata[threadIdx.x] = thread_max;\n    __syncthreads();\n\n    if (blockSize >= 512) {\n        if (threadIdx.x < 256) sdata[threadIdx.x] = fmaxf(sdata[threadIdx.x], sdata[threadIdx.x + 256]);\n        __syncthreads();\n    }\n    if (blockSize >= 256) {\n        if (threadIdx.x < 128) sdata[threadIdx.x] = fmaxf(sdata[threadIdx.x], sdata[threadIdx.x + 128]);\n        __syncthreads();\n    }\n    if (blockSize >= 128) {\n        if (threadIdx.x < 64) sdata[threadIdx.x] = fmaxf(sdata[threadIdx.x], sdata[threadIdx.x + 64]);\n        __syncthreads();\n    }\n    if (threadIdx.x < 32) warpReduceMax<blockSize>(sdata, threadIdx.x);\n    __syncthreads();\n    float max_val = sdata[0];\n\n    // Step 2: Compute sum\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    sdata[threadIdx.x] = thread_sum;\n    __syncthreads();\n\n    if (blockSize >= 512) {\n        if (threadIdx.x < 256) sdata[threadIdx.x] += sdata[threadIdx.x + 256];\n        __syncthreads();\n    }\n    if (blockSize >= 256) {\n        if (threadIdx.x < 128) sdata[threadIdx.x] += sdata[threadIdx.x + 128];\n        __syncthreads();\n    }\n    if (blockSize >= 128) {\n        if (threadIdx.x < 64) sdata[threadIdx.x] += sdata[threadIdx.x + 64];\n        __syncthreads();\n    }\n    if (threadIdx.x < 32) warpReduceSum<blockSize>(sdata, threadIdx.x);\n    __syncthreads();\n    float sum_exp = sdata[0];\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(512, num_features);\n    int blocks = batch_size;\n\n    switch (threads) {\n        case 512: softmax_kernel<512><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 256: softmax_kernel<256><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 128: softmax_kernel<128><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 64: softmax_kernel<64><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 32: softmax_kernel<32><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 16: softmax_kernel<16><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 8: softmax_kernel<8><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 4: softmax_kernel<4><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 2: softmax_kernel<2><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 1: softmax_kernel<1><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        default: softmax_kernel<256><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    }\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimized softmax kernel using parallel reduction for max and sum computations with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    const int row = blockIdx.x;\n    const int tid = threadIdx.x;\n    const int warp_id = tid / 32;\n    const int lane_id = tid % 32;\n    \n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float s_max[32];\n    __shared__ float s_sum[32];\n\n    // Step 1: Find max value in the row\n    float thread_max = -FLT_MAX;\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (lane_id == 0) s_max[warp_id] = thread_max;\n    __syncthreads();\n    \n    float max_val = lane_id < blockDim.x / 32 ? s_max[lane_id] : -FLT_MAX;\n    max_val = warpReduceMax(max_val);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (lane_id == 0) s_sum[warp_id] = thread_sum;\n    __syncthreads();\n    \n    float sum_exp = lane_id < blockDim.x / 32 ? s_sum[lane_id] : 0.0f;\n    sum_exp = warpReduceSum(sum_exp);\n\n    // Step 3: Compute softmax\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimized softmax implementation using warp-level reductions for max and sum computations, and improved memory access patterns with vectorized loads.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T, int num_threads>\n__device__ void warp_reduce_max(T& val) {\n    for (int offset = 16; offset > 0; offset /= 2) {\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    }\n}\n\ntemplate <typename T, int num_threads>\n__device__ void warp_reduce_sum(T& val) {\n    for (int offset = 16; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n}\n\ntemplate <int num_threads>\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    const int row = blockIdx.x;\n    const int tid = threadIdx.x;\n    const int lane_id = tid % 32;\n    const int warp_id = tid / 32;\n    \n    input  += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float smem_max[32];\n    __shared__ float smem_sum[32];\n    \n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += num_threads) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    warp_reduce_max<float, num_threads>(thread_max);\n    if (lane_id == 0) smem_max[warp_id] = thread_max;\n    __syncthreads();\n    \n    float max_val = (tid < 32) ? smem_max[lane_id] : -FLT_MAX;\n    if (tid < 32) warp_reduce_max<float, 32>(max_val);\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += num_threads) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    warp_reduce_sum<float, num_threads>(thread_sum);\n    if (lane_id == 0) smem_sum[warp_id] = thread_sum;\n    __syncthreads();\n    \n    float sum_exp = (tid < 32) ? smem_sum[lane_id] : 0.0f;\n    if (tid < 32) warp_reduce_sum<float, 32>(sum_exp);\n    \n    // Step 3: Compute softmax\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += num_threads) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    const int threads = 256;\n    int blocks = batch_size;\n    \n    softmax_kernel<threads><<<blocks, threads>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimized softmax implementation using parallel reduction for max and sum computations, with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    extern __shared__ float sdata[];\n    \n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value in row\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    \n    float max_val = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) {\n        sdata[threadIdx.x / 32] = max_val;\n    }\n    __syncthreads();\n    \n    if (threadIdx.x < 32) {\n        max_val = (threadIdx.x < blockDim.x / 32) ? sdata[threadIdx.x] : -FLT_MAX;\n        max_val = warpReduceMax(max_val);\n    }\n    __syncthreads();\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    \n    float sum_exp = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) {\n        sdata[threadIdx.x / 32] = sum_exp;\n    }\n    __syncthreads();\n    \n    if (threadIdx.x < 32) {\n        sum_exp = (threadIdx.x < blockDim.x / 32) ? sdata[threadIdx.x] : 0.0f;\n        sum_exp = warpReduceSum(sum_exp);\n    }\n    __syncthreads();\n    \n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    int smem_size = ((threads + 31) / 32) * sizeof(float);\n    \n    softmax_kernel<<<blocks, threads, smem_size>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimized softmax implementation using warp-level reductions for max and sum calculations, and unrolling the final softmax computation for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__device__ __forceinline__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Compute max\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicMax_block((int*)&thread_max, __float_as_int(thread_max));\n    }\n    __syncthreads();\n    float max_val = __int_as_float(((int*)&thread_max)[0]);\n\n    // Step 2: Compute sum\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicAdd_block(&thread_sum, thread_sum);\n    }\n    __syncthreads();\n    float sum_exp = thread_sum;\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE;\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimized implementation using warp-level reductions for max and sum calculations, and unrolled loops for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    const int row = blockIdx.x;\n    const int tid = threadIdx.x;\n    \n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value in the row\n    float thread_max = -FLT_MAX;\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    \n    float max_val = warpReduceMax(thread_max);\n    if ((tid & 31) == 0) {\n        __shared__ float smax_val;\n        if (tid == 0) smax_val = max_val;\n        __syncthreads();\n        max_val = smax_val;\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    \n    float sum_exp = warpReduceSum(thread_sum);\n    if ((tid & 31) == 0) {\n        __shared__ float ssum_exp;\n        if (tid == 0) ssum_exp = sum_exp;\n        __syncthreads();\n        sum_exp = ssum_exp;\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n    \n    // Step 3: Compute softmax output\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimized softmax implementation using parallel reduction for max and sum computations, with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    extern __shared__ float sdata[];\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max in the row\n    float thread_max = -FLT_MAX;\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[i]);\n    }\n    \n    float max_val = warpReduceMax(thread_max);\n    if ((threadIdx.x & 0x1F) == 0) sdata[threadIdx.x >> 5] = max_val;\n    __syncthreads();\n    \n    if (threadIdx.x < 32) {\n        max_val = (threadIdx.x < (blockDim.x >> 5)) ? sdata[threadIdx.x] : -FLT_MAX;\n        max_val = warpReduceMax(max_val);\n    }\n    __syncthreads();\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        thread_sum += expf(input[i] - max_val);\n    }\n    \n    float sum_exp = warpReduceSum(thread_sum);\n    if ((threadIdx.x & 0x1F) == 0) sdata[threadIdx.x >> 5] = sum_exp;\n    __syncthreads();\n    \n    if (threadIdx.x < 32) {\n        sum_exp = (threadIdx.x < (blockDim.x >> 5)) ? sdata[threadIdx.x] : 0.0f;\n        sum_exp = warpReduceSum(sum_exp);\n    }\n    __syncthreads();\n    \n    // Step 3: Compute softmax\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        output[i] = expf(input[i] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    int smem_size = ((threads + 31) / 32) * sizeof(float);\n    \n    softmax_kernel<<<blocks, threads, smem_size>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimized softmax implementation using warp-level reductions and vectorized memory access to minimize shared memory usage and maximize parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n#include <cub/cub.cuh>\n\ntemplate <int BLOCK_SIZE>\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    const int row = blockIdx.x;\n    const int tid = threadIdx.x;\n    \n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value in the row\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < num_features; i += BLOCK_SIZE) {\n        thread_max = fmaxf(thread_max, input[i]);\n    }\n    \n    typedef cub::BlockReduce<float, BLOCK_SIZE> BlockReduce;\n    __shared__ typename BlockReduce::TempStorage temp_storage;\n    float max_val = BlockReduce(temp_storage).Reduce(thread_max, cub::Max());\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int i = tid; i < num_features; i += BLOCK_SIZE) {\n        thread_sum += expf(input[i] - max_val);\n    }\n    float sum_exp = BlockReduce(temp_storage).Sum(thread_sum);\n    \n    // Step 3: Compute softmax\n    for (int i = tid; i < num_features; i += BLOCK_SIZE) {\n        output[i] = expf(input[i] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    const int BLOCK_SIZE = 256;\n    dim3 blocks(batch_size);\n    dim3 threads(BLOCK_SIZE);\n    \n    softmax_kernel<BLOCK_SIZE><<<blocks, threads>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimized softmax implementation using parallel reduction for max and sum computations with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <int BLOCK_SIZE>\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float sdata[BLOCK_SIZE];\n\n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += BLOCK_SIZE) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    \n    if (threadIdx.x % 32 == 0) {\n        sdata[threadIdx.x / 32] = thread_max;\n    }\n    __syncthreads();\n    \n    float max_val = thread_max;\n    if (threadIdx.x < 32) {\n        max_val = sdata[threadIdx.x];\n        max_val = warpReduceMax(max_val);\n    }\n    max_val = __shfl_sync(0xffffffff, max_val, 0);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += BLOCK_SIZE) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    \n    if (threadIdx.x % 32 == 0) {\n        sdata[threadIdx.x / 32] = thread_sum;\n    }\n    __syncthreads();\n    \n    float sum_exp = thread_sum;\n    if (threadIdx.x < 32) {\n        sum_exp = sdata[threadIdx.x];\n        sum_exp = warpReduceSum(sum_exp);\n    }\n    sum_exp = __shfl_sync(0xffffffff, sum_exp, 0);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += BLOCK_SIZE) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\n__device__ __forceinline__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    return val;\n}\n\n__device__ __forceinline__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    const int BLOCK_SIZE = 256;\n    dim3 blocks(batch_size);\n    dim3 threads(BLOCK_SIZE);\n\n    softmax_kernel<BLOCK_SIZE><<<blocks, threads>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimized algorithm: Process each row in parallel with warp-level reductions for max and sum computations, followed by warp-level broadcasts to avoid shared memory bank conflicts and improve memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n__inline__ __device__ float warp_reduce_max(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    float max_val = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        max_val = fmaxf(max_val, input[j]);\n    }\n    max_val = warp_reduce_max(max_val);\n    max_val = fmaxf(max_val, __shfl_sync(0xffffffff, max_val, 0));\n\n    float sum_exp = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        sum_exp += expf(input[j] - max_val);\n    }\n    sum_exp = warp_reduce_sum(sum_exp);\n    sum_exp = __shfl_sync(0xffffffff, sum_exp, 0);\n\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimized softmax kernel using warp-level reductions and vectorized memory access to minimize shared memory usage and maximize parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Compute max per warp\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicMax_block((int*)&thread_max, __float_as_int(thread_max));\n    }\n    __syncthreads();\n    float max_val = __int_as_float(*((int*)&thread_max));\n\n    // Step 2: Compute sum per warp\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicAdd_block(&thread_sum, thread_sum);\n    }\n    __syncthreads();\n    float sum_exp = thread_sum;\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(1024, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimized softmax kernel using warp-level reductions and loop unrolling for better parallelism and reduced shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n#define WARP_SIZE 32\n\n__inline__ __device__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n\n    // Step 1: Find max value in row\n    float thread_max = -FLT_MAX;\n    #pragma unroll 4\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    float max_val = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        __shared__ float smem_max;\n        if (threadIdx.x == 0) smem_max = -FLT_MAX;\n        __syncthreads();\n        atomicMaxf(&smem_max, max_val);\n        __syncthreads();\n        max_val = smem_max;\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    #pragma unroll 4\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    float sum_exp = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        __shared__ float smem_sum;\n        if (threadIdx.x == 0) smem_sum = 0.0f;\n        __syncthreads();\n        atomicAdd(&smem_sum, sum_exp);\n        __syncthreads();\n        sum_exp = smem_sum;\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n\n    // Step 3: Compute softmax\n    #pragma unroll 4\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimized softmax implementation using warp-level reductions and shared memory to minimize thread divergence and maximize memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__device__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float s_max[WARP_SIZE];\n    __shared__ float s_sum[WARP_SIZE];\n\n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        s_max[threadIdx.x / WARP_SIZE] = thread_max;\n    }\n    __syncthreads();\n\n    float max_val = (threadIdx.x < blockDim.x / WARP_SIZE) ? s_max[threadIdx.x] : -FLT_MAX;\n    max_val = warp_reduce_max(max_val);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        s_sum[threadIdx.x / WARP_SIZE] = thread_sum;\n    }\n    __syncthreads();\n\n    float sum_exp = (threadIdx.x < blockDim.x / WARP_SIZE) ? s_sum[threadIdx.x] : 0.0f;\n    sum_exp = warp_reduce_sum(sum_exp);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimized softmax implementation using parallel reduction for max and sum computations with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    extern __shared__ float sdata[];\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value in the row\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if ((threadIdx.x & 0x1f) == 0) sdata[threadIdx.x >> 5] = thread_max;\n    __syncthreads();\n    \n    float max_val = (threadIdx.x < (blockDim.x + 31) / 32) ? sdata[threadIdx.x] : -FLT_MAX;\n    if ((blockDim.x + 31) / 32 > 1) {\n        max_val = warpReduceMax(max_val);\n        if (threadIdx.x == 0) sdata[0] = max_val;\n        __syncthreads();\n        max_val = sdata[0];\n    }\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if ((threadIdx.x & 0x1f) == 0) sdata[threadIdx.x >> 5] = thread_sum;\n    __syncthreads();\n    \n    float sum_exp = (threadIdx.x < (blockDim.x + 31) / 32) ? sdata[threadIdx.x] : 0.0f;\n    if ((blockDim.x + 31) / 32 > 1) {\n        sum_exp = warpReduceSum(sum_exp);\n        if (threadIdx.x == 0) sdata[0] = sum_exp;\n        __syncthreads();\n        sum_exp = sdata[0];\n    }\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    int smem_size = ((threads + 31) / 32) * sizeof(float);\n    \n    softmax_kernel<<<blocks, threads, smem_size>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimized softmax implementation using warp-level reductions for max and sum calculations, and loop unrolling for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__device__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Compute max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    float max_val = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        __shared__ float smax[WARP_SIZE];\n        smax[threadIdx.x / WARP_SIZE] = max_val;\n    }\n    __syncthreads();\n    if (threadIdx.x < WARP_SIZE) max_val = smax[threadIdx.x];\n    max_val = warp_reduce_max(max_val);\n\n    // Step 2: Compute sum of exp\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    float sum_exp = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        __shared__ float ssum[WARP_SIZE];\n        ssum[threadIdx.x / WARP_SIZE] = sum_exp;\n    }\n    __syncthreads();\n    if (threadIdx.x < WARP_SIZE) sum_exp = ssum[threadIdx.x];\n    sum_exp = warp_reduce_sum(sum_exp);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimized softmax kernel using warp-level reductions and vectorized memory access to minimize shared memory usage and maximize parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    int row = blockIdx.x;\n    int tid = threadIdx.x;\n    int lane_id = tid % WARP_SIZE;\n    int warp_id = tid / WARP_SIZE;\n    \n    const float* row_input = input + row * num_features;\n    float* row_output = output + row * num_features;\n    \n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n    \n    // Process elements with stride of blockDim.x\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, row_input[j]);\n    }\n    \n    // Warp-level reduction for max\n    float max_val = warpReduceMax(thread_max);\n    if (lane_id == 0) {\n        __shared__ float smax[32];\n        smax[warp_id] = max_val;\n    }\n    __syncthreads();\n    \n    max_val = (tid < blockDim.x / WARP_SIZE) ? smax[lane_id] : -FLT_MAX;\n    if (warp_id == 0) max_val = warpReduceMax(max_val);\n    \n    // Compute sum of exp(x - max_val)\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_sum += expf(row_input[j] - max_val);\n    }\n    \n    // Warp-level reduction for sum\n    float sum_exp = warpReduceSum(thread_sum);\n    if (lane_id == 0) {\n        __shared__ float ssum[32];\n        ssum[warp_id] = sum_exp;\n    }\n    __syncthreads();\n    \n    sum_exp = (tid < blockDim.x / WARP_SIZE) ? ssum[lane_id] : 0.0f;\n    if (warp_id == 0) sum_exp = warpReduceSum(sum_exp);\n    \n    // Compute final softmax values\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        row_output[j] = expf(row_input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimized softmax implementation using warp-level reductions for max and sum calculations, and unrolling the final division loop for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n#include <cooperative_groups.h>\n#include <cooperative_groups/reduce.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n    \n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n\n    // Step 1: Find max value in the row\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = cg::reduce(warp, thread_max, cg::greater<float>());\n    float max_val = cg::reduce(block, thread_max, cg::greater<float>());\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = cg::reduce(warp, thread_sum, cg::plus<float>());\n    float sum_exp = cg::reduce(block, thread_sum, cg::plus<float>());\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimized softmax kernel using warp-level reductions for max and sum computations, and unrolling the final write to improve memory efficiency.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__device__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    }\n    return val;\n}\n\n__device__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n\n    // Step 1: Compute max per thread\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n\n    // Step 2: Warp reduce max\n    float max_val = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicMax_block((int*)&max_val, __float_as_int(max_val));\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n\n    // Step 3: Compute sum per thread\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n\n    // Step 4: Warp reduce sum\n    float sum_exp = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        atomicAdd_block(&sum_exp, sum_exp);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n\n    // Step 5: Write results\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimized softmax implementation using warp-level reductions for max and sum computations, and loop unrolling for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n#define WARP_SIZE 32\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value in the row\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        ((volatile float*)&thread_max)[0] = thread_max;\n    }\n    __syncthreads();\n    float max_val = ((volatile float*)&thread_max)[0];\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) {\n        ((volatile float*)&thread_sum)[0] = thread_sum;\n    }\n    __syncthreads();\n    float sum_exp = ((volatile float*)&thread_sum)[0];\n\n    // Step 3: Compute softmax output\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(1024, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimized softmax implementation using warp-level reductions and shared memory to minimize synchronization overhead while maintaining numerical stability.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float s_max[WARP_SIZE];\n    __shared__ float s_sum[WARP_SIZE];\n\n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) s_max[threadIdx.x / WARP_SIZE] = thread_max;\n    __syncthreads();\n    \n    float max_val = threadIdx.x < blockDim.x / WARP_SIZE ? s_max[threadIdx.x] : -FLT_MAX;\n    max_val = warpReduceMax(max_val);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) s_sum[threadIdx.x / WARP_SIZE] = thread_sum;\n    __syncthreads();\n    \n    float sum_exp = threadIdx.x < blockDim.x / WARP_SIZE ? s_sum[threadIdx.x] : 0.0f;\n    sum_exp = warpReduceSum(sum_exp);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads, (threads / WARP_SIZE) * 2 * sizeof(float)>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimized softmax implementation using warp-level reductions for max and sum computations, and loop unrolling for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    const int row = blockIdx.x;\n    const int tid = threadIdx.x;\n    const int lane_id = tid % 32;\n    const int warp_id = tid / 32;\n    \n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float smem_max[32];\n    __shared__ float smem_sum[32];\n    \n    // Step 1: Find max value in row\n    float thread_max = -FLT_MAX;\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    \n    float max_val = warpReduceMax(thread_max);\n    if (lane_id == 0) smem_max[warp_id] = max_val;\n    __syncthreads();\n    \n    max_val = (tid < 32) ? smem_max[lane_id] : -FLT_MAX;\n    if (warp_id == 0) max_val = warpReduceMax(max_val);\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    \n    float sum_exp = warpReduceSum(thread_sum);\n    if (lane_id == 0) smem_sum[warp_id] = sum_exp;\n    __syncthreads();\n    \n    sum_exp = (tid < 32) ? smem_sum[lane_id] : 0.0f;\n    if (warp_id == 0) sum_exp = warpReduceSum(sum_exp);\n    \n    // Step 3: Compute softmax output\n    #pragma unroll 4\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimized softmax implementation using warp-level reductions for max and sum computations, and unrolling the final write to improve memory efficiency.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Compute max per warp then across warps\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    float max_val = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) {\n        atomicMax_block((int*)&max_val, __float_as_int(max_val));\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    float sum_exp = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) {\n        atomicAdd_block(&sum_exp, sum_exp);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n\n    // Step 3: Write results\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimized softmax implementation using parallel reduction for max and sum calculations, with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float sdata[32];\n    \n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) sdata[threadIdx.x / 32] = thread_max;\n    __syncthreads();\n    \n    float max_val = (threadIdx.x < 32) ? warpReduceMax(sdata[threadIdx.x]) : 0.0f;\n    if (threadIdx.x == 0) sdata[0] = max_val;\n    __syncthreads();\n    max_val = sdata[0];\n    \n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) sdata[threadIdx.x / 32] = thread_sum;\n    __syncthreads();\n    \n    float sum_exp = (threadIdx.x < 32) ? warpReduceSum(sdata[threadIdx.x]) : 0.0f;\n    if (threadIdx.x == 0) sdata[0] = sum_exp;\n    __syncthreads();\n    sum_exp = sdata[0];\n    \n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimized algorithm using warp-level reductions for max and sum computations, and loop unrolling for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <int BLOCK_SIZE>\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float sdata[BLOCK_SIZE];\n\n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += BLOCK_SIZE) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) {\n        sdata[threadIdx.x / 32] = thread_max;\n    }\n    __syncthreads();\n    float max_val = threadIdx.x < BLOCK_SIZE / 32 ? sdata[threadIdx.x] : -FLT_MAX;\n    max_val = warpReduceMax(max_val);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += BLOCK_SIZE) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) {\n        sdata[threadIdx.x / 32] = thread_sum;\n    }\n    __syncthreads();\n    float sum_exp = threadIdx.x < BLOCK_SIZE / 32 ? sdata[threadIdx.x] : 0.0f;\n    sum_exp = warpReduceSum(sum_exp);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += BLOCK_SIZE) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\n__device__ __forceinline__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ __forceinline__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    const int BLOCK_SIZE = 256;\n    dim3 blocks(batch_size);\n    dim3 threads(BLOCK_SIZE);\n\n    softmax_kernel<BLOCK_SIZE><<<blocks, threads>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        num_features\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimized implementation using parallel reduction for max and sum calculations with improved memory access patterns and warp-level optimizations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <unsigned blockSize>\n__device__ void warpReduceMax(volatile float* sdata, int tid) {\n    if (blockSize >= 64) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 32]);\n    if (blockSize >= 32) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 16]);\n    if (blockSize >= 16) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 8]);\n    if (blockSize >= 8) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 4]);\n    if (blockSize >= 4) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 2]);\n    if (blockSize >= 2) sdata[tid] = fmaxf(sdata[tid], sdata[tid + 1]);\n}\n\ntemplate <unsigned blockSize>\n__device__ void warpReduceSum(volatile float* sdata, int tid) {\n    if (blockSize >= 64) sdata[tid] += sdata[tid + 32];\n    if (blockSize >= 32) sdata[tid] += sdata[tid + 16];\n    if (blockSize >= 16) sdata[tid] += sdata[tid + 8];\n    if (blockSize >= 8) sdata[tid] += sdata[tid + 4];\n    if (blockSize >= 4) sdata[tid] += sdata[tid + 2];\n    if (blockSize >= 2) sdata[tid] += sdata[tid + 1];\n}\n\ntemplate <unsigned blockSize>\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    extern __shared__ float sdata[];\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    int tid = threadIdx.x;\n    float thread_max = -FLT_MAX;\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    sdata[tid] = thread_max;\n    __syncthreads();\n\n    if (blockSize >= 512) { if (tid < 256) { sdata[tid] = fmaxf(sdata[tid], sdata[tid + 256]); } __syncthreads(); }\n    if (blockSize >= 256) { if (tid < 128) { sdata[tid] = fmaxf(sdata[tid], sdata[tid + 128]); } __syncthreads(); }\n    if (blockSize >= 128) { if (tid < 64) { sdata[tid] = fmaxf(sdata[tid], sdata[tid + 64]); } __syncthreads(); }\n    if (tid < 32) warpReduceMax<blockSize>(sdata, tid);\n    float max_val = sdata[0];\n\n    float thread_sum = 0.0f;\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    sdata[tid] = thread_sum;\n    __syncthreads();\n\n    if (blockSize >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }\n    if (blockSize >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }\n    if (blockSize >= 128) { if (tid < 64) { sdata[tid] += sdata[tid + 64]; } __syncthreads(); }\n    if (tid < 32) warpReduceSum<blockSize>(sdata, tid);\n    float sum_exp = sdata[0];\n\n    for (int j = tid; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(512, num_features);\n    int blocks = batch_size;\n\n    switch (threads) {\n        case 512: softmax_kernel<512><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 256: softmax_kernel<256><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 128: softmax_kernel<128><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 64: softmax_kernel<64><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 32: softmax_kernel<32><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 16: softmax_kernel<16><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 8: softmax_kernel<8><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 4: softmax_kernel<4><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 2: softmax_kernel<2><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n        case 1: softmax_kernel<1><<<blocks, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features); break;\n    }\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimized softmax implementation using warp-level reductions for max and sum computations, and loop unrolling for better instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__inline__ __device__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* input, float* output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float s_max[WARP_SIZE];\n    __shared__ float s_sum[WARP_SIZE];\n\n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    #pragma unroll 4\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) s_max[threadIdx.x / WARP_SIZE] = thread_max;\n    __syncthreads();\n    float max_val = threadIdx.x < blockDim.x / WARP_SIZE ? s_max[threadIdx.x] : -FLT_MAX;\n    max_val = warp_reduce_max(max_val);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    #pragma unroll 4\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) s_sum[threadIdx.x / WARP_SIZE] = thread_sum;\n    __syncthreads();\n    float sum_exp = threadIdx.x < blockDim.x / WARP_SIZE ? s_sum[threadIdx.x] : 0.0f;\n    sum_exp = warp_reduce_sum(sum_exp);\n\n    // Step 3: Compute softmax\n    #pragma unroll 4\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(512, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimized implementation using warp-level reductions and loop unrolling to minimize shared memory accesses and synchronization overhead while maintaining numerical stability.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    __shared__ float sdata[WARP_SIZE];\n\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (threadIdx.x % WARP_SIZE == 0) sdata[threadIdx.x / WARP_SIZE] = thread_max;\n    __syncthreads();\n    float max_val = (threadIdx.x < blockDim.x / WARP_SIZE) ? sdata[threadIdx.x] : -FLT_MAX;\n    max_val = warpReduceMax(max_val);\n\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (threadIdx.x % WARP_SIZE == 0) sdata[threadIdx.x / WARP_SIZE] = thread_sum;\n    __syncthreads();\n    float sum_exp = (threadIdx.x < blockDim.x / WARP_SIZE) ? sdata[threadIdx.x] : 0.0f;\n    sum_exp = warpReduceSum(sum_exp);\n\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimized softmax kernel using warp-level reductions and vectorized memory access to minimize shared memory usage and maximize parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    int row = blockIdx.x;\n    input  += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value\n    float thread_max = -FLT_MAX;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    float max_val = warpReduceMax(thread_max);\n    if (threadIdx.x % 32 == 0) {\n        atomicMax_block((int*)&max_val, __float_as_int(max_val));\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    float sum_exp = warpReduceSum(thread_sum);\n    if (threadIdx.x % 32 == 0) {\n        atomicAdd_block(&sum_exp, sum_exp);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n\n    // Step 3: Compute softmax\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n    \n    int threads = min(1024, ((num_features + 31) / 32) * 32);\n    int blocks = batch_size;\n    \n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimized softmax implementation using warp-level reductions and loop unrolling for better parallelism and reduced shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cfloat>\n#include <math.h>\n\nconstexpr int WARP_SIZE = 32;\n\n__device__ __forceinline__ float warp_reduce_max(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = WARP_SIZE / 2; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void softmax_kernel(const float* __restrict__ input, float* __restrict__ output, int num_features) {\n    int row = blockIdx.x;\n    input += row * num_features;\n    output += row * num_features;\n    \n    // Step 1: Find max value in row using warp reductions\n    float thread_max = -FLT_MAX;\n    #pragma unroll 4\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[j]);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    float max_val = __shfl_sync(0xFFFFFFFF, thread_max, 0);\n\n    // Step 2: Compute sum of exp(x - max_val)\n    float thread_sum = 0.0f;\n    #pragma unroll 4\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        thread_sum += expf(input[j] - max_val);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    float sum_exp = __shfl_sync(0xFFFFFFFF, thread_sum, 0);\n\n    // Step 3: Compute softmax\n    #pragma unroll 4\n    for (int j = threadIdx.x; j < num_features; j += blockDim.x) {\n        output[j] = expf(input[j] - max_val) / sum_exp;\n    }\n}\n\ntorch::Tensor softmax_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int num_features = input.size(1);\n\n    int threads = min(256, ((num_features + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n    int blocks = batch_size;\n\n    softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &softmax_forward, \"Softmax forward (CUDA)\");\n}",
        "score": null
    }
]