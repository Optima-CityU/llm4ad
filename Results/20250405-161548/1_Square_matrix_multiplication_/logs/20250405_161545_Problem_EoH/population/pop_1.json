[
    {
        "algorithm": "{Optimized the matrix multiplication using shared memory tiling with 16x16 blocks and loop unrolling to improve memory access patterns and reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6231935262680053
    },
    {
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tile-based matrix multiplication with proper synchronization and block size tuning.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.623593544960022
    },
    {
        "algorithm": "{Optimized matrix multiplication using shared memory for tile-based computation and loop unrolling for improved memory access patterns and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6247615575790406
    },
    {
        "algorithm": "{Optimize the matrix multiplication using shared memory to reduce global memory accesses and improve memory coalescing, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6265567541122437
    },
    {
        "algorithm": "{Optimize the matrix multiplication kernel by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -Infinity
    }
]