[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the matrix multiplication using shared memory to reduce global memory accesses and improve memory coalescing, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6265567541122437
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling and loop unrolling to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_row = t * TILE_SIZE + threadIdx.y;\n        int tiled_col = t * TILE_SIZE + threadIdx.x;\n\n        As[threadIdx.y][threadIdx.x] = (row < N && tiled_col < N) ? A[row * N + tiled_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (tiled_row < N && col < N) ? B[tiled_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimized matrix multiplication using shared memory for tile-based computation and loop unrolling for improved memory access patterns and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6247615575790406
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tile-based matrix multiplication with proper synchronization and block size tuning.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.623593544960022
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimized the matrix multiplication using shared memory tiling with 16x16 blocks and loop unrolling to improve memory access patterns and reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6231935262680053
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the matrix multiplication kernel by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6248063325881956
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling, loop unrolling, and increasing thread block size to improve memory access patterns and arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_col = t * TILE_SIZE + threadIdx.x;\n        int tiled_row = t * TILE_SIZE + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < N && tiled_col < N) ? A[row * N + tiled_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (tiled_row < N && col < N) ? B[tiled_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6827391386032104
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimized the matrix multiplication using shared memory for tiled computation and increased thread block size for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_row = t * TILE_SIZE + threadIdx.y;\n        int tiled_col = t * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tiled_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && tiled_row < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6845151662826536
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimized matrix multiplication using shared memory for tile-based computation and loop unrolling to reduce memory accesses and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.620153546333313
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimized matrix multiplication using warp-level tiling and register accumulation to reduce shared memory usage and improve memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x % WARP_SIZE;\n    const int warp_id = threadIdx.x / WARP_SIZE;\n\n    const int row = blockIdx.y * TILE_SIZE + warp_row;\n    const int col = blockIdx.x * TILE_SIZE + warp_col;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        const int tile_offset = tile * TILE_SIZE;\n        const int k = tile_offset + warp_col;\n\n        float a_val = (row < N && k < N) ? A[row * N + k] : 0.0f;\n        float b_val = (col < N && k < N) ? B[k * N + col] : 0.0f;\n\n        for (int i = 0; i < WARP_SIZE; ++i) {\n            value += __shfl_sync(0xFFFFFFFF, a_val, i) * \n                     __shfl_sync(0xFFFFFFFF, b_val, i);\n        }\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(32, 32);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the CUDA matmul kernel by using register tiling with warp-level matrix multiplication and double buffering to hide memory latency.}",
        "function": "#include <torch/extension.h>\n\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    const int warp_id = threadIdx.y;\n    const int lane_id = threadIdx.x;\n    \n    const int row = (blockIdx.y * WARPS_PER_BLOCK + warp_id) * TILE_SIZE + (lane_id / (WARP_SIZE/TILE_SIZE));\n    const int col = (blockIdx.x * WARPS_PER_BLOCK) * TILE_SIZE + (lane_id % (WARP_SIZE/TILE_SIZE)) * TILE_SIZE;\n    \n    float reg_A[TILE_SIZE];\n    float reg_B[TILE_SIZE];\n    float reg_C[TILE_SIZE] = {0.0f};\n    \n    for (int tile = 0; tile < N; tile += TILE_SIZE) {\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            int a_row = row + i;\n            int a_col = tile + (lane_id % TILE_SIZE);\n            reg_A[i] = (a_row < N && a_col < N) ? A[a_row * N + a_col] : 0.0f;\n            \n            int b_row = tile + (lane_id / TILE_SIZE);\n            int b_col = col + i;\n            reg_B[i] = (b_row < N && b_col < N) ? B[b_row * N + b_col] : 0.0f;\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            #pragma unroll\n            for (int j = 0; j < TILE_SIZE; ++j) {\n                reg_C[i] += reg_A[j] * reg_B[(j + i) % TILE_SIZE];\n            }\n        }\n    }\n    \n    #pragma unroll\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        int c_row = row + i;\n        int c_col = col + (lane_id % TILE_SIZE);\n        if (c_row < N && c_col < N) {\n            C[c_row * N + c_col] = reg_C[i];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(WARP_SIZE, WARPS_PER_BLOCK);\n    const dim3 grid((N + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK),\n                    (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the matrix multiplication using register tiling and warp-level operations to maximize instruction-level parallelism and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    const int warp_id = threadIdx.y;\n    const int lane_id = threadIdx.x;\n    const int warp_row = blockIdx.y * WARPS_PER_BLOCK + warp_id;\n    const int warp_col = blockIdx.x;\n\n    float reg_A[TILE_SIZE/WARPS_PER_BLOCK];\n    float reg_B[TILE_SIZE/THREADS_PER_WARP];\n    float reg_C[TILE_SIZE/WARPS_PER_BLOCK][TILE_SIZE/THREADS_PER_WARP] = {0};\n\n    for (int tile = 0; tile < N; tile += TILE_SIZE) {\n        // Load A tile into registers\n        for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n            int row = warp_row * (TILE_SIZE/WARPS_PER_BLOCK) + i;\n            int col = tile + lane_id;\n            if (row < N && col < N) {\n                reg_A[i] = A[row * N + col];\n            } else {\n                reg_A[i] = 0.0f;\n            }\n        }\n\n        // Load B tile into registers\n        for (int j = 0; j < TILE_SIZE/THREADS_PER_WARP; ++j) {\n            int row = tile + warp_id;\n            int col = warp_col * TILE_SIZE + lane_id + j * THREADS_PER_WARP;\n            if (row < N && col < N) {\n                reg_B[j] = B[row * N + col];\n            } else {\n                reg_B[j] = 0.0f;\n            }\n        }\n\n        // Compute partial results\n        for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n            for (int j = 0; j < TILE_SIZE/THREADS_PER_WARP; ++j) {\n                for (int k = 0; k < TILE_SIZE; ++k) {\n                    if (k % THREADS_PER_WARP == warp_id) {\n                        reg_C[i][j] += reg_A[i] * reg_B[k/TILE_SIZE];\n                    }\n                }\n            }\n        }\n    }\n\n    // Store results\n    for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n        for (int j = 0; j < TILE_SIZE/THREADS_PER_WARP; ++j) {\n            int row = warp_row * (TILE_SIZE/WARPS_PER_BLOCK) + i;\n            int col = warp_col * TILE_SIZE + lane_id + j * THREADS_PER_WARP;\n            if (row < N && col < N) {\n                C[row * N + col] = reg_C[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(THREADS_PER_WARP, WARPS_PER_BLOCK);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE/WARPS_PER_BLOCK - 1) / (TILE_SIZE/WARPS_PER_BLOCK));\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the CUDA matmul kernel by using register tiling with warp-level matrix multiplication and cooperative fetching to reduce shared memory bank conflicts and improve memory access efficiency.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define SUB_TILE_SIZE 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int row = blockIdx.y * TILE_SIZE + warp_id * SUB_TILE_SIZE;\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n\n    float reg_A[SUB_TILE_SIZE];\n    float reg_B[SUB_TILE_SIZE];\n    float reg_C[SUB_TILE_SIZE] = {0.0f};\n\n    for (int tile = 0; tile < N; tile += TILE_SIZE) {\n        #pragma unroll\n        for (int i = 0; i < SUB_TILE_SIZE; ++i) {\n            if (row + i < N && tile + lane_id < N) {\n                reg_A[i] = A[(row + i) * N + tile + lane_id];\n            } else {\n                reg_A[i] = 0.0f;\n            }\n        }\n\n        #pragma unroll\n        for (int j = 0; j < SUB_TILE_SIZE; ++j) {\n            if (tile + j < N && col < N) {\n                reg_B[j] = B[(tile + j) * N + col];\n            } else {\n                reg_B[j] = 0.0f;\n            }\n        }\n\n        #pragma unroll\n        for (int i = 0; i < SUB_TILE_SIZE; ++i) {\n            #pragma unroll\n            for (int j = 0; j < SUB_TILE_SIZE; ++j) {\n                reg_C[i] += reg_A[i] * reg_B[j];\n            }\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < SUB_TILE_SIZE; ++i) {\n        if (row + i < N && col < N) {\n            C[(row + i) * N + col] = reg_C[i];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(WARP_SIZE * (TILE_SIZE / SUB_TILE_SIZE));\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the matrix multiplication by using shared memory with a larger tile size (32x32) and loop unrolling to better utilize the RTX 4090's resources while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < N / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE;\n        As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k + threadIdx.x];\n        Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6434784173965453
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize matrix multiplication by increasing the tile size to 32x32 for better memory access efficiency and utilizing warp-level operations for reduced synchronization overhead.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6858239650726317
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the matrix multiplication by transposing matrix B for better memory coalescing and using shared memory for both matrices with a larger tile size to improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * N + tiled_k];  // B is transposed\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[threadIdx.x][k];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimized matrix multiplication using register tiling with warp-level operations to reduce shared memory usage and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    int warp_id = threadIdx.x / THREADS_PER_WARP;\n    int lane_id = threadIdx.x % THREADS_PER_WARP;\n    \n    int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float reg_A[TILE_SIZE / WARPS_PER_BLOCK] = {0.0f};\n    float reg_B[TILE_SIZE / WARPS_PER_BLOCK] = {0.0f};\n    float value = 0.0f;\n    \n    for (int tile = 0; tile < N; tile += TILE_SIZE) {\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            if (row + i < N && tile + lane_id < N) {\n                reg_A[i] = A[(row + i) * N + (tile + lane_id)];\n            }\n            if (tile + i < N && col < N) {\n                reg_B[i] = B[(tile + i) * N + col];\n            }\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            #pragma unroll\n            for (int j = 0; j < TILE_SIZE / WARPS_PER_BLOCK; ++j) {\n                value += reg_A[i] * reg_B[j];\n            }\n        }\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(THREADS_PER_WARP * WARPS_PER_BLOCK);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory with double buffering to overlap memory transfers and computation while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n    int stage = 0;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        // Preload next tile while computing current tile\n        if (row < N && tile_col < N) {\n            As[1-stage][threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        }\n        if (tile_row < N && col < N) {\n            Bs[1-stage][threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        }\n        __syncthreads();\n\n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[stage][threadIdx.y][k] * Bs[stage][k][threadIdx.x];\n        }\n        __syncthreads();\n\n        stage = 1 - stage; // Switch buffers\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory with double buffering to overlap memory transfers and computation, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    int phase = 0;\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        // Load next tile while computing current tile\n        if (row < N && tile_col < N) {\n            As[1-phase][threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[1-phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[1-phase][threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[1-phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[phase][threadIdx.y][k] * Bs[phase][k][threadIdx.x];\n        }\n\n        __syncthreads();\n        phase = 1 - phase;\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimized matrix multiplication using shared memory with double buffering to overlap memory transfers and computation, and increased tile size for better memory locality.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        // Preload next tile while computing current tile\n        int tiled_k = (tile + 1) * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = (tile + 1) * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n        }\n\n        __syncthreads();\n        load_phase = 1 - load_phase;\n        compute_phase = 1 - compute_phase;\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimized matrix multiplication using shared memory with double buffering to overlap memory transfers and computation, and increased tile size for better utilization of shared memory bandwidth.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    int load_phase = 0;\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        int compute_phase = load_phase;\n        load_phase ^= 1;\n        \n        if (tile > 0) {\n            #pragma unroll\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n    \n    #pragma unroll\n    for (int k = 0; k < TILE_SIZE; ++k) {\n        value += As[load_phase][threadIdx.y][k] * Bs[load_phase][k][threadIdx.x];\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n    \n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n    \n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimized matrix multiplication using shared memory with larger tile size (32x32) and loop unrolling for improved memory access patterns and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.686726379394531
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimized matrix multiplication using shared memory with larger tile size (32x32) and loop unrolling for improved memory access patterns and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.696566367149353
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimized matrix multiplication using shared memory with larger tile size (32x32) and loop unrolling for improved memory access patterns and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6873727798461915
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimized matrix multiplication using shared memory with larger tile size (32x32) and double buffering to hide memory latency and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int phase = tile % 2;\n        \n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[phase][threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[phase][threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (tile > 0) {\n            int prev_phase = (tile - 1) % 2;\n            #pragma unroll\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                value += As[prev_phase][threadIdx.y][k] * Bs[prev_phase][k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    // Process last phase\n    int last_phase = ((N + TILE_SIZE - 1) / TILE_SIZE - 1) % 2;\n    #pragma unroll\n    for (int k = 0; k < TILE_SIZE; ++k) {\n        value += As[last_phase][threadIdx.y][k] * Bs[last_phase][k][threadIdx.x];\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.751977562904358
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the CUDA matmul kernel by using register tiling with warp-level matrix multiplication and cooperative fetching to maximize memory throughput and minimize shared memory usage.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_K 8\n#define TILE_M 8\n#define TILE_N 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    \n    int row = blockIdx.y * (blockDim.y * TILE_M) + warp_id * TILE_M;\n    int col = blockIdx.x * (blockDim.x * TILE_N) + (lane_id % TILE_N) * TILE_N;\n    \n    float reg_A[TILE_M];\n    float reg_B[TILE_N];\n    float reg_C[TILE_M][TILE_N] = {0.0f};\n    \n    for (int k = 0; k < N; k += TILE_K) {\n        // Cooperative fetching of tiles\n        for (int m = 0; m < TILE_M; ++m) {\n            if (row + m < N && k + lane_id < N) {\n                reg_A[m] = A[(row + m) * N + k + lane_id];\n            } else {\n                reg_A[m] = 0.0f;\n            }\n        }\n        \n        for (int n = 0; n < TILE_N; ++n) {\n            if (col + n < N && k + lane_id < N) {\n                reg_B[n] = B[(k + lane_id) * N + col + n];\n            } else {\n                reg_B[n] = 0.0f;\n            }\n        }\n        \n        // Warp-level matrix multiplication\n        for (int kk = 0; kk < TILE_K; ++kk) {\n            for (int m = 0; m < TILE_M; ++m) {\n                for (int n = 0; n < TILE_N; ++n) {\n                    reg_C[m][n] += reg_A[m] * reg_B[n];\n                }\n            }\n        }\n    }\n    \n    // Write results\n    for (int m = 0; m < TILE_M; ++m) {\n        for (int n = 0; n < TILE_N; ++n) {\n            if (row + m < N && col + n < N) {\n                C[(row + m) * N + col + n] = reg_C[m][n];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n    \n    const int warps_per_block = 4;\n    const int threads_per_block = warps_per_block * WARP_SIZE;\n    const dim3 block(threads_per_block);\n    const dim3 grid((N + (TILE_N * WARP_SIZE) - 1) / (TILE_N * WARP_SIZE),\n                 (N + (TILE_M * warps_per_block) - 1) / (TILE_M * warps_per_block));\n    \n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the CUDA matmul kernel by utilizing warp-level matrix multiplication with cooperative loading and vectorized memory access for improved memory throughput.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x;\n    const int warp_id = warp_row * blockDim.x + warp_col;\n    const int lane_id = warp_id % WARP_SIZE;\n\n    const int row = blockIdx.y * TILE_SIZE + warp_row * 8;\n    const int col = blockIdx.x * TILE_SIZE + warp_col * 8;\n\n    float4 c[2][2] = {{{0.0f}}};\n\n    for (int tile = 0; tile < N; tile += TILE_SIZE) {\n        __shared__ float As[TILE_SIZE][TILE_SIZE];\n        __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n        float4 a[2], b[2];\n\n        // Cooperative loading of tiles\n        for (int i = 0; i < 8; ++i) {\n            int load_row = row + i;\n            int load_col = tile + lane_id;\n            if (load_row < N && load_col < N) {\n                As[load_row % TILE_SIZE][load_col % TILE_SIZE] = A[load_row * N + load_col];\n            }\n\n            load_row = tile + lane_id;\n            load_col = col + i;\n            if (load_row < N && load_col < N) {\n                Bs[load_row % TILE_SIZE][load_col % TILE_SIZE] = B[load_row * N + load_col];\n            }\n        }\n        __syncthreads();\n\n        // Warp-level matrix multiplication\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            a[0] = *reinterpret_cast<float4*>(&As[warp_row * 8][k]);\n            a[1] = *reinterpret_cast<float4*>(&As[warp_row * 8 + 4][k]);\n            b[0] = *reinterpret_cast<float4*>(&Bs[k][warp_col * 8]);\n            b[1] = *reinterpret_cast<float4*>(&Bs[k][warp_col * 8 + 4]);\n\n            for (int i = 0; i < 2; ++i) {\n                for (int j = 0; j < 2; ++j) {\n                    c[i][j].x += a[i].x * b[j].x;\n                    c[i][j].y += a[i].y * b[j].y;\n                    c[i][j].z += a[i].z * b[j].z;\n                    c[i][j].w += a[i].w * b[j].w;\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    // Store results\n    if (row < N && col < N) {\n        for (int i = 0; i < 2; ++i) {\n            for (int j = 0; j < 2; ++j) {\n                int store_row = row + i * 4;\n                int store_col = col + j * 4;\n                if (store_row < N && store_col < N) {\n                    *reinterpret_cast<float4*>(&C[store_row * N + store_col]) = c[i][j];\n                }\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(4, 4);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the CUDA matmul kernel by using register tiling with warp-level matrix multiplication and cooperative loading of tiles from global memory.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    const int warp_id = threadIdx.x / THREADS_PER_WARP;\n    const int lane_id = threadIdx.x % THREADS_PER_WARP;\n    const int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    const int col = blockIdx.x * TILE_SIZE + lane_id;\n\n    float reg_A[TILE_SIZE / WARPS_PER_BLOCK] = {0.0f};\n    float reg_B[TILE_SIZE / THREADS_PER_WARP] = {0.0f};\n    float reg_C[TILE_SIZE / WARPS_PER_BLOCK][TILE_SIZE / THREADS_PER_WARP] = {{0.0f}};\n\n    for (int tile = 0; tile < N; tile += TILE_SIZE) {\n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            int load_row = row + i;\n            int load_col = tile + lane_id;\n            if (load_row < N && load_col < N) {\n                reg_A[i] = A[load_row * N + load_col];\n            }\n        }\n\n        for (int j = 0; j < TILE_SIZE / THREADS_PER_WARP; ++j) {\n            int load_row = tile + warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + j;\n            int load_col = col;\n            if (load_row < N && load_col < N) {\n                reg_B[j] = B[load_row * N + load_col];\n            }\n        }\n\n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            for (int j = 0; j < TILE_SIZE / THREADS_PER_WARP; ++j) {\n                reg_C[i][j] += reg_A[i] * reg_B[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n        for (int j = 0; j < TILE_SIZE / THREADS_PER_WARP; ++j) {\n            int store_row = row + i;\n            int store_col = col + j;\n            if (store_row < N && store_col < N) {\n                C[store_row * N + store_col] = reg_C[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(WARPS_PER_BLOCK * THREADS_PER_WARP);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the CUDA matmul kernel by using register tiling with warp-level matrix multiply-accumulate operations and double buffering to hide memory latency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 8\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    const int warp_id = threadIdx.x / THREADS_PER_WARP;\n    const int lane_id = threadIdx.x % THREADS_PER_WARP;\n    const int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    const int col = blockIdx.x * TILE_SIZE + lane_id;\n\n    float reg_a[TILE_SIZE / WARPS_PER_BLOCK] = {0.0f};\n    float reg_b[TILE_SIZE / THREADS_PER_WARP] = {0.0f};\n    float reg_c[TILE_SIZE / WARPS_PER_BLOCK][TILE_SIZE / THREADS_PER_WARP] = {{0.0f}};\n\n    for (int tile = 0; tile < N; tile += TILE_SIZE) {\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            if (row + i < N && tile + lane_id < N) {\n                reg_a[i] = A[(row + i) * N + tile + lane_id];\n            }\n        }\n\n        #pragma unroll\n        for (int j = 0; j < TILE_SIZE / THREADS_PER_WARP; ++j) {\n            if (tile + warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + j < N && col < N) {\n                reg_b[j] = B[(tile + warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + j) * N + col];\n            }\n        }\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            #pragma unroll\n            for (int j = 0; j < TILE_SIZE / THREADS_PER_WARP; ++j) {\n                reg_c[i][j] += reg_a[i] * reg_b[j];\n            }\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n        #pragma unroll\n        for (int j = 0; j < TILE_SIZE / THREADS_PER_WARP; ++j) {\n            if (row + i < N && col + j * THREADS_PER_WARP < N) {\n                C[(row + i) * N + col + j * THREADS_PER_WARP] = reg_c[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(THREADS_PER_WARP * WARPS_PER_BLOCK);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimized matrix multiplication using shared memory with double buffering to overlap memory transfers with computation and reduce synchronization overhead.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_idx = 0;\n    int compute_idx = 0;\n    \n    // Preload first tile\n    int tile_row = threadIdx.y;\n    int tile_col = threadIdx.x;\n    \n    if (row < N && tile_col < N) {\n        As[load_idx][tile_row][tile_col] = A[row * N + tile_col];\n    } else {\n        As[load_idx][tile_row][tile_col] = 0.0f;\n    }\n\n    if (tile_row < N && col < N) {\n        Bs[load_idx][tile_row][tile_col] = B[tile_row * N + col];\n    } else {\n        Bs[load_idx][tile_row][tile_col] = 0.0f;\n    }\n\n    __syncthreads();\n\n    for (int tile = 1; tile <= (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        load_idx = 1 - load_idx;\n        compute_idx = 1 - compute_idx;\n\n        // Load next tile while computing current one\n        if (tile < (N + TILE_SIZE - 1) / TILE_SIZE) {\n            int next_tile_row = tile * TILE_SIZE + threadIdx.y;\n            int next_tile_col = tile * TILE_SIZE + threadIdx.x;\n\n            if (row < N && next_tile_col < N) {\n                As[load_idx][threadIdx.y][threadIdx.x] = A[row * N + next_tile_col];\n            } else {\n                As[load_idx][threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            if (next_tile_row < N && col < N) {\n                Bs[load_idx][threadIdx.y][threadIdx.x] = B[next_tile_row * N + col];\n            } else {\n                Bs[load_idx][threadIdx.y][threadIdx.x] = 0.0f;\n            }\n        }\n\n        // Compute current tile\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_idx][threadIdx.y][k] * Bs[compute_idx][k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize matrix multiplication by increasing the tile size to 32x32 for better utilization of shared memory and GPU resources, while maintaining correctness through proper boundary checks.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory with a larger tile size (32x32) and loop unrolling to reduce synchronization overhead and improve memory access efficiency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < N / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        \n        As[threadIdx.y][threadIdx.x] = A[row * N + tile_offset + threadIdx.x];\n        Bs[threadIdx.y][threadIdx.x] = B[(tile_offset + threadIdx.y) * N + col];\n        \n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6445600032806396
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimized matrix multiplication using shared memory with double buffering to overlap memory transfers with computation and increased tile size for better memory locality.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 0;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        // Load next tile while computing current one\n        if (row < N && tile_col < N) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute previous tile\n        if (tile > 0) {\n            #pragma unroll\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n            }\n        }\n\n        __syncthreads();\n\n        // Swap phases\n        compute_phase = load_phase;\n        load_phase = 1 - load_phase;\n    }\n\n    // Compute last tile\n    #pragma unroll\n    for (int k = 0; k < TILE_SIZE; ++k) {\n        value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.745030331611633
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimized matrix multiplication using shared memory with double buffering to overlap memory transfers and computation, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    // Preload first tile\n    int tiled_k = threadIdx.x;\n    if (row < N && tiled_k < N) {\n        As[0][threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n    } else {\n        As[0][threadIdx.y][threadIdx.x] = 0.0f;\n    }\n\n    tiled_k = threadIdx.y;\n    if (col < N && tiled_k < N) {\n        Bs[0][threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n    } else {\n        Bs[0][threadIdx.y][threadIdx.x] = 0.0f;\n    }\n\n    __syncthreads();\n\n    for (int tile = 1; tile <= (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        // Preload next tile while computing current\n        int next_tile = tile;\n        int next_tiled_k = next_tile * TILE_SIZE + threadIdx.x;\n        if (next_tile < (N + TILE_SIZE - 1) / TILE_SIZE) {\n            if (row < N && next_tiled_k < N) {\n                As[tile%2][threadIdx.y][threadIdx.x] = A[row * N + next_tiled_k];\n            } else {\n                As[tile%2][threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            next_tiled_k = next_tile * TILE_SIZE + threadIdx.y;\n            if (col < N && next_tiled_k < N) {\n                Bs[tile%2][threadIdx.y][threadIdx.x] = B[next_tiled_k * N + col];\n            } else {\n                Bs[tile%2][threadIdx.y][threadIdx.x] = 0.0f;\n            }\n        }\n\n        // Compute current tile\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[(tile-1)%2][threadIdx.y][k] * Bs[(tile-1)%2][k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.7974464178085325
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimized matrix multiplication using shared memory with double buffering to overlap memory transfers and computation, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        // Prefetch next tile while computing current tile\n        int next_tile = tile + 1;\n        int next_tile_row = next_tile * TILE_SIZE + threadIdx.y;\n        int next_tile_col = next_tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && next_tile_col < N) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * N + next_tile_col];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (next_tile_row < N && col < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[next_tile_row * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n        }\n\n        __syncthreads();\n\n        // Swap phases\n        int temp = load_phase;\n        load_phase = compute_phase;\n        compute_phase = temp;\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory with double buffering to overlap memory transfers with computation, while maintaining the same block size for simplicity.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        // Prefetch next tile while computing current tile\n        if (row < N && tile_col < N) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute with previous tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n        }\n\n        __syncthreads();\n\n        // Swap phases\n        int temp = load_phase;\n        load_phase = compute_phase;\n        compute_phase = temp;\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory with double buffering to overlap memory transfers and computation, while maintaining correctness and using a tile size of 32 for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n        \n        // Load next tile while computing current tile\n        int next_tile = tile + 1;\n        int next_tile_row = next_tile * TILE_SIZE + threadIdx.y;\n        int next_tile_col = next_tile * TILE_SIZE + threadIdx.x;\n        \n        if (row < N && next_tile_col < N) {\n            As[(tile+1)%2][threadIdx.y][threadIdx.x] = A[row * N + next_tile_col];\n        } else {\n            As[(tile+1)%2][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (next_tile_row < N && col < N) {\n            Bs[(tile+1)%2][threadIdx.y][threadIdx.x] = B[next_tile_row * N + col];\n        } else {\n            Bs[(tile+1)%2][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[tile%2][threadIdx.y][k] * Bs[tile%2][k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory with increased tile size (32x32) for better memory access patterns and reduced global memory accesses, while maintaining correctness through proper boundary checks and synchronization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6882943391799925
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimized matrix multiplication using larger shared memory tiles (32x32) for improved memory locality and reduced global memory accesses, with loop unrolling and boundary condition optimizations.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6855743646621706
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimized matrix multiplication using larger tile size (32x32) for better memory access efficiency and reduced global memory transactions, with loop unrolling and shared memory utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.685999941825867
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimized matrix multiplication using shared memory with larger tile size (32x32) for better memory access efficiency and reduced global memory transactions.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(TILE_SIZE, TILE_SIZE);\n    const dim3 grid((N + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -2.6877567768096924
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the CUDA matmul kernel by using register tiling with warp-level matrix multiplication and cooperative fetching to maximize memory throughput and minimize shared memory usage.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    const int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) / WARP_SIZE;\n    const int warp_col = blockIdx.x * blockDim.x + threadIdx.x;\n    const int lane_id = threadIdx.y % WARP_SIZE;\n\n    float reg_A[TILE_SIZE];\n    float reg_B[TILE_SIZE];\n    float reg_C[TILE_SIZE][TILE_SIZE] = {0.0f};\n\n    for (int tile = 0; tile < (N + TILE_SIZE * WARP_SIZE - 1) / (TILE_SIZE * WARP_SIZE); ++tile) {\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            int row = warp_row * TILE_SIZE + i;\n            int col = tile * TILE_SIZE * WARP_SIZE + lane_id;\n            reg_A[i] = (row < N && col < N) ? A[row * N + col] : 0.0f;\n        }\n\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int row = tile * TILE_SIZE * WARP_SIZE + lane_id;\n            int col = warp_col * TILE_SIZE + j;\n            reg_B[j] = (row < N && col < N) ? B[row * N + col] : 0.0f;\n        }\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            for (int j = 0; j < TILE_SIZE; ++j) {\n                reg_C[i][j] += reg_A[i] * reg_B[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int row = warp_row * TILE_SIZE + i;\n            int col = warp_col * TILE_SIZE + j;\n            if (row < N && col < N) {\n                C[row * N + col] = reg_C[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(8, WARP_SIZE);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the CUDA matmul kernel by using register tiling with warp-level matrix multiplication and cooperative loading to maximize memory throughput and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define REG_TILE 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int warp_row = (blockIdx.y * blockDim.y + warp_id) * REG_TILE;\n    int warp_col = (blockIdx.x * blockDim.x) + (lane_id / (TILE_SIZE/REG_TILE)) * REG_TILE;\n\n    float reg_A[REG_TILE];\n    float reg_B[REG_TILE];\n    float reg_C[REG_TILE][REG_TILE] = {0};\n\n    for (int tile = 0; tile < N; tile += TILE_SIZE) {\n        for (int i = 0; i < REG_TILE; ++i) {\n            int load_row = warp_row + i;\n            int load_col = tile + (lane_id % (TILE_SIZE/REG_TILE));\n            if (load_row < N && load_col < N) {\n                reg_A[i] = A[load_row * N + load_col];\n            } else {\n                reg_A[i] = 0.0f;\n            }\n\n            load_row = tile + (lane_id / (TILE_SIZE/REG_TILE)) + i;\n            load_col = warp_col + (lane_id % REG_TILE);\n            if (load_row < N && load_col < N) {\n                reg_B[i] = B[load_row * N + load_col];\n            } else {\n                reg_B[i] = 0.0f;\n            }\n        }\n\n        for (int i = 0; i < REG_TILE; ++i) {\n            for (int j = 0; j < REG_TILE; ++j) {\n                reg_C[i][j] += reg_A[i] * reg_B[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < REG_TILE; ++i) {\n        for (int j = 0; j < REG_TILE; ++j) {\n            int store_row = warp_row + i;\n            int store_col = warp_col + j;\n            if (store_row < N && store_col < N) {\n                C[store_row * N + store_col] = reg_C[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(256);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the CUDA matmul kernel by utilizing warp-level matrix multiplication with cooperative loading and register tiling for improved memory efficiency and reduced shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    const int row = blockIdx.y * (WARPS_PER_BLOCK * TILE_SIZE) + warp_id * TILE_SIZE;\n    const int col = blockIdx.x * TILE_SIZE;\n\n    float reg_a[TILE_SIZE] = {0.0f};\n    float reg_b[TILE_SIZE] = {0.0f};\n    float reg_c[TILE_SIZE][TILE_SIZE] = {{0.0f}};\n\n    for (int tile = 0; tile < N; tile += TILE_SIZE) {\n        // Cooperative loading of tiles\n        for (int i = 0; i < TILE_SIZE; i++) {\n            int a_row = row + i;\n            int a_col = tile + lane_id;\n            if (a_row < N && a_col < N) {\n                reg_a[i] = A[a_row * N + a_col];\n            }\n\n            int b_row = tile + i;\n            int b_col = col + lane_id;\n            if (b_row < N && b_col < N) {\n                reg_b[i] = B[b_row * N + b_col];\n            }\n        }\n\n        // Warp-level matrix multiplication\n        for (int i = 0; i < TILE_SIZE; i++) {\n            for (int j = 0; j < TILE_SIZE; j++) {\n                reg_c[i][j] += reg_a[i] * reg_b[j];\n            }\n        }\n    }\n\n    // Write results to global memory\n    for (int i = 0; i < TILE_SIZE; i++) {\n        for (int j = 0; j < TILE_SIZE; j++) {\n            int c_row = row + i;\n            int c_col = col + j;\n            if (c_row < N && c_col < N) {\n                C[c_row * N + c_col] = reg_c[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(WARP_SIZE * WARPS_PER_BLOCK);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + (WARPS_PER_BLOCK * TILE_SIZE) - 1) / (WARPS_PER_BLOCK * TILE_SIZE));\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the CUDA matmul kernel by using register tiling with warp-level matrix multiplication and cooperative fetching to maximize memory throughput and minimize shared memory usage.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define SUB_TILE 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int warp_id = threadIdx.y;\n    int lane_id = threadIdx.x;\n    \n    int row = blockIdx.y * TILE_SIZE + warp_id * SUB_TILE;\n    int col = blockIdx.x * TILE_SIZE + lane_id * SUB_TILE;\n\n    float accum[SUB_TILE][SUB_TILE] = {0.0f};\n\n    for (int k = 0; k < N; k += TILE_SIZE) {\n        float a_frag[SUB_TILE];\n        float b_frag[SUB_TILE];\n\n        for (int i = 0; i < SUB_TILE; ++i) {\n            int load_row = row + i;\n            int load_col = k + lane_id;\n            if (load_row < N && load_col < N) {\n                a_frag[i] = A[load_row * N + load_col];\n            } else {\n                a_frag[i] = 0.0f;\n            }\n        }\n\n        for (int j = 0; j < SUB_TILE; ++j) {\n            int load_row = k + warp_id;\n            int load_col = col + j;\n            if (load_row < N && load_col < N) {\n                b_frag[j] = B[load_row * N + load_col];\n            } else {\n                b_frag[j] = 0.0f;\n            }\n        }\n\n        for (int i = 0; i < SUB_TILE; ++i) {\n            for (int j = 0; j < SUB_TILE; ++j) {\n                accum[i][j] += a_frag[i] * b_frag[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < SUB_TILE; ++i) {\n        for (int j = 0; j < SUB_TILE; ++j) {\n            int store_row = row + i;\n            int store_col = col + j;\n            if (store_row < N && store_col < N) {\n                C[store_row * N + store_col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const dim3 block(WARP_SIZE, WARP_SIZE/SUB_TILE);\n    const dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    }
]