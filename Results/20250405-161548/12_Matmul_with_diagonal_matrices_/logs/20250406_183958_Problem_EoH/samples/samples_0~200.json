[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the diagonal matrix multiplication by leveraging coalesced memory access and increased thread utilization through a 1D grid-stride loop pattern.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  int threads = 256;\n  int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the diagonal matrix multiplication by processing each row of B in parallel, with each thread handling multiple consecutive elements in a row to improve memory access patterns and reduce thread overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.x;\n  int tid = threadIdx.x;\n  int stride = blockDim.x;\n  \n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (int col = tid; col < M; col += stride) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const dim3 blocks(N);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the CUDA kernel by increasing thread utilization through better block and grid sizing, and using vectorized memory accesses where possible.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x - 1) / threads_x, (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the diagonal matrix multiplication by utilizing coalesced memory access and increasing thread utilization through a 1D grid-stride loop pattern.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  int threads = 256;\n  int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the CUDA kernel by increasing thread utilization through better block and grid sizing, and ensuring coalesced memory accesses by transposing the matrix traversal pattern.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x - 1) / threads_x, (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and increased thread utilization through a 1D grid-stride loop.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  int threads = 256;\n  int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the diagonal matrix multiplication by leveraging coalesced memory access and increased thread utilization through block tiling and vectorized memory operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  \n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (; col < M; col += gridDim.x * blockDim.x) {\n      if (col < M) {\n        C[row * M + col] = a_val * B[row * M + col];\n      }\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x - 1) / threads_x, (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D grid layout with coalesced memory access and increased parallelism by processing multiple elements per thread.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  int threads = 256;\n  int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimized algorithm: Process each row of B in parallel, with each thread handling contiguous elements in a row for better memory coalescing, using a 1D block layout for efficient memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.x;\n  int tid = threadIdx.x;\n  int stride = blockDim.x;\n  \n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (int col = tid; col < M; col += stride) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const dim3 blocks(N);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D grid layout and coalesced memory accesses, with each thread handling a column of B for its assigned row.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (int col = 0; col < M; ++col) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const dim3 blocks((N + threads - 1) / threads);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory accesses and increasing thread utilization through a 1D grid-stride loop pattern.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  int threads = 256;\n  int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D thread block layout and coalesced memory accesses to improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D grid layout with coalesced memory access and increased thread utilization per block.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D thread block layout and coalesced memory accesses to improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const dim3 threadsPerBlock(threads);\n  const dim3 blocks((M + threads - 1) / threads, N);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the CUDA kernel by increasing thread utilization through better block and thread configuration, and improve memory access patterns by coalescing reads and writes.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x - 1) / threads_x, (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimized algorithm: Use a 1D thread grid with coalesced memory access and vectorized loads/stores to maximize memory bandwidth utilization while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < M) {\n    scalar_t a_val = A[row];\n    scalar_t b_val = B[row * M + col];\n    C[row * M + col] = a_val * b_val;\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const dim3 threadsPerBlock(threads);\n  const dim3 blocks((M + threads - 1) / threads, N);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D grid layout with coalesced memory accesses and increased thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimized algorithm: Use a 1D grid-stride loop with coalesced memory access and increased occupancy by processing multiple elements per thread.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  int threads = 256;\n  int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory accesses and increasing thread utilization through a 1D grid-stride loop for columns while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (int col = threadIdx.x + blockIdx.x * blockDim.x; col < M; col += blockDim.x * gridDim.x) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  dim3 threads(256, 1);\n  dim3 blocks((M + threads.x - 1) / threads.x, N);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and increased thread utilization through proper block and grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x - 1) / threads_x, (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and increasing thread utilization by processing multiple elements per thread.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                  const scalar_t* __restrict__ B,\n                                  scalar_t* __restrict__ C,\n                                  int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col_start = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n  \n  if (row < N) {\n    scalar_t a_val = A[row];\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n      int col = col_start + i * blockDim.x;\n      if (col < M) {\n        C[row * M + col] = a_val * B[row * M + col];\n      }\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 32;\n  const dim3 threadsPerBlock(threads, 4);\n  const dim3 blocks((M + threads * 4 - 1) / (threads * 4), (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory accesses and maximizing memory bandwidth utilization through proper thread block and grid configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  // Optimized block size for coalesced memory access\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x - 1) / threads_x, (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the diagonal matrix multiplication by processing each row in parallel with coalesced memory access and improved thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (int col = 0; col < M; ++col) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D thread block layout and coalesced memory accesses, while maintaining correctness by ensuring each thread handles a single element of the output matrix.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D thread block layout and coalesced memory accesses to improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const dim3 threadsPerBlock(threads);\n  const dim3 blocks((M + threads - 1) / threads, N);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and increasing thread utilization with a 1D block layout instead of 2D.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimized algorithm uses coalesced memory access by having each thread handle a column of B for its assigned row, reducing global memory transactions and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (int col = 0; col < M; ++col) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const dim3 blocks((N + threads - 1) / threads);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the CUDA kernel by increasing thread utilization through vectorized memory accesses and adjusting block dimensions for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n  \n  if (row < N) {\n    scalar_t a_val = A[row];\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n      int current_col = col + i * blockDim.x;\n      if (current_col < M) {\n        C[row * M + current_col] = a_val * B[row * M + current_col];\n      }\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x * 4 - 1) / (threads_x * 4), (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the diagonal matrix multiplication by increasing thread utilization through better block and grid sizing, and using memory coalescing by having each thread handle multiple elements in a column-major order.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  \n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (; col < M; col += blockDim.x * gridDim.x) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  dim3 threads(32, 8);  // Better thread block configuration\n  dim3 blocks((M + threads.x - 1) / threads.x, \n             (N + threads.y - 1) / threads.y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the diagonal matrix multiplication by increasing memory access efficiency through coalesced reads and better thread utilization with a 1D block layout.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and maximizing memory bandwidth utilization through appropriate thread block and grid dimensions.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  \n  if (row < N && col < M) {\n    scalar_t a_val = A[row];\n    scalar_t b_val = B[row * M + col];\n    C[row * M + col] = a_val * b_val;\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x - 1) / threads_x, (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D grid-stride loop for better memory access patterns and increased parallelism, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  int num_threads = 256;\n  int num_blocks = (N * M + num_threads - 1) / num_threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<num_blocks, num_threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and increased thread utilization through a 1D block layout and strided indexing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and increased thread utilization with a 1D block layout.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the CUDA kernel by increasing thread utilization through better block and grid sizing, and improve memory access patterns by coalescing reads and writes.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                  const scalar_t* __restrict__ B,\n                                  scalar_t* __restrict__ C,\n                                  int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < M) {\n    scalar_t a_val = A[row];\n    scalar_t b_val = B[row * M + col];\n    C[row * M + col] = a_val * b_val;\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x - 1) / threads_x, (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the diagonal matrix multiplication by increasing thread utilization through better block and grid sizing, and using memory coalescing for efficient global memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  // Optimized block and grid dimensions\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x - 1) / threads_x, (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the diagonal matrix multiplication by coalescing memory accesses and using a 1D thread block layout to maximize memory bandwidth utilization while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the CUDA kernel by using a 1D grid-stride loop for better memory access patterns and increased occupancy, while maintaining the same mathematical operation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and increased thread utilization with a 1D block layout.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the CUDA kernel by increasing thread utilization through better block and grid dimensioning, and ensure coalesced memory access by having each thread handle multiple elements along the row dimension.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                  const scalar_t* __restrict__ B,\n                                  scalar_t* __restrict__ C,\n                                  int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col_start = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n  \n  if (row < N) {\n    scalar_t a_val = A[row];\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n      int col = col_start + i * blockDim.x;\n      if (col < M) {\n        C[row * M + col] = a_val * B[row * M + col];\n      }\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 8;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x * 4 - 1) / (threads_x * 4), (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and increasing thread utilization through a 1D grid-stride loop approach.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D grid and block layout that better matches the memory access pattern, reducing thread divergence and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory accesses and increasing thread utilization with a 1D block layout.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and increased thread utilization with a 1D block layout.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize memory access patterns by using coalesced reads and vectorized operations while maintaining correctness for diagonal matrix multiplication.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (int col = threadIdx.y; col < M; col += blockDim.y) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const dim3 threadsPerBlock(32, 4);  // Better memory access pattern\n  const dim3 blocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the diagonal matrix multiplication by increasing memory access efficiency through better thread utilization and coalesced memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (int col = threadIdx.y; col < M; col += blockDim.y) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const dim3 threadsPerBlock(32, 8);\n  const dim3 blocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory access and increasing thread utilization through a 1D grid-stride loop approach.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  int threads = 256;\n  int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize the diagonal matrix multiplication by increasing memory access efficiency through better thread utilization and memory coalescing, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (int col = 0; col < M; col++) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimized diagonal matrix multiplication by using coalesced memory access and increased thread utilization through proper block and grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y * blockDim.y + threadIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 4;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((M + threads_x - 1) / threads_x, (N + threads_y - 1) / threads_y);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory accesses and increased thread utilization through a 1D grid-stride loop pattern.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < N * M; i += stride) {\n    int row = i / M;\n    int col = i % M;\n    C[i] = A[row] * B[i];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  int threads = 256;\n  int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize the diagonal matrix multiplication by using a 1D grid and block layout to better utilize memory coalescing and reduce thread divergence, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N * M) {\n    int row = idx / M;\n    int col = idx % M;\n    C[idx] = A[row] * B[idx];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const int blocks = (N * M + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threads>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimize the diagonal matrix multiplication by increasing thread utilization through better block and grid sizing, and ensuring coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                  const scalar_t* __restrict__ B,\n                                  scalar_t* __restrict__ C,\n                                  int N, int M) {\n  int row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N) {\n    scalar_t a_val = A[row];\n    for (int col = threadIdx.y; col < M; col += blockDim.y) {\n      C[row * M + col] = a_val * B[row * M + col];\n    }\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  dim3 threadsPerBlock(32, 8);  // Better thread utilization\n  dim3 blocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimize the diagonal matrix multiplication by ensuring coalesced memory access and maximizing memory bandwidth utilization through proper thread block and grid configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.x * blockDim.y + threadIdx.y;\n  int col = threadIdx.x + blockIdx.y * blockDim.x * blockDim.y;\n  \n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads_x = 32;\n  const int threads_y = 8;\n  const dim3 threadsPerBlock(threads_x, threads_y);\n  const dim3 blocks((N + threads_y - 1) / threads_y, \n                    (M + threads_x * threads_y - 1) / (threads_x * threads_y));\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize the diagonal matrix multiplication by using coalesced memory accesses and increased thread utilization through a 1D block structure and vectorized loads/stores where possible.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void diag_matmul_kernel(const scalar_t* __restrict__ A,\n                                   const scalar_t* __restrict__ B,\n                                   scalar_t* __restrict__ C,\n                                   int N, int M) {\n  int row = blockIdx.y;\n  int col = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < N && col < M) {\n    C[row * M + col] = A[row] * B[row * M + col];\n  }\n}\n\ntorch::Tensor diag_matmul_cuda(torch::Tensor A, torch::Tensor B) {\n  const int N = A.size(0);\n  const int M = B.size(1);\n  auto C = torch::empty({N, M}, B.options());\n\n  const int threads = 256;\n  const dim3 threadsPerBlock(threads);\n  const dim3 blocks((M + threads - 1) / threads, N);\n\n  AT_DISPATCH_FLOATING_TYPES(B.scalar_type(), \"diag_matmul_cuda\", ([&] {\n    diag_matmul_kernel<scalar_t><<<blocks, threadsPerBlock>>>(\n      A.data_ptr<scalar_t>(),\n      B.data_ptr<scalar_t>(),\n      C.data_ptr<scalar_t>(),\n      N,\n      M\n    );\n  }));\n\n  return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &diag_matmul_cuda, \"Diagonal Matrix Multiplication (CUDA)\");\n}",
        "score": null
    }
]