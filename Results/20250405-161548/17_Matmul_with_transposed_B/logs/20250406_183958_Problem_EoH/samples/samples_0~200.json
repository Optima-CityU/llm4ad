[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of A and B, and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled = t * TILE_SIZE;\n        int a_col = tiled + threadIdx.y;\n        int b_row = tiled + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the matrix multiplication by using shared memory for tiling, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tiles into shared memory\n        if (row < M && (tiled_k + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + tiled_k + threadIdx.y];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && (tiled_k + threadIdx.x) < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + tiled_k + threadIdx.x];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute partial product with loop unrolling\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n            value += As[threadIdx.x][i+1] * Bs[threadIdx.y][i+1];\n            value += As[threadIdx.x][i+2] * Bs[threadIdx.y][i+2];\n            value += As[threadIdx.x][i+3] * Bs[threadIdx.y][i+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory for tiling, increasing thread block size for better occupancy, and unrolling the inner loop to reduce memory latency and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int loadArow = row;\n        int loadAcol = tiledK + threadIdx.y;\n        int loadBrow = col;\n        int loadBcol = tiledK + threadIdx.x;\n\n        if (loadArow < M && loadAcol < K) {\n            As[threadIdx.x][threadIdx.y] = A[loadArow * K + loadAcol];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (loadBrow < N && loadBcol < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[loadBrow * K + loadBcol];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n            if (i+1 < TILE_SIZE) value += As[threadIdx.x][i+1] * Bs[threadIdx.y][i+1];\n            if (i+2 < TILE_SIZE) value += As[threadIdx.x][i+2] * Bs[threadIdx.y][i+2];\n            if (i+3 < TILE_SIZE) value += As[threadIdx.x][i+3] * Bs[threadIdx.y][i+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimized using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled = t * TILE_SIZE;\n        int A_col = tiled + threadIdx.y;\n        int B_row = tiled + threadIdx.x;\n\n        As[threadIdx.x][threadIdx.y] = (row < M && A_col < K) ? A[row * K + A_col] : 0.0f;\n        Bs[threadIdx.x][threadIdx.y] = (col < N && B_row < K) ? B[col * K + B_row] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory for tiling and increasing thread block size to better utilize GPU resources while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the matrix multiplication by using shared memory for tiling, increasing thread block size, and unrolling the inner loop to reduce memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        \n        // Load tile from A\n        if (row < M && (tiledK + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiledK + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        // Load tile from B (transposed)\n        if (col < N && (tiledK + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + tiledK + threadIdx.y];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n\n        // Compute partial product with loop unrolling\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[threadIdx.y][i] * Bs[threadIdx.x][i];\n            value += As[threadIdx.y][i+1] * Bs[threadIdx.x][i+1];\n            value += As[threadIdx.y][i+2] * Bs[threadIdx.x][i+2];\n            value += As[threadIdx.y][i+3] * Bs[threadIdx.x][i+3];\n        }\n        \n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimized implementation using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + tiled_k];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[threadIdx.y][k];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimized using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = row;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = col;\n        int b_col = tiled_k + threadIdx.x;\n\n        if (a_row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[a_row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (b_row < N && b_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * K + b_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[threadIdx.y][k];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, and increase thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimized implementation using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled = t * TILE_SIZE;\n        int A_row = row;\n        int A_col = tiled + threadIdx.y;\n        int B_row = col;\n        int B_col = tiled + threadIdx.x;\n\n        if (A_row < M && A_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[A_row * K + A_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (B_row < N && B_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_row * K + B_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimized using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int aRow = row;\n        int aCol = tiledK + threadIdx.y;\n        int bRow = col;\n        int bCol = tiledK + threadIdx.x;\n\n        if (aRow < M && aCol < K) {\n            As[threadIdx.x][threadIdx.y] = A[aRow * K + aCol];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (bRow < N && bCol < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[bRow * K + bCol];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the matrix multiplication by using shared memory for tiling to reduce global memory accesses and increase memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_i];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + tiled_i];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of A and B, reducing global memory accesses, and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int a_col = tiledK + threadIdx.x;\n        int b_row = tiledK + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the matrix multiplication by using shared memory for tiling, increasing thread block size, and unrolling the inner loop for better memory access patterns and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int A_col = tiledK + threadIdx.y;\n        int B_row = tiledK + threadIdx.x;\n\n        if (row < M && A_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + A_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && B_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + B_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n            value += As[threadIdx.x][i+1] * Bs[threadIdx.y][i+1];\n            value += As[threadIdx.x][i+2] * Bs[threadIdx.y][i+2];\n            value += As[threadIdx.x][i+3] * Bs[threadIdx.y][i+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        if (row < M && t * TILE_SIZE + threadIdx.y < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + t * TILE_SIZE + threadIdx.y];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && t * TILE_SIZE + threadIdx.x < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + t * TILE_SIZE + threadIdx.x];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{M, N}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n            value += As[threadIdx.x][i+1] * Bs[threadIdx.y][i+1];\n            value += As[threadIdx.x][i+2] * Bs[threadIdx.y][i+2];\n            value += As[threadIdx.x][i+3] * Bs[threadIdx.y][i+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory for blocking and increasing thread block size for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int A_col = tiledK + threadIdx.y;\n        int B_row = tiledK + threadIdx.x;\n\n        if (row < M && A_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + A_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && B_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + B_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with improved thread block configuration and memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimized implementation using shared memory tiling to reduce global memory accesses and increase memory bandwidth utilization, with careful handling of boundary conditions.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + tiled_k];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[threadIdx.y][k];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the matrix multiplication kernel by using shared memory for tiling, increasing thread block size, and unrolling the inner loop for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n            if (i+1 < TILE_SIZE) value += As[threadIdx.x][i+1] * Bs[threadIdx.y][i+1];\n            if (i+2 < TILE_SIZE) value += As[threadIdx.x][i+2] * Bs[threadIdx.y][i+2];\n            if (i+3 < TILE_SIZE) value += As[threadIdx.x][i+3] * Bs[threadIdx.y][i+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the matrix multiplication by using shared memory for tiled computation and increasing thread block size for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (col < N && b_row < K) ? B[col * K + b_row] : 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimized using shared memory tiling to reduce global memory accesses and increase memory bandwidth utilization, with thread block size adjusted for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_col = tile_offset + threadIdx.y;\n        int b_row = tile_offset + threadIdx.x;\n\n        As[threadIdx.x][threadIdx.y] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.x][threadIdx.y] = (col < N && b_row < K) ? B[col * K + b_row] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the matrix multiplication by using shared memory for tiling and increasing thread block size to better utilize GPU resources while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the matmul kernel by using shared memory for tiling, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.x][k] * Bs[threadIdx.y][k];\n            if (k + 1 < TILE_SIZE) value += As[threadIdx.x][k+1] * Bs[threadIdx.y][k+1];\n            if (k + 2 < TILE_SIZE) value += As[threadIdx.x][k+2] * Bs[threadIdx.y][k+2];\n            if (k + 3 < TILE_SIZE) value += As[threadIdx.x][k+3] * Bs[threadIdx.y][k+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimized implementation using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + threadIdx.y;\n        int B_row = tiled_k + threadIdx.x;\n\n        if (row < M && A_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + A_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (B_row < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + B_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimized implementation using shared memory tiling to reduce global memory accesses and improve memory coalescing, with block size tuned for RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        if (row < M && t * TILE_SIZE + threadIdx.y < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + t * TILE_SIZE + threadIdx.y];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && t * TILE_SIZE + threadIdx.x < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + t * TILE_SIZE + threadIdx.x];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with improved memory access patterns and thread block configuration.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimized implementation using shared memory for tile-based matrix multiplication with improved memory access patterns and thread block configuration.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + tiled_k];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the matrix multiplication by using shared memory for tiled data access and increasing thread block size for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimized implementation using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int load_row = row;\n        int load_col = tiled_k + threadIdx.y;\n        if (load_row < M && load_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[load_row * K + load_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        load_row = tiled_k + threadIdx.x;\n        load_col = col;\n        if (load_row < K && load_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[load_col * K + load_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimized implementation using shared memory for tiling to reduce global memory accesses and increase memory bandwidth utilization, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int A_col = tiledK + threadIdx.x;\n        int B_row = tiledK + threadIdx.y;\n\n        if (row < M && A_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + B_row];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int aRow = row;\n        int aCol = tiledK + threadIdx.y;\n        int bRow = col;\n        int bCol = tiledK + threadIdx.x;\n\n        if (aRow < M && aCol < K) {\n            As[threadIdx.x][threadIdx.y] = A[aRow * K + aCol];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (bRow < N && bCol < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[bRow * K + bCol];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_col = tile_offset + threadIdx.y;\n        int b_row = tile_offset + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimized implementation using shared memory for tiling and increased thread block size for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimized using tiling with shared memory to reduce global memory accesses and improve memory coalescing, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with improved memory access patterns and thread block configuration for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + tiled_k];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + tiled_k];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[threadIdx.y][k];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + tiled_i];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + tiled_i];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimized implementation using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimized implementation using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimized implementation using shared memory for tiling, loop unrolling, and increased thread block size for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + tiled_k + threadIdx.y];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && (tiled_k + threadIdx.x) < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + tiled_k + threadIdx.x];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.x][k] * Bs[threadIdx.y][k];\n            value += As[threadIdx.x][k+1] * Bs[threadIdx.y][k+1];\n            value += As[threadIdx.x][k+2] * Bs[threadIdx.y][k+2];\n            value += As[threadIdx.x][k+3] * Bs[threadIdx.y][k+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize the matrix multiplication by using shared memory for tiling and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + tiled_k];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[threadIdx.y][k];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimized implementation using shared memory for tiling and increased thread block size for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        if (row < M && t * TILE_SIZE + threadIdx.y < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + t * TILE_SIZE + threadIdx.y];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && t * TILE_SIZE + threadIdx.x < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + t * TILE_SIZE + threadIdx.x];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int A_col = tiledK + threadIdx.y;\n        int B_row = tiledK + threadIdx.x;\n\n        if (row < M && A_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + A_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && B_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + B_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + b_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimized the matrix multiplication using shared memory tiling to reduce global memory accesses and improve memory coalescing, with a tile size of 32x32.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + tiled_k];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[threadIdx.y][k];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimized the kernel by using shared memory for tiled matrix multiplication with block size 32x32 and loop unrolling for better memory access patterns and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        if (row < M && t * TILE_SIZE + threadIdx.x < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && t * TILE_SIZE + threadIdx.y < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[col * K + t * TILE_SIZE + threadIdx.y];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float *A, const float *B, float *C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + tiled_k];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_forward(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(0);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_forward, \"Matrix multiplication with transposed B\");\n}",
        "score": null
    }
]