[
    {
        "sample_order": 1,
        "algorithm": "{Optimize by parallelizing mean and variance computation per instance (n,c) using shared memory and warp-level reductions, then normalize all elements in the instance.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    int n = blockIdx.y;\n    int c = blockIdx.z;\n    int tid = threadIdx.x;\n    \n    // Compute mean and variance for this instance (n,c)\n    float mean = 0.0f;\n    float var = 0.0f;\n    int offset = (n * C + c) * H * W;\n    \n    // First pass: compute mean\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    mean = warpReduceSum(mean);\n    if (tid % 32 == 0) {\n        smem[tid / 32] = mean;\n    }\n    __syncthreads();\n    \n    if (tid < 32) {\n        mean = (tid < (blockDim.x + 31) / 32) ? smem[tid] : 0.0f;\n        mean = warpReduceSum(mean);\n    }\n    mean /= (H * W);\n    \n    // Second pass: compute variance\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    var = warpReduceSum(var);\n    if (tid % 32 == 0) {\n        smem[tid / 32] = var;\n    }\n    __syncthreads();\n    \n    if (tid < 32) {\n        var = (tid < (blockDim.x + 31) / 32) ? smem[tid] : 0.0f;\n        var = warpReduceSum(var);\n    }\n    var /= (H * W);\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int h = i / W;\n        int w = i % W;\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(1, N, C);\n    int threads = min(1024, H * W);\n    size_t smem_size = ((threads + 31) / 32) * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize by processing each instance (n,c) in parallel, computing mean and variance once per instance using shared memory, then normalizing all elements in the instance.}",
        "function": "1. {Optimize by processing each instance (n,c) in parallel, computing mean and variance once per instance using shared memory, then normalizing all elements in the instance.}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared_data[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    shared_data[tid] = mean;\n    __syncthreads();\n    \n    // Parallel reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = shared_data[0] / (H * W);\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    shared_data[tid] = var;\n    __syncthreads();\n    \n    // Parallel reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    var = shared_data[0] / (H * W);\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(C, N);\n    int threads = 256;\n    size_t shared_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize by parallelizing across instances (N,C) and using shared memory for mean/variance computation within each instance, reducing redundant calculations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared[];\n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    shared[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = shared[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    shared[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    var = shared[0] / (H * W);\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n               \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n               \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(N, C);\n    int threads = min(1024, H * W);\n    size_t shared_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7026240050792694
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize by parallelizing mean and variance computation per instance (n,c) using shared memory and warp-level reductions, then normalizing all elements in the instance.}",
        "function": "1. {Optimize by parallelizing mean and variance computation per instance (n,c) using shared memory and warp-level reductions, then normalizing all elements in the instance.}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    int instance_size = H * W;\n    int offset = (n * C + c) * instance_size;\n\n    // Compute mean using parallel reduction\n    float mean = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    mean = blockReduceSum(mean);\n    if (tid == 0) {\n        mean /= instance_size;\n        sdata[0] = mean;\n    }\n    __syncthreads();\n    mean = sdata[0];\n\n    // Compute variance using parallel reduction\n    float var = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    var = blockReduceSum(var);\n    if (tid == 0) {\n        var /= instance_size;\n        sdata[0] = var;\n    }\n    __syncthreads();\n    var = sdata[0];\n\n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\n// Helper function for block-level reduction\n__device__ float blockReduceSum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    val = warpReduceSum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;\n    if (wid == 0) val = warpReduceSum(val);\n    return val;\n}\n\n// Helper function for warp-level reduction\n__device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n```",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize by parallelizing over instances (N,C) and using shared memory for per-instance statistics, then applying normalization across spatial dimensions (H,W).}",
        "function": "1. {Optimize by parallelizing over instances (N,C) and using shared memory for per-instance statistics, then applying normalization across spatial dimensions (H,W).}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    \n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    int spatial_size = H * W;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < spatial_size; i += blockDim.x) {\n        mean += x[(n * C + c) * spatial_size + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Parallel reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / spatial_size;\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < spatial_size; i += blockDim.x) {\n        float diff = x[(n * C + c) * spatial_size + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Parallel reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / spatial_size;\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < spatial_size; i += blockDim.x) {\n        int idx = (n * C + c) * spatial_size + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(N, C);\n    int threads = 256;\n    size_t shared_mem = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize by parallelizing over instances (n,c) and using shared memory for mean/variance computation within each instance, then applying normalization across spatial dimensions.}",
        "function": "1. {Optimize by parallelizing over instances (n,c) and using shared memory for mean/variance computation within each instance, then applying normalization across spatial dimensions.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float s_data[];\n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    int spatial_size = H * W;\n    int offset = (n * C + c) * spatial_size;\n    \n    // Compute mean using parallel reduction\n    float mean = 0.0f;\n    for (int i = tid; i < spatial_size; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    s_data[tid] = mean;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            s_data[tid] += s_data[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = s_data[0] / spatial_size;\n    __syncthreads();\n    \n    // Compute variance using parallel reduction\n    float var = 0.0f;\n    for (int i = tid; i < spatial_size; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    s_data[tid] = var;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            s_data[tid] += s_data[tid + s];\n        }\n        __syncthreads();\n    }\n    var = s_data[0] / spatial_size;\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < spatial_size; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n    int spatial_size = H * W;\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(N, C);\n    int threads = min(1024, ((spatial_size + 31) / 32) * 32);\n    size_t shared_mem = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize by parallelizing per instance (n,c) with shared memory for mean/var computation, then normalizing all spatial elements (h,w) for that instance in parallel.}",
        "function": "{Optimize by parallelizing per instance (n,c) with shared memory for mean/var computation, then normalizing all spatial elements (h,w) for that instance in parallel.}\n\nHere's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel_optimized(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    int offset = (n * C + c) * H * W;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / (H * W);\n    \n    // Normalize all elements for this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int h = i / W;\n        int w = i % W;\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n```",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using warp-level reductions, then normalizing all elements in the instance.}",
        "function": "#include <cooperative_groups.h>\n#include <cooperative_groups/reduce.h>\nnamespace cg = cooperative_groups;\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    if (n >= N || c >= C) return;\n    \n    int offset = (n * C + c) * H * W;\n    const float* instance = x + offset;\n    \n    // Compute mean using warp reduction\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += instance[i];\n    }\n    \n    cg::thread_block tb = cg::this_thread_block();\n    mean = cg::reduce(tb, mean, cg::plus<float>());\n    mean /= (H * W);\n    \n    // Compute variance using warp reduction\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = instance[i] - mean;\n        var += diff * diff;\n    }\n    \n    var = cg::reduce(tb, var, cg::plus<float>());\n    var /= (H * W);\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    float weight = inorm_weight[c];\n    float bias = inorm_bias[c];\n    \n    for (int i = tid; i < H * W; i += blockDim.x) {\n        out[offset + i] = (instance[i] - mean) * inv_std * weight + bias;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize by parallelizing per instance (n,c) and using shared memory for mean/variance computation within each block, then applying normalization across spatial dimensions.}",
        "function": "1. {Optimize by parallelizing per instance (n,c) and using shared memory for mean/variance computation within each block, then applying normalization across spatial dimensions.}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int n = blockIdx.x / C;\n    int c = blockIdx.x % C;\n    int tid = threadIdx.x;\n    int offset = (n * C + c) * H * W;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / (H * W);\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    float inv_std = rsqrtf(sdata[0] / (H * W) + 1e-5f);\n    \n    // Normalize\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int h = i / W;\n        int w = i % W;\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n```",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Compute instance normalization by first calculating mean and variance per channel in parallel using shared memory for intermediate results, then applying normalization with weight and bias.}",
        "function": "1. {Compute instance normalization by first calculating mean and variance per channel in parallel using shared memory for intermediate results, then applying normalization with weight and bias.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared_data[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Phase 1: Compute mean and variance per instance (n,c)\n    float mean = 0.0f;\n    float var = 0.0f;\n    \n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float val = x[(n * C + c) * H * W + i];\n        mean += val;\n        var += val * val;\n    }\n    \n    shared_data[tid] = mean;\n    shared_data[blockDim.x + tid] = var;\n    __syncthreads();\n    \n    // Reduction\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n            shared_data[blockDim.x + tid] += shared_data[blockDim.x + tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        mean = shared_data[0] / (H * W);\n        var = shared_data[blockDim.x] / (H * W) - mean * mean;\n        shared_data[0] = mean;\n        shared_data[1] = var;\n    }\n    __syncthreads();\n    \n    mean = shared_data[0];\n    float inv_std = rsqrtf(shared_data[1] + 1e-5f);\n    \n    // Phase 2: Normalize\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(C, N);\n    int threads = min(1024, H * W);\n    size_t shared_size = 2 * threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory for intermediate results, then normalize each element with weight and bias.}",
        "function": "1. {Optimize by computing mean and variance per instance (n,c) in parallel using shared memory for intermediate results, then normalize each element with weight and bias.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int n = blockIdx.y;\n    int c = blockIdx.z;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / (H * W);\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / (H * W);\n    \n    // Normalize elements\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 threads(256);\n    dim3 blocks(1, N, C);\n    size_t shmem_size = threads.x * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shmem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize by parallelizing over instances (N,C) and using shared memory to compute mean/variance per instance, then normalizing all spatial elements (H,W) for each instance in parallel.}",
        "function": "1. {Optimize by parallelizing over instances (N,C) and using shared memory to compute mean/variance per instance, then normalizing all spatial elements (H,W) for each instance in parallel.}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    \n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    int spatial_size = H * W;\n    int offset = (n * C + c) * spatial_size;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < spatial_size; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    for (int s = blockDim.x/2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / spatial_size;\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < spatial_size; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    for (int s = blockDim.x/2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / spatial_size;\n    float inv_std = rsqrtf(var + 1e-5f);\n    \n    // Normalize all elements for this instance\n    for (int i = tid; i < spatial_size; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(N, C);\n    int threads = min(1024, H * W);\n    size_t shared_mem = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and cooperative groups for better memory access patterns and reduced redundant calculations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared[];\n\n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    int pixel_count = H * W;\n    int instance_offset = (n * C + c) * pixel_count;\n\n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < pixel_count; i += blockDim.x) {\n        mean += x[instance_offset + i];\n    }\n\n    cg::thread_block block = cg::this_thread_block();\n    mean = cg::reduce(block, mean, cg::plus<float>());\n    if (block.thread_rank() == 0) {\n        shared[0] = mean / pixel_count;\n    }\n    block.sync();\n    mean = shared[0];\n\n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < pixel_count; i += blockDim.x) {\n        float diff = x[instance_offset + i] - mean;\n        var += diff * diff;\n    }\n\n    var = cg::reduce(block, var, cg::plus<float>());\n    if (block.thread_rank() == 0) {\n        shared[0] = var / pixel_count;\n    }\n    block.sync();\n    var = shared[0];\n\n    // Normalize all pixels in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < pixel_count; i += blockDim.x) {\n        int idx = instance_offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(C, N);\n    int threads = min(1024, H * W);\n    int shared_mem = sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory for intermediate sums, then normalize all elements in the instance using the computed statistics.}",
        "function": "1. {Optimize by computing mean and variance per instance (n,c) in parallel using shared memory for intermediate sums, then normalize all elements in the instance using the computed statistics.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    int instance_size = H * W;\n    int offset = (n * C + c) * instance_size;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    \n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Parallel reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        sdata[0] /= instance_size;\n    }\n    __syncthreads();\n    mean = sdata[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    \n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Parallel reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        sdata[0] /= instance_size;\n    }\n    __syncthreads();\n    float inv_std = rsqrtf(sdata[0] + 1e-5f);\n    \n    // Normalize all elements in this instance\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n    \n    auto out = at::empty_like(x);\n    \n    dim3 blocks(C, N);\n    int threads = 256;\n    size_t shared_mem = threads * sizeof(float);\n    \n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n    \n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize by parallelizing over instances (N,C) and using shared memory for mean/variance computation within each instance, then normalizing all elements in the instance with coalesced memory access.}",
        "function": "{Optimize by parallelizing over instances (N,C) and using shared memory for mean/variance computation within each instance, then normalizing all elements in the instance with coalesced memory access.}\n\nHere's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float s_data[];\n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    int offset = (n * C + c) * H * W;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    s_data[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            s_data[tid] += s_data[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = s_data[0] / (H * W);\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    s_data[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            s_data[tid] += s_data[tid + s];\n        }\n        __syncthreads();\n    }\n    var = s_data[0] / (H * W);\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n```",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using shared memory, then normalizing all spatial elements (H,W) for each instance in parallel.}",
        "function": "1. {Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using shared memory, then normalizing all spatial elements (H,W) for each instance in parallel.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared_data[];\n    \n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    \n    shared_data[tid] = mean;\n    __syncthreads();\n    \n    // Parallel reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        shared_data[0] /= (H * W);\n    }\n    __syncthreads();\n    mean = shared_data[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    \n    shared_data[tid] = var;\n    __syncthreads();\n    \n    // Parallel reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        shared_data[0] /= (H * W);\n    }\n    __syncthreads();\n    float inv_std = rsqrtf(shared_data[0] + 1e-5f);\n    \n    // Normalize all elements for this instance\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(N, C);\n    int threads = min(1024, H * W);\n    size_t shared_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using shared memory, then normalizing all spatial elements (H,W) for each instance in parallel.}",
        "function": "1. {Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using shared memory, then normalizing all spatial elements (H,W) for each instance in parallel.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared[];\n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    int offset = (n * C + c) * H * W;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    shared[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = shared[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    shared[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    var = shared[0] / (H * W);\n    \n    // Normalize all elements for this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n```",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory for intermediate sums, then normalize all elements in the instance using these precomputed statistics.}",
        "function": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory for intermediate sums, then normalize all elements in the instance using these precomputed statistics.}\n\nHere's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int nc = blockIdx.x;\n    int n = nc / C;\n    int c = nc % C;\n    int tid = threadIdx.x;\n    int offset = (n * C + c) * H * W;\n\n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / (H * W);\n\n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / (H * W);\n    float inv_std = rsqrtf(var + 1e-5f);\n\n    // Normalize all elements in this instance\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n```",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using shared memory, then normalizing all spatial elements (H,W) for each instance in parallel.}",
        "function": "1. {Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using shared memory, then normalizing all spatial elements (H,W) for each instance in parallel.}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / (H * W);\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(N, C);\n    int threads = min(1024, H * W);\n    size_t shared_mem = threads * sizeof(float);\n    \n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize by parallelizing mean and variance computations per instance (n,c) using shared memory for intermediate results, then normalizing all elements in the instance.}",
        "function": "1. {Optimize by parallelizing mean and variance computations per instance (n,c) using shared memory for intermediate results, then normalizing all elements in the instance.}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    \n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    int instance_size = H * W;\n    int offset = (n * C + c) * instance_size;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / instance_size;\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / instance_size;\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n```",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp reductions, then normalize all elements in the instance using coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int c = blockIdx.x;\n    int n = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    \n    // Warp reduction for mean\n    for (int offset = 16; offset > 0; offset /= 2) {\n        mean += __shfl_down_sync(0xffffffff, mean, offset);\n    }\n    \n    if (tid == 0) {\n        sdata[0] = mean / (H * W);\n    }\n    __syncthreads();\n    mean = sdata[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    \n    // Warp reduction for variance\n    for (int offset = 16; offset > 0; offset /= 2) {\n        var += __shfl_down_sync(0xffffffff, var, offset);\n    }\n    \n    if (tid == 0) {\n        sdata[0] = var / (H * W);\n    }\n    __syncthreads();\n    float inv_std = rsqrtf(sdata[0] + 1e-5f);\n    \n    // Normalize all elements in this instance\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(C, N);\n    int threads = min(1024, H * W);\n    \n    instance_norm_kernel<<<blocks, threads, sizeof(float)>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Compute instance normalization by first calculating mean and variance per instance-channel using parallel reduction, then normalizing each element using the computed statistics.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <unsigned blockSize>\n__device__ void warpReduce(volatile float* sdata, unsigned tid) {\n    if (blockSize >= 64) sdata[tid] += sdata[tid + 32];\n    if (blockSize >= 32) sdata[tid] += sdata[tid + 16];\n    if (blockSize >= 16) sdata[tid] += sdata[tid + 8];\n    if (blockSize >= 8) sdata[tid] += sdata[tid + 4];\n    if (blockSize >= 4) sdata[tid] += sdata[tid + 2];\n    if (blockSize >= 2) sdata[tid] += sdata[tid + 1];\n}\n\ntemplate <unsigned blockSize>\n__global__ void instance_norm_mean_var_kernel(\n    const float* __restrict__ x,\n    float* __restrict__ mean,\n    float* __restrict__ var,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    unsigned tid = threadIdx.x;\n    unsigned n = blockIdx.x / C;\n    unsigned c = blockIdx.x % C;\n    unsigned offset = (n * C + c) * H * W;\n    \n    // Compute mean\n    float sum = 0.0f;\n    for (unsigned i = tid; i < H * W; i += blockDim.x) {\n        sum += x[offset + i];\n    }\n    sdata[tid] = sum;\n    __syncthreads();\n    \n    if (blockSize >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }\n    if (blockSize >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }\n    if (blockSize >= 128) { if (tid < 64) { sdata[tid] += sdata[tid + 64]; } __syncthreads(); }\n    if (tid < 32) warpReduce<blockSize>(sdata, tid);\n    \n    if (tid == 0) {\n        mean[blockIdx.x] = sdata[0] / (H * W);\n    }\n    \n    // Compute variance\n    float m = mean[blockIdx.x];\n    sum = 0.0f;\n    for (unsigned i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - m;\n        sum += diff * diff;\n    }\n    sdata[tid] = sum;\n    __syncthreads();\n    \n    if (blockSize >= 512) { if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads(); }\n    if (blockSize >= 256) { if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads(); }\n    if (blockSize >= 128) { if (tid < 64) { sdata[tid] += sdata[tid + 64]; } __syncthreads(); }\n    if (tid < 32) warpReduce<blockSize>(sdata, tid);\n    \n    if (tid == 0) {\n        var[blockIdx.x] = sdata[0] / (H * W);\n    }\n}\n\n__global__ void instance_norm_apply_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ mean,\n    const float* __restrict__ var,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int total = N * C * H * W;\n    \n    if (index < total) {\n        int n = index / (C * H * W);\n        int c = (index % (C * H * W)) / (H * W);\n        int h = (index % (H * W)) / W;\n        int w = index % W;\n        \n        int nc = n * C + c;\n        float inv_std = rsqrtf(var[nc] + 1e-5f);\n        out[index] = (x[index] - mean[nc]) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    auto mean = at::empty({N * C}, x.options());\n    auto var = at::empty({N * C}, x.options());\n\n    const int threads = 512;\n    const int blocks = N * C;\n    \n    instance_norm_mean_var_kernel<512><<<blocks, threads, threads * sizeof(float)>>>(\n        x.data_ptr<float>(),\n        mean.data_ptr<float>(),\n        var.data_ptr<float>(),\n        N, C, H, W\n    );\n    \n    const int apply_threads = 256;\n    const int apply_blocks = (N * C * H * W + apply_threads - 1) / apply_threads;\n    \n    instance_norm_apply_kernel<<<apply_blocks, apply_threads>>>(\n        x.data_ptr<float>(),\n        mean.data_ptr<float>(),\n        var.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory for intermediate results, then normalizing all elements in the instance.}",
        "function": "1. {Optimize by computing mean and variance per instance (n,c) in parallel using shared memory for intermediate results, then normalizing all elements in the instance.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    int instance_size = H * W;\n    int offset = (n * C + c) * instance_size;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / instance_size;\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / instance_size;\n    \n    // Normalize all elements in the instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        out[offset + i] = (x[offset + i] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    int threads = 256;\n    dim3 blocks(C, N);\n    int shared_mem = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize by parallelizing across instances (N,C) and using shared memory for mean/variance computation within each instance, then applying normalization element-wise.}",
        "function": "1. {Optimize by parallelizing across instances (N,C) and using shared memory for mean/variance computation within each instance, then applying normalization element-wise.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared_data[];\n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    \n    shared_data[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = shared_data[0] / (H * W);\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    \n    shared_data[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    var = shared_data[0] / (H * W);\n    \n    // Normalize elements\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(N, C);\n    int threads = 256;\n    size_t shared_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using shared memory, then normalizing all spatial elements (H,W) for each instance in parallel.}",
        "function": "1. {Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using shared memory, then normalizing all spatial elements (H,W) for each instance in parallel.}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    \n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    int pixel_count = H * W;\n    int instance_offset = (n * C + c) * pixel_count;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < pixel_count; i += blockDim.x) {\n        mean += x[instance_offset + i];\n    }\n    \n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Parallel reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        mean = sdata[0] / pixel_count;\n        sdata[0] = mean;\n    }\n    __syncthreads();\n    mean = sdata[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < pixel_count; i += blockDim.x) {\n        float diff = x[instance_offset + i] - mean;\n        var += diff * diff;\n    }\n    \n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Parallel reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        var = sdata[0] / pixel_count;\n        sdata[0] = var;\n    }\n    __syncthreads();\n    var = sdata[0];\n    \n    // Normalize all elements for this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    float weight = inorm_weight[c];\n    float bias = inorm_bias[c];\n    \n    for (int i = tid; i < pixel_count; i += blockDim.x) {\n        out[instance_offset + i] = (x[instance_offset + i] - mean) * inv_std * weight + bias;\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(N, C);\n    int threads = min(1024, H * W);\n    int shared_mem = threads * sizeof(float);\n    \n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using shared memory, then normalizing all spatial elements (H,W) for each instance in parallel.}",
        "function": "1. {Optimize by parallelizing over instances (N,C) and computing mean/variance per instance using shared memory, then normalizing all spatial elements (H,W) for each instance in parallel.}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared_data[];\n    \n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    float* mean_shared = shared_data;\n    float* var_shared = shared_data + 1;\n    \n    // Compute mean and variance for this instance (n,c)\n    float mean = 0.0f;\n    float var = 0.0f;\n    \n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        float val = x[idx];\n        mean += val;\n        var += val * val;\n    }\n    \n    // Parallel reduction for mean and var\n    mean = blockReduceSum(mean);\n    var = blockReduceSum(var);\n    \n    if (tid == 0) {\n        mean /= (H * W);\n        var = var / (H * W) - mean * mean;\n        mean_shared[0] = mean;\n        var_shared[0] = var;\n    }\n    __syncthreads();\n    \n    mean = mean_shared[0];\n    var = var_shared[0];\n    float inv_std = rsqrtf(var + 1e-5f);\n    float weight = inorm_weight[c];\n    float bias = inorm_bias[c];\n    \n    // Normalize all elements for this instance\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * weight + bias;\n    }\n}\n\n// Helper function for block reduction\n__device__ float blockReduceSum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n    \n    val = warpReduceSum(val);\n    \n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n    \n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;\n    if (wid == 0) val = warpReduceSum(val);\n    return val;\n}\n\n__device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n```",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int nc = blockIdx.x;\n    int n = nc / C;\n    int c = nc % C;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    \n    // Warp-level reduction\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        mean += __shfl_down_sync(0xffffffff, mean, offset);\n    }\n    \n    if (threadIdx.x == 0) {\n        mean /= (H * W);\n        sdata[0] = mean;\n    }\n    __syncthreads();\n    mean = sdata[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    \n    // Warp-level reduction\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        var += __shfl_down_sync(0xffffffff, var, offset);\n    }\n    \n    if (threadIdx.x == 0) {\n        var /= (H * W);\n        sdata[1] = var;\n    }\n    __syncthreads();\n    var = sdata[1];\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(N * C);\n    dim3 threads(min(1024, H * W));\n    size_t shared_mem = 2 * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize by parallelizing mean and variance computation per instance (n,c) using shared memory for intermediate results, then normalize all elements in the instance using the computed statistics.}",
        "function": "1. {Optimize by parallelizing mean and variance computation per instance (n,c) using shared memory for intermediate results, then normalize all elements in the instance using the computed statistics.}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int n = blockIdx.y;\n    int c = blockIdx.z;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    int offset = (n * C + c) * H * W;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / (H * W);\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(1, N, C);\n    int threads = min(1024, H * W);\n    int shared_mem = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory for intermediate results, then normalize all elements in the instance.}",
        "function": "1. {Optimize by computing mean and variance per instance (n,c) in parallel using shared memory for intermediate results, then normalize all elements in the instance.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int instance_size = H * W;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        mean += x[(n * C + c) * instance_size + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / instance_size;\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        float diff = x[(n * C + c) * instance_size + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / instance_size;\n    \n    // Normalize all elements in the instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        int idx = (n * C + c) * instance_size + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n```",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize by parallelizing over instances (N,C) and using shared memory for mean/variance computation within each instance, then applying normalization across spatial dimensions (H,W).}",
        "function": "1. {Optimize by parallelizing over instances (N,C) and using shared memory for mean/variance computation within each instance, then applying normalization across spatial dimensions (H,W).}\n\n2. Here's the optimized kernel:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    int offset = (n * C + c) * H * W;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / (H * W);\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n```",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize by parallelizing mean and variance computation per instance (n,c) using shared memory and warp-level reductions, then normalize all elements in the instance.}",
        "function": "#include <cooperative_groups.h>\n#include <cooperative_groups/reduce.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    \n    int c = blockIdx.x;\n    int n = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    int instance_size = H * W;\n    int offset = (n * C + c) * instance_size;\n    \n    // Compute mean using warp-level reduction\n    float mean = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    \n    cg::thread_block block = cg::this_thread_block();\n    mean = cg::reduce(block, mean, cg::plus<float>());\n    if (block.thread_rank() == 0) {\n        smem[0] = mean / instance_size;\n    }\n    block.sync();\n    mean = smem[0];\n    \n    // Compute variance using warp-level reduction\n    float var = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    \n    var = cg::reduce(block, var, cg::plus<float>());\n    if (block.thread_rank() == 0) {\n        smem[1] = var / instance_size;\n    }\n    block.sync();\n    var = smem[1];\n    \n    // Normalize all elements in this instance\n    float inv_std = rsqrtf(var + 1e-5f);\n    float weight = inorm_weight[c];\n    float bias = inorm_bias[c];\n    \n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        out[offset + i] = (x[offset + i] - mean) * inv_std * weight + bias;\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(C, N);\n    int threads = min(1024, H * W);\n    size_t smem_size = 2 * sizeof(float); // for mean and var\n    \n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance using the computed statistics.}",
        "function": "#include <cooperative_groups.h>\n#include <cooperative_groups/reduce.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    int instance_size = H * W;\n    int offset = (n * C + c) * instance_size;\n    \n    // Compute mean using warp-level reduction\n    float mean = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    \n    cg::thread_block tb = cg::this_thread_block();\n    mean = cg::reduce(tb, mean, cg::plus<float>());\n    if (tb.thread_rank() == 0) {\n        smem[0] = mean / instance_size;\n    }\n    tb.sync();\n    mean = smem[0];\n    \n    // Compute variance using warp-level reduction\n    float var = 0.0f;\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    \n    var = cg::reduce(tb, var, cg::plus<float>());\n    if (tb.thread_rank() == 0) {\n        smem[1] = var / instance_size;\n    }\n    tb.sync();\n    float inv_std = rsqrtf(smem[1] + 1e-5f);\n    \n    // Normalize all elements in the instance\n    for (int i = tid; i < instance_size; i += blockDim.x) {\n        int h = i / W;\n        int w = i % W;\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(C, N);\n    int threads = min(1024, H * W);\n    size_t smem_size = 2 * sizeof(float); // For mean and var\n    \n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.z;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    mean = warpReduceSum(mean);\n    if (tid % 32 == 0) {\n        smem[tid / 32] = mean;\n    }\n    __syncthreads();\n    \n    if (tid < 32) {\n        mean = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        mean = warpReduceSum(mean);\n        if (tid == 0) smem[0] = mean / (H * W);\n    }\n    __syncthreads();\n    mean = smem[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    var = warpReduceSum(var);\n    if (tid % 32 == 0) {\n        smem[tid / 32] = var;\n    }\n    __syncthreads();\n    \n    if (tid < 32) {\n        var = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        var = warpReduceSum(var);\n        if (tid == 0) smem[1] = var / (H * W);\n    }\n    __syncthreads();\n    var = smem[1];\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(1, N, C);\n    int threads = min(1024, H * W);\n    int smem_size = ((threads + 31) / 32) * sizeof(float) * 2;\n\n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7137279987335206
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance.}",
        "function": "#include <cooperative_groups.h>\n#include <cooperative_groups/reduce.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    int c = blockIdx.x;\n    int n = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + 1;\n    \n    int offset = (n * C + c) * H * W;\n    \n    // Compute mean and variance using warp-level reductions\n    float mean = 0.0f;\n    float var = 0.0f;\n    \n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float val = x[offset + i];\n        mean += val;\n        var += val * val;\n    }\n    \n    cg::thread_block tb = cg::this_thread_block();\n    mean = cg::reduce(tb, mean, cg::plus<float>());\n    var = cg::reduce(tb, var, cg::plus<float>());\n    \n    if (tb.thread_rank() == 0) {\n        mean /= (H * W);\n        var = var / (H * W) - mean * mean;\n        mean_shared[0] = mean;\n        var_shared[0] = var;\n    }\n    tb.sync();\n    \n    mean = mean_shared[0];\n    float inv_std = rsqrtf(var_shared[0] + 1e-5f);\n    \n    // Normalize all elements in this instance\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int h = i / W;\n        int w = i % W;\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(C, N);\n    int threads = min(1024, H * W);\n    size_t smem_size = 2 * sizeof(float);\n    \n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Compute instance normalization by first calculating mean and variance for each instance-channel pair in parallel, then normalizing each element using these precomputed statistics.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void compute_stats_kernel(\n    const float* __restrict__ x,\n    float* __restrict__ means,\n    float* __restrict__ vars,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared[];\n    int nc = blockIdx.x;\n    int n = nc / C;\n    int c = nc % C;\n    int offset = (n * C + c) * H * W;\n    \n    float mean = 0.0f;\n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    \n    float* sdata = shared;\n    sdata[threadIdx.x] = mean;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (threadIdx.x < s) {\n            sdata[threadIdx.x] += sdata[threadIdx.x + s];\n        }\n        __syncthreads();\n    }\n    \n    if (threadIdx.x == 0) {\n        means[nc] = sdata[0] / (H * W);\n    }\n    \n    __syncthreads();\n    float m = means[nc];\n    float var = 0.0f;\n    \n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - m;\n        var += diff * diff;\n    }\n    \n    sdata[threadIdx.x] = var;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (threadIdx.x < s) {\n            sdata[threadIdx.x] += sdata[threadIdx.x + s];\n        }\n        __syncthreads();\n    }\n    \n    if (threadIdx.x == 0) {\n        vars[nc] = sdata[0] / (H * W);\n    }\n}\n\n__global__ void normalize_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ means,\n    const float* __restrict__ vars,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int total = N * C * H * W;\n    \n    if (index < total) {\n        int n = index / (C * H * W);\n        int c = (index % (C * H * W)) / (H * W);\n        int nc = n * C + c;\n        \n        float inv_std = rsqrtf(vars[nc] + 1e-5f);\n        out[index] = (x[index] - means[nc]) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    auto means = at::empty({N * C}, x.options());\n    auto vars = at::empty({N * C}, x.options());\n\n    int threads = 256;\n    int blocks = N * C;\n    \n    compute_stats_kernel<<<blocks, threads, threads * sizeof(float)>>>(\n        x.data_ptr<float>(),\n        means.data_ptr<float>(),\n        vars.data_ptr<float>(),\n        N, C, H, W\n    );\n    \n    int total = N * C * H * W;\n    threads = 1024;\n    blocks = (total + threads - 1) / threads;\n    \n    normalize_kernel<<<blocks, threads>>>(\n        x.data_ptr<float>(),\n        means.data_ptr<float>(),\n        vars.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance using the computed statistics.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int nc = blockIdx.x;\n    int n = nc / C;\n    int c = nc % C;\n    \n    // Compute mean and variance for this instance (n,c)\n    float mean = 0.0f;\n    float var = 0.0f;\n    int offset = (n * C + c) * H * W;\n    \n    // First pass: compute mean\n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[threadIdx.x] = mean;\n    __syncthreads();\n    \n    // Reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (threadIdx.x < s) {\n            sdata[threadIdx.x] += sdata[threadIdx.x + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / (H * W);\n    __syncthreads();\n    \n    // Second pass: compute variance\n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[threadIdx.x] = var;\n    __syncthreads();\n    \n    // Reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (threadIdx.x < s) {\n            sdata[threadIdx.x] += sdata[threadIdx.x + s];\n        }\n        __syncthreads();\n    }\n    float inv_std = rsqrtf(sdata[0] / (H * W) + 1e-5f);\n    \n    // Normalize all elements in this instance\n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        int h = i / W;\n        int w = i % W;\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    const int threads = 256;\n    const int blocks = N * C;\n    int shared_mem = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -1.1740416049957276
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimized algorithm computes instance normalization by parallelizing across channels and using shared memory for mean/variance calculations within each instance, reducing redundant computations.}",
        "function": "1. {Optimized algorithm computes instance normalization by parallelizing across channels and using shared memory for mean/variance calculations within each instance, reducing redundant computations.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    float mean = 0.0f;\n    float var = 0.0f;\n    int offset = (n * C + c) * H * W;\n    \n    // Compute mean in parallel\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Parallel reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / (H * W);\n    \n    // Compute variance in parallel\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Parallel reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    var = sdata[0] / (H * W);\n    \n    // Apply normalization\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int h = i / W;\n        int w = i % W;\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(C, N);\n    int threads = min(1024, H * W);\n    size_t shared_mem = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize by parallelizing mean and variance calculations per instance (n,c) using shared memory for intermediate results, then applying normalization in a separate step.}",
        "function": "1. {Optimize by parallelizing mean and variance calculations per instance (n,c) using shared memory for intermediate results, then applying normalization in a separate step.}\n\n2. Here's the optimized kernel implementation:\n\n```cpp\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    \n    sdata[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        mean = sdata[0] / (H * W);\n        sdata[0] = mean;\n    }\n    __syncthreads();\n    mean = sdata[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    \n    sdata[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        var = sdata[0] / (H * W);\n        sdata[0] = var;\n    }\n    __syncthreads();\n    var = sdata[0];\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(C, N);\n    int threads = min(1024, H * W);\n    int shared_mem = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n```",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.z;\n    int tid = threadIdx.x;\n    int pixel = tid;\n    \n    // Compute mean\n    float mean = 0.0f;\n    while (pixel < H * W) {\n        mean += x[(n * C + c) * H * W + pixel];\n        pixel += blockDim.x;\n    }\n    mean = warp_reduce_sum(mean);\n    if (tid % 32 == 0) smem[tid / 32] = mean;\n    __syncthreads();\n    \n    if (tid < 32) {\n        mean = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        mean = warp_reduce_sum(mean) / (H * W);\n        if (tid == 0) smem[0] = mean;\n    }\n    __syncthreads();\n    mean = smem[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    pixel = tid;\n    while (pixel < H * W) {\n        float diff = x[(n * C + c) * H * W + pixel] - mean;\n        var += diff * diff;\n        pixel += blockDim.x;\n    }\n    var = warp_reduce_sum(var);\n    if (tid % 32 == 0) smem[tid / 32] = var;\n    __syncthreads();\n    \n    if (tid < 32) {\n        var = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        var = warp_reduce_sum(var) / (H * W);\n        if (tid == 0) smem[1] = var;\n    }\n    __syncthreads();\n    var = smem[1];\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    pixel = tid;\n    while (pixel < H * W) {\n        int idx = (n * C + c) * H * W + pixel;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n        pixel += blockDim.x;\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(1, N, C);\n    int threads = min(1024, (H * W + 31) / 32 * 32);\n    int smem_size = ((threads + 31) / 32) * sizeof(float) * 2;\n\n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.713510411977768
    },
    {
        "sample_order": 40,
        "algorithm": "{The optimized algorithm computes instance normalization by first calculating mean and variance for each instance-channel pair in parallel using shared memory for reduction, then normalizes all elements in the instance-channel using the computed statistics.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared_data[];\n    \n    int c = blockIdx.x % C;\n    int n = blockIdx.x / C;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    \n    shared_data[tid] = mean;\n    __syncthreads();\n    \n    // Reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        shared_data[0] /= (H * W);\n    }\n    __syncthreads();\n    mean = shared_data[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    \n    shared_data[tid] = var;\n    __syncthreads();\n    \n    // Reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        shared_data[0] /= (H * W);\n    }\n    __syncthreads();\n    float inv_std = rsqrtf(shared_data[0] + 1e-5f);\n    \n    // Normalize all elements in this instance-channel\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    const int threads = 256;\n    const int blocks = N * C;\n    size_t shared_mem_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -1.1756479859352111
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize by parallelizing mean and variance computation per instance (n,c) using shared memory and warp-level reductions, then normalize all elements in the instance.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    mean = warp_reduce_sum(mean);\n    if (tid % 32 == 0) {\n        smem[tid / 32] = mean;\n    }\n    __syncthreads();\n    if (tid < 32) {\n        mean = tid < (blockDim.x + 31) / 32 ? smem[tid] : 0.0f;\n        mean = warp_reduce_sum(mean);\n    }\n    mean /= (H * W);\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    var = warp_reduce_sum(var);\n    if (tid % 32 == 0) {\n        smem[tid / 32] = var;\n    }\n    __syncthreads();\n    if (tid < 32) {\n        var = tid < (blockDim.x + 31) / 32 ? smem[tid] : 0.0f;\n        var = warp_reduce_sum(var);\n    }\n    var /= (H * W);\n    float inv_std = rsqrtf(var + 1e-5f);\n    \n    // Normalize\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int h = i / W;\n        int w = i % W;\n        int idx = (n * C + c) * H * W + h * W + w;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(C, N);\n    int threads = min(1024, H * W);\n    size_t smem_size = ((threads + 31) / 32) * sizeof(float);\n    \n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Compute instance normalization by first calculating mean and variance per instance-channel in parallel using shared memory for reduction, then applying normalization across spatial dimensions.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared[];\n    \n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    shared[tid] = mean;\n    __syncthreads();\n    \n    // Reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = shared[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    shared[tid] = var;\n    __syncthreads();\n    \n    // Reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    var = shared[0] / (H * W);\n    \n    // Normalize and store\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(N, C);\n    int threads = min(1024, H * W);\n    size_t shared_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7035871982574463
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize by parallelizing mean and variance computation per instance (n,c) using shared memory and cooperative groups, then normalize all elements in the instance.}",
        "function": "#include <cooperative_groups.h>\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    cg::thread_block block = cg::this_thread_block();\n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int instance_size = H * W;\n    int offset = (n * C + c) * instance_size;\n    \n    extern __shared__ float shared_data[];\n    float* shared_sum = shared_data;\n    float* shared_sqsum = shared_data + blockDim.x;\n    \n    // Compute mean and variance using parallel reduction\n    float sum = 0.0f;\n    float sqsum = 0.0f;\n    for (int i = threadIdx.x; i < instance_size; i += blockDim.x) {\n        float val = x[offset + i];\n        sum += val;\n        sqsum += val * val;\n    }\n    \n    shared_sum[threadIdx.x] = sum;\n    shared_sqsum[threadIdx.x] = sqsum;\n    block.sync();\n    \n    // Parallel reduction\n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        if (threadIdx.x < stride) {\n            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];\n            shared_sqsum[threadIdx.x] += shared_sqsum[threadIdx.x + stride];\n        }\n        block.sync();\n    }\n    \n    float mean = shared_sum[0] / instance_size;\n    float var = (shared_sqsum[0] / instance_size) - (mean * mean);\n    float inv_std = rsqrtf(var + 1e-5f);\n    \n    // Normalize all elements in this instance\n    for (int i = threadIdx.x; i < instance_size; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(C, N);\n    int threads = min(1024, H * W);\n    int shared_mem = 2 * threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7344256043434143
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize by processing multiple instances (n,c) per block, using shared memory for intermediate results, and vectorized memory access for improved throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\nconstexpr int INSTANCES_PER_BLOCK = 4;\nconstexpr int THREADS_PER_INSTANCE = 256;\n\n__device__ void block_reduce_sum(float& val, float* smem) {\n    int tid = threadIdx.x % THREADS_PER_INSTANCE;\n    int lane = tid % 32;\n    int wid = tid / 32;\n    \n    val = warpReduceSum(val);\n    if (lane == 0) smem[wid] = val;\n    __syncthreads();\n    \n    if (wid == 0) {\n        val = (lane < (THREADS_PER_INSTANCE + 31) / 32) ? smem[lane] : 0.0f;\n        val = warpReduceSum(val);\n    }\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    float* mean_smem = smem;\n    float* var_smem = smem + INSTANCES_PER_BLOCK * ((THREADS_PER_INSTANCE + 31) / 32);\n    \n    int instance_idx = blockIdx.x * INSTANCES_PER_BLOCK + threadIdx.x / THREADS_PER_INSTANCE;\n    if (instance_idx >= N * C) return;\n    \n    int n = instance_idx / C;\n    int c = instance_idx % C;\n    int tid = threadIdx.x % THREADS_PER_INSTANCE;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += THREADS_PER_INSTANCE) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    block_reduce_sum(mean, mean_smem + (threadIdx.x / THREADS_PER_INSTANCE) * ((THREADS_PER_INSTANCE + 31) / 32));\n    mean /= (H * W);\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += THREADS_PER_INSTANCE) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    block_reduce_sum(var, var_smem + (threadIdx.x / THREADS_PER_INSTANCE) * ((THREADS_PER_INSTANCE + 31) / 32));\n    var /= (H * W);\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += THREADS_PER_INSTANCE) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    int instances = N * C;\n    int blocks = (instances + INSTANCES_PER_BLOCK - 1) / INSTANCES_PER_BLOCK;\n    int threads = INSTANCES_PER_BLOCK * THREADS_PER_INSTANCE;\n    int smem_size = 2 * INSTANCES_PER_BLOCK * ((THREADS_PER_INSTANCE + 31) / 32) * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Compute mean and variance for each instance-channel using parallel reduction with warp shuffles and shared memory, then normalize all elements in the instance-channel with coalesced memory access, while optimizing thread utilization across warps.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    __shared__ float smem_mean[32];\n    __shared__ float smem_var[32];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    int warp_id = tid / 32;\n    int lane_id = tid % 32;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    mean = warp_reduce(mean);\n    if (lane_id == 0) smem_mean[warp_id] = mean;\n    __syncthreads();\n    \n    if (tid < 32) {\n        mean = (tid < (blockDim.x + 31) / 32) ? smem_mean[tid] : 0.0f;\n        mean = warp_reduce(mean) / (H * W);\n        if (tid == 0) smem_mean[0] = mean;\n    }\n    __syncthreads();\n    mean = smem_mean[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    var = warp_reduce(var);\n    if (lane_id == 0) smem_var[warp_id] = var;\n    __syncthreads();\n    \n    if (tid < 32) {\n        var = (tid < (blockDim.x + 31) / 32) ? smem_var[tid] : 0.0f;\n        var = warp_reduce(var) / (H * W);\n        if (tid == 0) smem_var[0] = var;\n    }\n    __syncthreads();\n    \n    // Normalize\n    float inv_std = rsqrtf(smem_var[0] + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(C, N);\n    int threads = min(1024, ((H * W + 31) / 32) * 32);\n    size_t smem_size = ((threads + 31) / 32) * sizeof(float) * 2;\n\n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7034720063209534
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize by processing each instance (n,c) with a separate block, computing mean and variance using parallel reduction across warps, then normalizing elements with vectorized memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <int NUM_WARPS>\n__device__ void warpReduce(float& val) {\n    #pragma unroll\n    for (int offset = 16; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    const int nc = blockIdx.x;\n    const int n = nc / C;\n    const int c = nc % C;\n    const int tid = threadIdx.x;\n    const int warp_id = tid / 32;\n    const int lane_id = tid % 32;\n    \n    float mean = 0.0f;\n    float var = 0.0f;\n    const int offset = (n * C + c) * H * W;\n    \n    // Compute mean\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    warpReduce<8>(mean);\n    \n    if (lane_id == 0) {\n        atomicAdd(&out[nc], mean);\n    }\n    __syncthreads();\n    \n    if (tid == 0) {\n        mean = out[nc] / (H * W);\n        out[nc] = mean;\n    }\n    __syncthreads();\n    mean = out[nc];\n    \n    // Compute variance\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    warpReduce<8>(var);\n    \n    if (lane_id == 0) {\n        atomicAdd(&out[N * C + nc], var);\n    }\n    __syncthreads();\n    \n    if (tid == 0) {\n        float inv_std = rsqrtf(out[N * C + nc] / (H * W) + 1e-5f);\n        out[N * C + nc] = inv_std;\n    }\n    __syncthreads();\n    \n    float inv_std = out[N * C + nc];\n    float weight = inorm_weight[c];\n    float bias = inorm_bias[c];\n    \n    // Normalize with vectorized access\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * weight + bias;\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    auto tmp = at::zeros({N * C * 2}, x.options());\n    \n    const int threads = 256;\n    const int blocks = N * C;\n    \n    instance_norm_kernel<<<blocks, threads>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n    \n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");",
        "score": null
    }
]