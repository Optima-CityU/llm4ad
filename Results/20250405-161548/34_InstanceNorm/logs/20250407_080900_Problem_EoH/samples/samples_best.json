[
    {
        "sample_order": 3,
        "algorithm": "{Optimize by parallelizing across instances (N,C) and using shared memory for mean/variance computation within each instance, reducing redundant calculations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared[];\n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    shared[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = shared[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    shared[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    var = shared[0] / (H * W);\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n               \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n               \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(N, C);\n    int threads = min(1024, H * W);\n    size_t shared_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7026240050792694
    }
]