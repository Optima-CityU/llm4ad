[
    {
        "algorithm": "{Optimize by parallelizing across instances (N,C) and using shared memory for mean/variance computation within each instance, reducing redundant calculations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared[];\n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    shared[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = shared[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    shared[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    var = shared[0] / (H * W);\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n               \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n               \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(N, C);\n    int threads = min(1024, H * W);\n    size_t shared_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7026240050792694
    },
    {
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.z;\n    int tid = threadIdx.x;\n    int pixel = tid;\n    \n    // Compute mean\n    float mean = 0.0f;\n    while (pixel < H * W) {\n        mean += x[(n * C + c) * H * W + pixel];\n        pixel += blockDim.x;\n    }\n    mean = warp_reduce_sum(mean);\n    if (tid % 32 == 0) smem[tid / 32] = mean;\n    __syncthreads();\n    \n    if (tid < 32) {\n        mean = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        mean = warp_reduce_sum(mean) / (H * W);\n        if (tid == 0) smem[0] = mean;\n    }\n    __syncthreads();\n    mean = smem[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    pixel = tid;\n    while (pixel < H * W) {\n        float diff = x[(n * C + c) * H * W + pixel] - mean;\n        var += diff * diff;\n        pixel += blockDim.x;\n    }\n    var = warp_reduce_sum(var);\n    if (tid % 32 == 0) smem[tid / 32] = var;\n    __syncthreads();\n    \n    if (tid < 32) {\n        var = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        var = warp_reduce_sum(var) / (H * W);\n        if (tid == 0) smem[1] = var;\n    }\n    __syncthreads();\n    var = smem[1];\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    pixel = tid;\n    while (pixel < H * W) {\n        int idx = (n * C + c) * H * W + pixel;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n        pixel += blockDim.x;\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(1, N, C);\n    int threads = min(1024, (H * W + 31) / 32 * 32);\n    int smem_size = ((threads + 31) / 32) * sizeof(float) * 2;\n\n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.713510411977768
    },
    {
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.z;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    mean = warpReduceSum(mean);\n    if (tid % 32 == 0) {\n        smem[tid / 32] = mean;\n    }\n    __syncthreads();\n    \n    if (tid < 32) {\n        mean = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        mean = warpReduceSum(mean);\n        if (tid == 0) smem[0] = mean / (H * W);\n    }\n    __syncthreads();\n    mean = smem[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    var = warpReduceSum(var);\n    if (tid % 32 == 0) {\n        smem[tid / 32] = var;\n    }\n    __syncthreads();\n    \n    if (tid < 32) {\n        var = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        var = warpReduceSum(var);\n        if (tid == 0) smem[1] = var / (H * W);\n    }\n    __syncthreads();\n    var = smem[1];\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(1, N, C);\n    int threads = min(1024, H * W);\n    int smem_size = ((threads + 31) / 32) * sizeof(float) * 2;\n\n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7137279987335206
    },
    {
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance using the computed statistics.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float sdata[];\n    int nc = blockIdx.x;\n    int n = nc / C;\n    int c = nc % C;\n    \n    // Compute mean and variance for this instance (n,c)\n    float mean = 0.0f;\n    float var = 0.0f;\n    int offset = (n * C + c) * H * W;\n    \n    // First pass: compute mean\n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        mean += x[offset + i];\n    }\n    sdata[threadIdx.x] = mean;\n    __syncthreads();\n    \n    // Reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (threadIdx.x < s) {\n            sdata[threadIdx.x] += sdata[threadIdx.x + s];\n        }\n        __syncthreads();\n    }\n    mean = sdata[0] / (H * W);\n    __syncthreads();\n    \n    // Second pass: compute variance\n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        float diff = x[offset + i] - mean;\n        var += diff * diff;\n    }\n    sdata[threadIdx.x] = var;\n    __syncthreads();\n    \n    // Reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (threadIdx.x < s) {\n            sdata[threadIdx.x] += sdata[threadIdx.x + s];\n        }\n        __syncthreads();\n    }\n    float inv_std = rsqrtf(sdata[0] / (H * W) + 1e-5f);\n    \n    // Normalize all elements in this instance\n    for (int i = threadIdx.x; i < H * W; i += blockDim.x) {\n        int h = i / W;\n        int w = i % W;\n        int idx = offset + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    const int threads = 256;\n    const int blocks = N * C;\n    int shared_mem = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -1.1740416049957276
    },
    {
        "algorithm": "{The optimized algorithm computes instance normalization by first calculating mean and variance for each instance-channel pair in parallel using shared memory for reduction, then normalizes all elements in the instance-channel using the computed statistics.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared_data[];\n    \n    int c = blockIdx.x % C;\n    int n = blockIdx.x / C;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    \n    shared_data[tid] = mean;\n    __syncthreads();\n    \n    // Reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        shared_data[0] /= (H * W);\n    }\n    __syncthreads();\n    mean = shared_data[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    \n    shared_data[tid] = var;\n    __syncthreads();\n    \n    // Reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        shared_data[0] /= (H * W);\n    }\n    __syncthreads();\n    float inv_std = rsqrtf(shared_data[0] + 1e-5f);\n    \n    // Normalize all elements in this instance-channel\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    const int threads = 256;\n    const int blocks = N * C;\n    size_t shared_mem_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_mem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -1.1756479859352111
    }
]