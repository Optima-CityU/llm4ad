[
    {
        "algorithm": "{Optimize by parallelizing across instances (N,C) and using shared memory for mean/variance computation within each instance, reducing redundant calculations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared[];\n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    shared[tid] = mean;\n    __syncthreads();\n    \n    // Reduce mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = shared[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    shared[tid] = var;\n    __syncthreads();\n    \n    // Reduce variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    var = shared[0] / (H * W);\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n               \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n               \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(N, C);\n    int threads = min(1024, H * W);\n    size_t shared_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7026240050792694
    },
    {
        "algorithm": "{Compute mean and variance for each instance-channel using parallel reduction with warp shuffles and shared memory, then normalize all elements in the instance-channel with coalesced memory access, while optimizing thread utilization across warps.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    __shared__ float smem_mean[32];\n    __shared__ float smem_var[32];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.x;\n    int tid = threadIdx.x;\n    int warp_id = tid / 32;\n    int lane_id = tid % 32;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    mean = warp_reduce(mean);\n    if (lane_id == 0) smem_mean[warp_id] = mean;\n    __syncthreads();\n    \n    if (tid < 32) {\n        mean = (tid < (blockDim.x + 31) / 32) ? smem_mean[tid] : 0.0f;\n        mean = warp_reduce(mean) / (H * W);\n        if (tid == 0) smem_mean[0] = mean;\n    }\n    __syncthreads();\n    mean = smem_mean[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    var = warp_reduce(var);\n    if (lane_id == 0) smem_var[warp_id] = var;\n    __syncthreads();\n    \n    if (tid < 32) {\n        var = (tid < (blockDim.x + 31) / 32) ? smem_var[tid] : 0.0f;\n        var = warp_reduce(var) / (H * W);\n        if (tid == 0) smem_var[0] = var;\n    }\n    __syncthreads();\n    \n    // Normalize\n    float inv_std = rsqrtf(smem_var[0] + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(C, N);\n    int threads = min(1024, ((H * W + 31) / 32) * 32);\n    size_t smem_size = ((threads + 31) / 32) * sizeof(float) * 2;\n\n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7034720063209534
    },
    {
        "algorithm": "{Compute instance normalization by first calculating mean and variance per instance-channel in parallel using shared memory for reduction, then applying normalization across spatial dimensions.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float shared[];\n    \n    int n = blockIdx.x;\n    int c = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    shared[tid] = mean;\n    __syncthreads();\n    \n    // Reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    mean = shared[0] / (H * W);\n    __syncthreads();\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    shared[tid] = var;\n    __syncthreads();\n    \n    // Reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared[tid] += shared[tid + s];\n        }\n        __syncthreads();\n    }\n    var = shared[0] / (H * W);\n    \n    // Normalize and store\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(N, C);\n    int threads = min(1024, H * W);\n    size_t shared_size = threads * sizeof(float);\n\n    instance_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7035871982574463
    },
    {
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.z;\n    int tid = threadIdx.x;\n    int pixel = tid;\n    \n    // Compute mean\n    float mean = 0.0f;\n    while (pixel < H * W) {\n        mean += x[(n * C + c) * H * W + pixel];\n        pixel += blockDim.x;\n    }\n    mean = warp_reduce_sum(mean);\n    if (tid % 32 == 0) smem[tid / 32] = mean;\n    __syncthreads();\n    \n    if (tid < 32) {\n        mean = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        mean = warp_reduce_sum(mean) / (H * W);\n        if (tid == 0) smem[0] = mean;\n    }\n    __syncthreads();\n    mean = smem[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    pixel = tid;\n    while (pixel < H * W) {\n        float diff = x[(n * C + c) * H * W + pixel] - mean;\n        var += diff * diff;\n        pixel += blockDim.x;\n    }\n    var = warp_reduce_sum(var);\n    if (tid % 32 == 0) smem[tid / 32] = var;\n    __syncthreads();\n    \n    if (tid < 32) {\n        var = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        var = warp_reduce_sum(var) / (H * W);\n        if (tid == 0) smem[1] = var;\n    }\n    __syncthreads();\n    var = smem[1];\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    pixel = tid;\n    while (pixel < H * W) {\n        int idx = (n * C + c) * H * W + pixel;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n        pixel += blockDim.x;\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n\n    dim3 blocks(1, N, C);\n    int threads = min(1024, (H * W + 31) / 32 * 32);\n    int smem_size = ((threads + 31) / 32) * sizeof(float) * 2;\n\n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.713510411977768
    },
    {
        "algorithm": "{Optimize by computing mean and variance per instance (n,c) in parallel using shared memory and warp-level reductions, then normalize all elements in the instance.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void instance_norm_kernel(\n    const float* __restrict__ x,\n    const float* __restrict__ inorm_weight,\n    const float* __restrict__ inorm_bias,\n    float* __restrict__ out,\n    int N, int C, int H, int W\n) {\n    extern __shared__ float smem[];\n    \n    int n = blockIdx.y;\n    int c = blockIdx.z;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        mean += x[(n * C + c) * H * W + i];\n    }\n    mean = warpReduceSum(mean);\n    if (tid % 32 == 0) {\n        smem[tid / 32] = mean;\n    }\n    __syncthreads();\n    \n    if (tid < 32) {\n        mean = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        mean = warpReduceSum(mean);\n        if (tid == 0) smem[0] = mean / (H * W);\n    }\n    __syncthreads();\n    mean = smem[0];\n    \n    // Compute variance\n    float var = 0.0f;\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        float diff = x[(n * C + c) * H * W + i] - mean;\n        var += diff * diff;\n    }\n    var = warpReduceSum(var);\n    if (tid % 32 == 0) {\n        smem[tid / 32] = var;\n    }\n    __syncthreads();\n    \n    if (tid < 32) {\n        var = (tid < blockDim.x / 32) ? smem[tid] : 0.0f;\n        var = warpReduceSum(var);\n        if (tid == 0) smem[1] = var / (H * W);\n    }\n    __syncthreads();\n    var = smem[1];\n    \n    // Normalize\n    float inv_std = rsqrtf(var + 1e-5f);\n    for (int i = tid; i < H * W; i += blockDim.x) {\n        int idx = (n * C + c) * H * W + i;\n        out[idx] = (x[idx] - mean) * inv_std * inorm_weight[c] + inorm_bias[c];\n    }\n}\n\nat::Tensor forward(\n    const at::Tensor& x,\n    const at::Tensor& inorm_weight,\n    const at::Tensor& inorm_bias\n) {\n    TORCH_CHECK(x.dim() == 4, \"Input tensor must be 4D\");\n    TORCH_CHECK(inorm_weight.dim() == 1 && inorm_weight.size(0) == x.size(1),\n                \"Weight tensor must be of shape (C,)\");\n    TORCH_CHECK(inorm_bias.dim() == 1 && inorm_bias.size(0) == x.size(1),\n                \"Bias tensor must be of shape (C,)\");\n\n    int N = x.size(0);\n    int C = x.size(1);\n    int H = x.size(2);\n    int W = x.size(3);\n\n    auto out = at::empty_like(x);\n    \n    dim3 blocks(1, N, C);\n    int threads = min(1024, H * W);\n    int smem_size = ((threads + 31) / 32) * sizeof(float) * 2;\n\n    instance_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(),\n        inorm_weight.data_ptr<float>(),\n        inorm_bias.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, H, W\n    );\n\n    return out;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Instance Norm CUDA forward\");\n}",
        "score": -0.7137279987335206
    }
]