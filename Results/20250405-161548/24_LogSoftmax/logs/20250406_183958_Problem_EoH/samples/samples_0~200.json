[
    {
        "sample_order": 1,
        "algorithm": "{The optimized algorithm computes the maximum value and sum of exponentials in parallel using shared memory within thread blocks, then applies the log softmax formula to each element.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // First pass: find max in the dimension\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    shared_mem[tid] = max_val;\n    __syncthreads();\n    \n    // Reduce max in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_mem[0];\n    __syncthreads();\n    \n    // Second pass: compute sum of exponentials\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    shared_mem[tid] = sum_exp;\n    __syncthreads();\n    \n    // Reduce sum in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_mem[0];\n    __syncthreads();\n    \n    // Final pass: compute log softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads_per_block = 256;\n    int shared_mem_size = threads_per_block * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize by parallelizing max reduction and sum-exp calculations across threads, then using shared memory for efficient intra-block communication and cooperative reductions.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Phase 1: Find max value in the dimension\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    shared_mem[tid] = max_val;\n    __syncthreads();\n    \n    // Reduction for max value\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_mem[0];\n    __syncthreads();\n    \n    // Phase 2: Compute sum of exp(x_i - max_val)\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    shared_mem[tid] = sum_exp;\n    __syncthreads();\n    \n    // Reduction for sum_exp\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_mem[0];\n    __syncthreads();\n    \n    // Phase 3: Compute final log_softmax values\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = input[batch_index * dim_size + tid] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    dim3 blocks(batch_size);\n    int threads = min(1024, ((dim_size + 31) / 32) * 32);\n    size_t shared_mem_size = threads * sizeof(float);\n    \n    log_softmax_kernel<<<blocks, threads, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{The optimized algorithm uses parallel reduction for max and sum computations, splits the work into two kernels (one for max/sum and one for final computation), and employs shared memory for efficient data access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\n__global__ void log_softmax_max_reduce_kernel(const float* input, float* max_vals, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    int batch_idx = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_idx * dim_size + i]);\n    }\n    \n    shared_mem[tid] = thread_max;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        max_vals[batch_idx] = shared_mem[0];\n    }\n}\n\n__global__ void log_softmax_sum_reduce_kernel(const float* input, const float* max_vals, float* sum_vals, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    int batch_idx = blockIdx.x;\n    int tid = threadIdx.x;\n    float max_val = max_vals[batch_idx];\n    \n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_idx * dim_size + i] - max_val);\n    }\n    \n    shared_mem[tid] = thread_sum;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        sum_vals[batch_idx] = shared_mem[0];\n    }\n}\n\n__global__ void log_softmax_final_kernel(const float* input, const float* max_vals, const float* sum_vals, float* output, int batch_size, int dim_size) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < batch_size * dim_size) {\n        int batch_idx = index / dim_size;\n        int dim_idx = index % dim_size;\n        float max_val = max_vals[batch_idx];\n        float sum_val = sum_vals[batch_idx];\n        output[index] = input[index] - max_val - logf(sum_val);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    auto max_vals = torch::empty({batch_size}, input.options());\n    auto sum_vals = torch::empty({batch_size}, input.options());\n    \n    int reduce_threads = 256;\n    int final_threads = 256;\n    \n    log_softmax_max_reduce_kernel<<<batch_size, reduce_threads, reduce_threads * sizeof(float)>>>(\n        input.data_ptr<float>(), max_vals.data_ptr<float>(), batch_size, dim_size);\n    \n    log_softmax_sum_reduce_kernel<<<batch_size, reduce_threads, reduce_threads * sizeof(float)>>>(\n        input.data_ptr<float>(), max_vals.data_ptr<float>(), sum_vals.data_ptr<float>(), batch_size, dim_size);\n    \n    int final_blocks = (batch_size * dim_size + final_threads - 1) / final_threads;\n    log_softmax_final_kernel<<<final_blocks, final_threads>>>(\n        input.data_ptr<float>(), max_vals.data_ptr<float>(), sum_vals.data_ptr<float>(), \n        output.data_ptr<float>(), batch_size, dim_size);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize by computing max and sum_exp in parallel using shared memory within each block, then applying log_softmax to all elements in parallel with proper synchronization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    float *max_vals = shared_mem;\n    float *sum_exps = &shared_mem[batch_size];\n    \n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Find max value for this batch\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    \n    max_vals[tid] = thread_max;\n    __syncthreads();\n    \n    // Reduce max values\n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            max_vals[tid] = fmaxf(max_vals[tid], max_vals[tid + stride]);\n        }\n        __syncthreads();\n    }\n    \n    float max_val = max_vals[0];\n    __syncthreads();\n    \n    // Compute sum of exponentials\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    sum_exps[tid] = thread_sum;\n    __syncthreads();\n    \n    // Reduce sums\n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            sum_exps[tid] += sum_exps[tid + stride];\n        }\n        __syncthreads();\n    }\n    \n    // Compute final log softmax\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = input[batch_index * dim_size + tid] - max_val - logf(sum_exps[0]);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads_per_block = 256;\n    int shared_mem_size = 2 * threads_per_block * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize by computing max and sum_exp in parallel using warp-level reductions within each thread block, then broadcasting results to all threads for the final log_softmax calculation.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    int lane_id = tid % 32;\n    \n    float max_val = -FLT_MAX;\n    float sum_exp = 0.0f;\n    \n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = input[batch_index * dim_size + i];\n        max_val = fmaxf(max_val, val);\n    }\n    \n    max_val = warpReduceMax(max_val);\n    if (lane_id == 0) {\n        max_val = warpReduceMax(max_val);\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n    \n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = input[batch_index * dim_size + i];\n        sum_exp += expf(val - max_val);\n    }\n    \n    sum_exp = warpReduceSum(sum_exp);\n    if (lane_id == 0) {\n        sum_exp = warpReduceSum(sum_exp);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n    \n    if (tid < dim_size) {\n        float val = input[batch_index * dim_size + tid];\n        output[batch_index * dim_size + tid] = val - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    dim3 blocks(batch_size);\n    dim3 threads(min(1024, ((dim_size + 31) / 32) * 32));\n\n    log_softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{The optimized algorithm computes the maximum and sum of exponentials in parallel using warp-level reductions within each thread block, then applies the log softmax formula to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n#include <cuda_runtime.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warpReduceMax(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__device__ __forceinline__ T warpReduceSum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    extern __shared__ float shared[]; \n    float *max_vals = shared;\n    float *sum_exps = shared + blockDim.x;\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    int lane = tid % 32;\n    int warp = tid / 32;\n\n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = input[batch_index * dim_size + i];\n        thread_max = fmaxf(thread_max, val);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (lane == 0) max_vals[warp] = thread_max;\n    __syncthreads();\n\n    float max_val = max_vals[0];\n    if (warp == 0) max_val = warpReduceMax(lane < blockDim.x / 32 ? max_vals[lane] : -FLT_MAX);\n    __syncthreads();\n\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = input[batch_index * dim_size + i];\n        thread_sum += expf(val - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (lane == 0) sum_exps[warp] = thread_sum;\n    __syncthreads();\n\n    float sum_exp = sum_exps[0];\n    if (warp == 0) sum_exp = warpReduceSum(lane < blockDim.x / 32 ? sum_exps[lane] : 0.0f);\n    __syncthreads();\n\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = input[batch_index * dim_size + i];\n        output[batch_index * dim_size + i] = val - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads = 256;\n    int shared_mem = 2 * (threads / 32 + 1) * sizeof(float);\n\n    log_softmax_kernel<<<batch_size, threads, shared_mem>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{The optimized algorithm computes max and sum-exp values per batch using shared memory for parallel reduction, then applies log-softmax to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // First pass: find max in the dimension\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    shared_mem[tid] = max_val;\n    __syncthreads();\n    \n    // Parallel reduction for max\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_mem[0];\n    __syncthreads();\n    \n    // Second pass: compute sum of exp(x - max)\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    shared_mem[tid] = sum_exp;\n    __syncthreads();\n    \n    // Parallel reduction for sum_exp\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_mem[0];\n    __syncthreads();\n    \n    // Final pass: compute log_softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads_per_block = 256;\n    int shared_mem_size = threads_per_block * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{The optimized algorithm computes log softmax by first finding the maximum value per batch using parallel reduction, then calculates the sum of exponentials (shifted by max) using another parallel reduction, and finally computes the log softmax values for each element.}",
        "function": "#include <torch/extension.h>\n#include <cub/cub.cuh>\n\ntemplate <int BLOCK_SIZE>\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    __shared__ float sdata[BLOCK_SIZE];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Step 1: Find max value in the row\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n\n    sdata[tid] = max_val;\n    __syncthreads();\n\n    // Parallel reduction for max\n    for (int s = BLOCK_SIZE / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] = fmaxf(sdata[tid], sdata[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = sdata[0];\n    __syncthreads();\n\n    // Step 2: Compute sum of exp(x_i - max)\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n\n    sdata[tid] = sum_exp;\n    __syncthreads();\n\n    // Parallel reduction for sum\n    for (int s = BLOCK_SIZE / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = sdata[0];\n    __syncthreads();\n\n    // Step 3: Compute log softmax\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    const int BLOCK_SIZE = 256;\n    log_softmax_kernel<BLOCK_SIZE><<<batch_size, BLOCK_SIZE>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{The optimized algorithm computes the max and sum_exp values in parallel using shared memory for each batch element, then applies the log_softmax formula to all elements in parallel.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Load input into shared memory\n    float *shared_input = shared_mem;\n    if (tid < dim_size) {\n        shared_input[tid] = input[batch_index * dim_size + tid];\n    }\n    __syncthreads();\n\n    // Find max value in the row\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, shared_input[i]);\n    }\n    max_val = blockReduceMax(max_val);\n    __syncthreads();\n\n    // Compute sum of exponentials\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(shared_input[i] - max_val);\n    }\n    sum_exp = blockReduceSum(sum_exp);\n    __syncthreads();\n\n    // Compute log softmax\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = shared_input[tid] - max_val - logf(sum_exp);\n    }\n}\n\n__device__ float blockReduceMax(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    val = warpReduceMax(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : -FLT_MAX;\n    val = warpReduceMax(val);\n    return val;\n}\n\n__device__ float blockReduceSum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    val = warpReduceSum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;\n    val = warpReduceSum(val);\n    return val;\n}\n\n__device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    return val;\n}\n\n__device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads = 256;\n    int shared_mem_size = dim_size * sizeof(float);\n\n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{The optimized algorithm computes the max and sum-exp values per batch in parallel using shared memory for reduction, then applies the log-softmax formula to each element.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // First pass: find max in the dimension\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    shared_mem[tid] = max_val;\n    __syncthreads();\n    \n    // Reduce max in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_mem[0];\n    __syncthreads();\n    \n    // Second pass: compute sum of exp(x - max)\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    shared_mem[tid] = sum_exp;\n    __syncthreads();\n    \n    // Reduce sum in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_mem[0];\n    __syncthreads();\n    \n    // Final pass: compute log softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads = min(1024, dim_size);\n    int shared_mem_size = threads * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{The optimized algorithm computes the maximum and sum of exponentials in parallel using warp-level reductions, then applies the log-softmax formula to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_max(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    const float *row_ptr = input + batch_index * dim_size;\n    float *out_ptr = output + batch_index * dim_size;\n\n    // Compute max value in row\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, row_ptr[i]);\n    }\n    float max_val = warp_reduce_max(thread_max);\n    if (tid == 0) {\n        max_val = warp_reduce_max(max_val);\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n\n    // Compute sum of exponentials\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(row_ptr[i] - max_val);\n    }\n    float sum_exp = warp_reduce_sum(thread_sum);\n    if (tid == 0) {\n        sum_exp = warp_reduce_sum(sum_exp);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n    float log_sum_exp = logf(sum_exp);\n\n    // Compute log softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        out_ptr[i] = row_ptr[i] - max_val - log_sum_exp;\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    dim3 blocks(batch_size);\n    dim3 threads(256);\n    \n    log_softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize by parallelizing max and sum reductions across threads, using shared memory for intermediate results, and avoiding redundant computations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // First pass: find max value in the dimension\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    shared_mem[tid] = max_val;\n    __syncthreads();\n    \n    // Reduce max in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_mem[0];\n    __syncthreads();\n    \n    // Second pass: compute sum of exponentials\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    shared_mem[tid] = sum_exp;\n    __syncthreads();\n    \n    // Reduce sum in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_mem[0];\n    __syncthreads();\n    \n    // Final computation\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads_per_block = 256;\n    int shared_mem_size = threads_per_block * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{The optimized algorithm computes the maximum value and sum of exponentials in parallel using warp-level reductions, then applies the log-softmax formula to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_max(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    const float *row_input = input + batch_index * dim_size;\n    float *row_output = output + batch_index * dim_size;\n\n    // Compute max value in each row\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, row_input[i]);\n    }\n    float max_val = warp_reduce_max(thread_max);\n    if (tid == 0) {\n        max_val = warp_reduce_max(max_val);\n    }\n    max_val = __shfl_sync(0xffffffff, max_val, 0);\n\n    // Compute sum of exponentials\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(row_input[i] - max_val);\n    }\n    float sum_exp = warp_reduce_sum(thread_sum);\n    if (tid == 0) {\n        sum_exp = warp_reduce_sum(sum_exp);\n    }\n    sum_exp = __shfl_sync(0xffffffff, sum_exp, 0);\n\n    // Compute log softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        row_output[i] = row_input[i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    dim3 blocks(batch_size);\n    dim3 threads(256);\n\n    log_softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{The optimized algorithm computes the max and sum-exp values per batch in parallel using shared memory for intermediate results, then applies the log-softmax formula to each element.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_data[];\n    float* max_vals = shared_data;\n    float* sum_exps = &shared_data[batch_size];\n    \n    int tid = threadIdx.x;\n    int batch = blockIdx.x;\n    \n    // Find max value for each batch\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch * dim_size + i]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (tid % 32 == 0) {\n        atomicMaxFloat(&max_vals[batch], thread_max);\n    }\n    __syncthreads();\n    \n    // Compute sum_exp for each batch\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch * dim_size + i] - max_vals[batch]);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (tid % 32 == 0) {\n        atomicAdd(&sum_exps[batch], thread_sum);\n    }\n    __syncthreads();\n    \n    // Compute log_softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch * dim_size + i] = input[batch * dim_size + i] - max_vals[batch] - logf(sum_exps[batch]);\n    }\n}\n\n__device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__device__ void atomicMaxFloat(float* address, float val) {\n    int* address_as_i = (int*)address;\n    int old = *address_as_i;\n    int assumed;\n    do {\n        assumed = old;\n        old = atomicCAS(address_as_i, assumed, __float_as_int(fmaxf(val, __int_as_float(assumed))));\n    } while (assumed != old);\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads = 256;\n    int shared_mem_size = batch_size * 2 * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimized algorithm uses parallel reduction for max and sum operations, followed by element-wise log-softmax computation with improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cub/cub.cuh>\n\ntemplate <int BLOCK_SIZE>\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    float *max_vals = shared_mem;\n    float *sum_exps = shared_mem + BLOCK_SIZE;\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // First pass: find max value\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n\n    thread_max = cub::BlockReduce<float, BLOCK_SIZE>(thread_max, cub::Max()).Reduce(thread_max, cub::Max());\n\n    if (tid == 0) {\n        max_vals[0] = thread_max;\n    }\n    __syncthreads();\n\n    // Second pass: compute sum of exps\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_vals[0]);\n    }\n\n    thread_sum = cub::BlockReduce<float, BLOCK_SIZE>(thread_sum, cub::Sum()).Reduce(thread_sum, cub::Sum());\n\n    if (tid == 0) {\n        sum_exps[0] = thread_sum;\n    }\n    __syncthreads();\n\n    // Final pass: compute log softmax\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_vals[0] - logf(sum_exps[0]);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    const int BLOCK_SIZE = 256;\n    size_t shared_mem_size = 2 * BLOCK_SIZE * sizeof(float);\n\n    log_softmax_kernel<BLOCK_SIZE><<<batch_size, BLOCK_SIZE, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{The optimized algorithm computes the max and sum-exp values in parallel using warp-level reductions, then applies the log-softmax formula to each element with improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n#include <cuda_runtime.h>\n\n__device__ __forceinline__ float warp_reduce_max(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    int batch_idx = blockIdx.x;\n    int tid = threadIdx.x;\n    const float *row = input + batch_idx * dim_size;\n    float *out_row = output + batch_idx * dim_size;\n\n    // Compute max value\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, row[i]);\n    }\n    float max_val = warp_reduce_max(thread_max);\n    if (tid == 0) {\n        max_val = warp_reduce_max(max_val);\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n\n    // Compute sum exp\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(row[i] - max_val);\n    }\n    float sum_exp = warp_reduce_sum(thread_sum);\n    if (tid == 0) {\n        sum_exp = warp_reduce_sum(sum_exp);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n\n    // Compute log softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        out_row[i] = row[i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    dim3 blocks(batch_size);\n    dim3 threads(256);\n\n    log_softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize by computing max and sum_exp in parallel using warp-level reductions within each thread block, then broadcasting results to all threads for final log_softmax calculation.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cub/cub.cuh>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Load input to shared memory\n    float *shared_input = shared_mem;\n    if (tid < dim_size) {\n        shared_input[tid] = input[batch_index * dim_size + tid];\n    }\n    __syncthreads();\n\n    // Compute max using warp reductions\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, shared_input[i]);\n    }\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        max_val = fmaxf(max_val, __shfl_down_sync(0xFFFFFFFF, max_val, offset));\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n\n    // Compute sum_exp using warp reductions\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(shared_input[i] - max_val);\n    }\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        sum_exp += __shfl_down_sync(0xFFFFFFFF, sum_exp, offset);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n\n    // Write results\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = shared_input[tid] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads = min(1024, ((dim_size + 31) / 32) * 32);\n    int shared_mem_size = dim_size * sizeof(float);\n\n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{The optimized algorithm computes the max and sum_exp values per batch in parallel using warp reductions, then applies the log_softmax formula to each element with improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_max(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel_optimized(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    extern __shared__ float shared_mem[];\n    float* max_vals = shared_mem;\n    float* sum_exps = shared_mem + blockDim.x / 32;\n    \n    // Compute max value for this batch\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    if (tid % 32 == 0) max_vals[tid / 32] = thread_max;\n    __syncthreads();\n    \n    float max_val = max_vals[0];\n    if (blockDim.x > 32) max_val = fmaxf(max_val, max_vals[1]);\n    \n    // Compute sum_exp for this batch\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (tid % 32 == 0) sum_exps[tid / 32] = thread_sum;\n    __syncthreads();\n    \n    float sum_exp = sum_exps[0];\n    if (blockDim.x > 32) sum_exp += sum_exps[1];\n    \n    // Compute log_softmax for each element\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = input[batch_index * dim_size + tid] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    dim3 blocks(batch_size);\n    int threads = min(1024, ((dim_size + 31) / 32) * 32);\n    size_t shared_mem_size = (threads / 32) * 2 * sizeof(float);\n    \n    log_softmax_kernel_optimized<<<blocks, threads, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{The optimized algorithm computes the max and sum_exp values per batch in parallel using shared memory for reduction, then applies the log_softmax formula to each element.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // First pass: find max in the dimension\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    shared_mem[tid] = max_val;\n    __syncthreads();\n    \n    // Reduce max in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_mem[0];\n    __syncthreads();\n    \n    // Second pass: compute sum of exp(x - max)\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    shared_mem[tid] = sum_exp;\n    __syncthreads();\n    \n    // Reduce sum in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_mem[0];\n    __syncthreads();\n    \n    // Third pass: compute log_softmax for each element\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads_per_block = 256;\n    int shared_mem_size = threads_per_block * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{The optimized algorithm computes the max and sum-exp values per batch in parallel using shared memory for reduction, then calculates log-softmax for each element.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // First pass: find max in the dimension\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    shared_mem[tid] = max_val;\n    __syncthreads();\n    \n    // Reduction to find global max\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_mem[0];\n    __syncthreads();\n    \n    // Second pass: compute sum_exp\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    shared_mem[tid] = sum_exp;\n    __syncthreads();\n    \n    // Reduction to find global sum_exp\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_mem[0];\n    __syncthreads();\n    \n    // Final pass: compute log_softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads = min(1024, dim_size);\n    int shared_mem_size = threads * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimized algorithm computes max values and sum-exponentials per batch using parallel reduction, then applies log-softmax formula to each element with proper synchronization.}",
        "function": "#include <torch/extension.h>\n#include <cub/cub.cuh>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    float *max_val_shared = shared_mem;\n    float *sum_exp_shared = shared_mem + blockDim.x;\n\n    // Step 1: Find max value in the dimension (parallel reduction)\n    float thread_max = -FLT_MAX;\n    for (int i = threadIdx.x; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    thread_max = cub::BlockReduce<float>(max_val_shared).Reduce(thread_max, cub::Max());\n    if (threadIdx.x == 0) {\n        max_val_shared[0] = thread_max;\n    }\n    __syncthreads();\n    float max_val = max_val_shared[0];\n\n    // Step 2: Compute sum of exponentials (parallel reduction)\n    float thread_sum = 0.0f;\n    for (int i = threadIdx.x; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    thread_sum = cub::BlockReduce<float>(sum_exp_shared).Sum(thread_sum);\n    if (threadIdx.x == 0) {\n        sum_exp_shared[0] = thread_sum;\n    }\n    __syncthreads();\n    float sum_exp = sum_exp_shared[0];\n\n    // Step 3: Compute log softmax for each element\n    for (int i = threadIdx.x; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads_per_block = 256;\n    size_t shared_mem_size = 2 * threads_per_block * sizeof(float);\n\n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize by computing max and sum-exp in parallel using warp-level reductions and shared memory, then applying log-softmax to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_max(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    float *max_vals = shared_mem;\n    float *sum_exps = shared_mem + blockDim.y;\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.y * blockDim.x + threadIdx.x;\n    int lane_id = threadIdx.x;\n\n    // Each warp handles one row\n    if (batch_index >= batch_size) return;\n\n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n    float val;\n\n    // Compute max and sum-exp in parallel\n    for (int i = threadIdx.y * blockDim.x + threadIdx.x; i < dim_size; i += blockDim.x * blockDim.y) {\n        val = input[batch_index * dim_size + i];\n        thread_max = fmaxf(thread_max, val);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    if (lane_id == 0) max_vals[threadIdx.y] = thread_max;\n    __syncthreads();\n\n    float row_max = max_vals[0];\n    for (int i = 1; i < blockDim.y; i++) {\n        row_max = fmaxf(row_max, max_vals[i]);\n    }\n\n    for (int i = threadIdx.y * blockDim.x + threadIdx.x; i < dim_size; i += blockDim.x * blockDim.y) {\n        val = input[batch_index * dim_size + i];\n        thread_sum += expf(val - row_max);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (lane_id == 0) sum_exps[threadIdx.y] = thread_sum;\n    __syncthreads();\n\n    float row_sum = sum_exps[0];\n    for (int i = 1; i < blockDim.y; i++) {\n        row_sum += sum_exps[i];\n    }\n\n    // Apply log-softmax\n    for (int i = threadIdx.y * blockDim.x + threadIdx.x; i < dim_size; i += blockDim.x * blockDim.y) {\n        val = input[batch_index * dim_size + i];\n        output[batch_index * dim_size + i] = val - row_max - logf(row_sum);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    dim3 threads(32, 8); // 256 threads per block\n    int shared_mem_size = 2 * threads.y * sizeof(float);\n\n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{The optimized algorithm computes the max and sum_exp values in parallel using shared memory within each thread block, then applies the log_softmax formula to each element with proper synchronization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // First pass: find max in the dimension\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    shared_mem[tid] = max_val;\n    __syncthreads();\n    \n    // Reduce max in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_mem[0];\n    __syncthreads();\n    \n    // Second pass: compute sum_exp\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    shared_mem[tid] = sum_exp;\n    __syncthreads();\n    \n    // Reduce sum in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_mem[0];\n    __syncthreads();\n    \n    // Final pass: compute log_softmax\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = input[batch_index * dim_size + tid] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads = 256;\n    int shared_mem_size = threads * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{The optimized algorithm computes the maximum value and sum of exponentials in parallel using shared memory for each batch element, then applies the log-softmax formula to all elements in parallel.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_data[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // First pass: find max value\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    shared_data[tid] = max_val;\n    __syncthreads();\n    \n    // Reduce max in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] = fmaxf(shared_data[tid], shared_data[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_data[0];\n    __syncthreads();\n    \n    // Second pass: compute sum of exponentials\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    shared_data[tid] = sum_exp;\n    __syncthreads();\n    \n    // Reduce sum in shared memory\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_data[0];\n    __syncthreads();\n    \n    // Final pass: compute log softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads_per_block = 256;\n    int shared_mem_size = threads_per_block * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the log_softmax kernel by first computing max values and sum_exp in parallel using shared memory reduction within each thread block, then applying the log_softmax formula to all elements in parallel.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Phase 1: Find max value in the row\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    shared_mem[tid] = max_val;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_mem[0];\n    __syncthreads();\n    \n    // Phase 2: Compute sum of exp(x - max_val)\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    shared_mem[tid] = sum_exp;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_mem[0];\n    __syncthreads();\n    \n    // Phase 3: Compute log_softmax for each element\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads_per_block = 256;\n    int shared_mem_size = threads_per_block * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimized algorithm computes max and sum-exp in parallel reductions per batch, then applies log-softmax formula to all elements in parallel.}",
        "function": "#include <torch/extension.h>\n#include <cub/cub.cuh>\n\ntemplate <int BLOCK_SIZE>\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    typedef cub::BlockReduce<float, BLOCK_SIZE> BlockReduce;\n    __shared__ typename BlockReduce::TempStorage temp_storage_max;\n    __shared__ typename BlockReduce::TempStorage temp_storage_sum;\n    __shared__ float shared_max;\n    __shared__ float shared_sum;\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Each block handles one batch element\n    if (batch_index >= batch_size) return;\n\n    const float *batch_input = input + batch_index * dim_size;\n    float *batch_output = output + batch_index * dim_size;\n\n    // Find max value in the batch element\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        thread_max = fmaxf(thread_max, batch_input[i]);\n    }\n    float block_max = BlockReduce(temp_storage_max).Reduce(thread_max, cub::Max());\n    if (tid == 0) shared_max = block_max;\n    __syncthreads();\n\n    // Compute sum of exp(x_i - max)\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        thread_sum += expf(batch_input[i] - shared_max);\n    }\n    float block_sum = BlockReduce(temp_storage_sum).Reduce(thread_sum, cub::Sum());\n    if (tid == 0) shared_sum = block_sum;\n    __syncthreads();\n\n    // Compute log softmax for all elements\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        batch_output[i] = batch_input[i] - shared_max - logf(shared_sum);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    const int BLOCK_SIZE = 256;\n    log_softmax_kernel<BLOCK_SIZE><<<batch_size, BLOCK_SIZE>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{The optimized algorithm computes log softmax by first finding the maximum value per batch using parallel reduction, then computes the sum of exponentials using another parallel reduction, and finally applies the log softmax formula to each element.}",
        "function": "#include <torch/extension.h>\n#include <cub/cub.cuh>\n\ntemplate <int BLOCK_SIZE>\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    \n    // First reduction: find max value\n    float max_val = -FLT_MAX;\n    for (int i = threadIdx.x; i < dim_size; i += BLOCK_SIZE) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    \n    typedef cub::BlockReduce<float, BLOCK_SIZE> BlockReduce;\n    __shared__ typename BlockReduce::TempStorage temp_storage;\n    float block_max = BlockReduce(temp_storage).Reduce(max_val, cub::Max());\n    \n    if (threadIdx.x == 0) {\n        shared_mem[0] = block_max;\n    }\n    __syncthreads();\n    max_val = shared_mem[0];\n    \n    // Second reduction: sum exp\n    float sum_exp = 0.0f;\n    for (int i = threadIdx.x; i < dim_size; i += BLOCK_SIZE) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    float block_sum = BlockReduce(temp_storage).Reduce(sum_exp, cub::Sum());\n    \n    if (threadIdx.x == 0) {\n        shared_mem[1] = block_sum;\n    }\n    __syncthreads();\n    float log_sum = logf(shared_mem[1]);\n    \n    // Apply log softmax\n    for (int i = threadIdx.x; i < dim_size; i += BLOCK_SIZE) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - log_sum;\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    const int BLOCK_SIZE = 256;\n    int shared_mem_size = 2 * sizeof(float);\n    \n    log_softmax_kernel<BLOCK_SIZE><<<batch_size, BLOCK_SIZE, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimized algorithm computes max and sum-exp in parallel using warp-level reductions, then applies log-softmax to each element with improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_max(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    int batch_idx = blockIdx.x;\n    if (batch_idx >= batch_size) return;\n\n    const float *row_in = input + batch_idx * dim_size;\n    float *row_out = output + batch_idx * dim_size;\n\n    // First pass: find max in row\n    float thread_max = -FLT_MAX;\n    for (int i = threadIdx.x; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, row_in[i]);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    if (threadIdx.x == 0) {\n        thread_max = warp_reduce_max(thread_max);\n    }\n    __syncthreads();\n\n    // Second pass: compute sum exp\n    float thread_sum = 0.0f;\n    for (int i = threadIdx.x; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(row_in[i] - thread_max);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (threadIdx.x == 0) {\n        thread_sum = warp_reduce_sum(thread_sum);\n    }\n    float log_sum = logf(thread_sum);\n    __syncthreads();\n\n    // Third pass: compute log softmax\n    for (int i = threadIdx.x; i < dim_size; i += blockDim.x) {\n        row_out[i] = row_in[i] - thread_max - log_sum;\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    dim3 blocks(batch_size);\n    dim3 threads(256);\n    log_softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{The optimized algorithm computes the maximum value and sum of exponentials per batch using parallel reduction, then applies the log-softmax formula to each element in parallel.}",
        "function": "#include <torch/extension.h>\n#include <cub/cub.cuh>\n\ntemplate<int BLOCK_SIZE>\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    __shared__ float s_max[BLOCK_SIZE];\n    __shared__ float s_sum[BLOCK_SIZE];\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Compute max value\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    s_max[tid] = max_val;\n    __syncthreads();\n\n    for (int s = BLOCK_SIZE / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            s_max[tid] = fmaxf(s_max[tid], s_max[tid + s]);\n        }\n        __syncthreads();\n    }\n\n    max_val = s_max[0];\n    __syncthreads();\n\n    // Compute sum of exponentials\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    s_sum[tid] = sum_exp;\n    __syncthreads();\n\n    for (int s = BLOCK_SIZE / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            s_sum[tid] += s_sum[tid + s];\n        }\n        __syncthreads();\n    }\n\n    sum_exp = s_sum[0];\n    float log_sum = logf(sum_exp);\n\n    // Compute log softmax\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - log_sum;\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    const int BLOCK_SIZE = 256;\n    log_softmax_kernel<BLOCK_SIZE><<<batch_size, BLOCK_SIZE>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize by computing max and sum-exp in parallel using warp-level reductions and shared memory, then applying log-softmax to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared[];\n    float *max_vals = shared;\n    float *sum_exps = shared + batch_size;\n    \n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Compute max within each warp\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (tid % 32 == 0) {\n        max_vals[batch_index] = thread_max;\n    }\n    __syncthreads();\n    \n    float max_val = max_vals[batch_index];\n    \n    // Compute sum_exp within each warp\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (tid % 32 == 0) {\n        sum_exps[batch_index] = thread_sum;\n    }\n    __syncthreads();\n    \n    float log_sum = logf(sum_exps[batch_index]);\n    \n    // Compute final values\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - log_sum;\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    dim3 blocks(batch_size);\n    int threads = 256;\n    size_t shared_mem = 2 * batch_size * sizeof(float);\n    \n    log_softmax_kernel<<<blocks, threads, shared_mem>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{The optimized algorithm computes the max and sum_exp values in parallel using warp-level reductions, then applies the log_softmax formula to each element with improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\n__device__ __forceinline__ float warp_reduce_max(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    extern __shared__ float shared_mem[];\n    float *max_vals = shared_mem;\n    float *sum_exps = shared_mem + blockDim.x / 32;\n    \n    // Each warp computes max and sum_exp for one batch\n    float max_val = -FLT_MAX;\n    float sum_exp = 0.0f;\n    \n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = input[batch_index * dim_size + i];\n        max_val = fmaxf(max_val, val);\n    }\n    \n    max_val = warp_reduce_max(max_val);\n    if (tid % 32 == 0) max_vals[tid / 32] = max_val;\n    __syncthreads();\n    \n    if (tid < 32) max_val = max_vals[tid];\n    max_val = warp_reduce_max(max_val);\n    __syncthreads();\n    \n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = input[batch_index * dim_size + i];\n        sum_exp += expf(val - max_val);\n    }\n    \n    sum_exp = warp_reduce_sum(sum_exp);\n    if (tid % 32 == 0) sum_exps[tid / 32] = sum_exp;\n    __syncthreads();\n    \n    if (tid < 32) sum_exp = sum_exps[tid];\n    sum_exp = warp_reduce_sum(sum_exp);\n    \n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = input[batch_index * dim_size + i];\n        output[batch_index * dim_size + i] = val - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads = min(1024, ((dim_size + 31) / 32) * 32;\n    int shared_mem_size = (threads / 32) * 2 * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimized implementation using parallel reduction for max and sum computations, followed by element-wise log-softmax calculation.}",
        "function": "#include <torch/extension.h>\n#include <cub/cub.cuh>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Load input to shared memory\n    float *shared_data = shared_mem;\n    if (tid < dim_size) {\n        shared_data[tid] = input[batch_index * dim_size + tid];\n    }\n    __syncthreads();\n\n    // Parallel reduction for max value\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, shared_data[i]);\n    }\n    max_val = cub::BlockReduce<float>(temp_storage).Reduce(max_val, cub::Max());\n    __syncthreads();\n\n    // Parallel reduction for sum_exp\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(shared_data[i] - max_val);\n    }\n    sum_exp = cub::BlockReduce<float>(temp_storage).Reduce(sum_exp, cub::Sum());\n    __syncthreads();\n\n    // Compute final log softmax\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = shared_data[tid] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads = min(1024, ((dim_size + 31) / 32) * 32);\n    size_t shared_mem_size = dim_size * sizeof(float) + cub::DeviceReduce::TempStorage<float>::AllocationSize;\n\n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimized algorithm computes max and sum-exp in parallel using warp-level reductions, then applies log-softmax formula with improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\ntemplate <int BLOCK_SIZE>\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    extern __shared__ float shared_mem[];\n    float* max_vals = shared_mem;\n    float* sum_exps = shared_mem + BLOCK_SIZE;\n    \n    // Step 1: Find max value in the dimension\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    \n    typedef cub::BlockReduce<float, BLOCK_SIZE> BlockReduce;\n    __shared__ typename BlockReduce::TempStorage temp_storage;\n    float block_max = BlockReduce(temp_storage).Reduce(thread_max, cub::Max());\n    \n    if (tid == 0) {\n        max_vals[0] = block_max;\n    }\n    __syncthreads();\n    \n    // Step 2: Compute sum of exponentials\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_vals[0]);\n    }\n    \n    float block_sum = BlockReduce(temp_storage).Sum(thread_sum);\n    \n    if (tid == 0) {\n        sum_exps[0] = block_sum;\n    }\n    __syncthreads();\n    \n    // Step 3: Compute log softmax\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_vals[0] - logf(sum_exps[0]);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    const int BLOCK_SIZE = 256;\n    int shared_mem_size = 2 * BLOCK_SIZE * sizeof(float);\n    \n    log_softmax_kernel<BLOCK_SIZE><<<batch_size, BLOCK_SIZE, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimized algorithm computes max and sum-exp in parallel using warp-level reductions, then applies log-softmax formula with improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    if (batch_index >= batch_size) return;\n    \n    const float *batch_input = input + batch_index * dim_size;\n    float *batch_output = output + batch_index * dim_size;\n\n    // Compute max value\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, batch_input[i]);\n    }\n    max_val = warpReduceMax(max_val);\n    if (threadIdx.x % 32 == 0) {\n        atomicMaxf(block_max + (threadIdx.x / 32), max_val);\n    }\n    __syncthreads();\n    \n    max_val = threadIdx.x < 32 ? block_max[threadIdx.x] : -FLT_MAX;\n    max_val = warpReduceMax(max_val);\n    \n    // Compute sum_exp\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(batch_input[i] - max_val);\n    }\n    sum_exp = warpReduceSum(sum_exp);\n    if (threadIdx.x % 32 == 0) {\n        atomicAdd(block_sum + (threadIdx.x / 32), sum_exp);\n    }\n    __syncthreads();\n    \n    sum_exp = threadIdx.x < 32 ? block_sum[threadIdx.x] : 0.0f;\n    sum_exp = warpReduceSum(sum_exp);\n    \n    // Compute log_softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        batch_output[i] = batch_input[i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    dim3 blocks(batch_size);\n    dim3 threads(256);\n    \n    log_softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize by parallelizing max and sum computations across threads, using shared memory for intermediate results, and reducing memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_data[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Load input to shared memory\n    if (tid < dim_size) {\n        shared_data[tid] = input[batch_index * dim_size + tid];\n    }\n    __syncthreads();\n\n    // Find max value\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, shared_data[i]);\n    }\n    max_val = warpReduceMax(max_val);\n    if (tid == 0) {\n        shared_data[dim_size] = max_val;\n    }\n    __syncthreads();\n    max_val = shared_data[dim_size];\n\n    // Compute sum of exponentials\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(shared_data[i] - max_val);\n    }\n    sum_exp = warpReduceSum(sum_exp);\n    if (tid == 0) {\n        shared_data[dim_size + 1] = sum_exp;\n    }\n    __syncthreads();\n    sum_exp = shared_data[dim_size + 1];\n\n    // Compute final log softmax\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = shared_data[tid] - max_val - logf(sum_exp);\n    }\n}\n\n__device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads = min(1024, dim_size);\n    int shared_mem_size = (dim_size + 2) * sizeof(float);\n\n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize by parallelizing max reduction and sum-exp computation across threads using shared memory, then applying log-softmax element-wise.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Load input to shared memory\n    float *shared_input = shared_mem;\n    float *shared_max = shared_mem + dim_size;\n    float *shared_sum = shared_mem + dim_size + 1;\n    \n    if (tid < dim_size) {\n        shared_input[tid] = input[batch_index * dim_size + tid];\n    }\n    __syncthreads();\n    \n    // Parallel max reduction\n    if (tid < dim_size) {\n        shared_max[tid] = shared_input[tid];\n    }\n    __syncthreads();\n    \n    for (int s = blockDim.x/2; s > 0; s >>= 1) {\n        if (tid < s && tid + s < dim_size) {\n            shared_max[tid] = fmaxf(shared_max[tid], shared_max[tid + s]);\n        }\n        __syncthreads();\n    }\n    \n    float max_val = shared_max[0];\n    __syncthreads();\n    \n    // Parallel sum-exp computation\n    if (tid < dim_size) {\n        shared_sum[tid] = expf(shared_input[tid] - max_val);\n    }\n    __syncthreads();\n    \n    for (int s = blockDim.x/2; s > 0; s >>= 1) {\n        if (tid < s && tid + s < dim_size) {\n            shared_sum[tid] += shared_sum[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    // Compute log-softmax\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = shared_input[tid] - max_val - logf(shared_sum[0]);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads = min(1024, dim_size);\n    int shared_mem_size = (dim_size + 1 + 1) * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{The optimized algorithm computes max values and sum of exponentials per batch using parallel reduction, then applies log-softmax in a single pass with shared memory for intermediate results.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\ntemplate <int BLOCK_SIZE>\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    float *max_vals = shared_mem;\n    float *sum_exps = shared_mem + blockDim.x;\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Step 1: Parallel reduction for max value\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    thread_max = cub::BlockReduce<float, BLOCK_SIZE>(thread_max, cub::Max()).Reduce(thread_max, cub::Max());\n    if (tid == 0) {\n        max_vals[0] = thread_max;\n    }\n    __syncthreads();\n\n    // Step 2: Parallel reduction for sum of exps\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_vals[0]);\n    }\n    thread_sum = cub::BlockReduce<float, BLOCK_SIZE>(thread_sum, cub::Sum()).Reduce(thread_sum, cub::Sum());\n    if (tid == 0) {\n        sum_exps[0] = thread_sum;\n    }\n    __syncthreads();\n\n    // Step 3: Compute final log-softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_vals[0] - logf(sum_exps[0]);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    const int BLOCK_SIZE = 256;\n    int shared_mem_size = 2 * BLOCK_SIZE * sizeof(float);\n\n    log_softmax_kernel<BLOCK_SIZE><<<batch_size, BLOCK_SIZE, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{The optimized algorithm computes the max and sum-exp values per batch in parallel using shared memory reduction, then applies the log-softmax formula to each element.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Phase 1: Find max value\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    shared_mem[tid] = max_val;\n    __syncthreads();\n    \n    // Reduction for max\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] = fmaxf(shared_mem[tid], shared_mem[tid + s]);\n        }\n        __syncthreads();\n    }\n    max_val = shared_mem[0];\n    __syncthreads();\n    \n    // Phase 2: Compute sum_exp\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    shared_mem[tid] = sum_exp;\n    __syncthreads();\n    \n    // Reduction for sum_exp\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_mem[tid] += shared_mem[tid + s];\n        }\n        __syncthreads();\n    }\n    sum_exp = shared_mem[0];\n    __syncthreads();\n    \n    // Phase 3: Compute log_softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads_per_block = 256;\n    int shared_mem_size = threads_per_block * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimized algorithm: Compute max values and sum of exponentials per batch using parallel reduction, then apply log-softmax formula to each element with shared memory for intermediate results.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    float *max_vals = shared_mem;\n    float *sum_exps = &shared_mem[batch_size];\n    \n    int batch_idx = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Find max value in the row\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_idx * dim_size + i]);\n    }\n    \n    thread_max = cub::BlockReduce<float>(thread_max).Reduce(cub::Max());\n    if (tid == 0) {\n        max_vals[batch_idx] = thread_max;\n    }\n    __syncthreads();\n    \n    // Compute sum of exponentials\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_idx * dim_size + i] - max_vals[batch_idx]);\n    }\n    \n    thread_sum = cub::BlockReduce<float>(thread_sum).Sum();\n    if (tid == 0) {\n        sum_exps[batch_idx] = thread_sum;\n    }\n    __syncthreads();\n    \n    // Compute log softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_idx * dim_size + i] = input[batch_idx * dim_size + i] - max_vals[batch_idx] - logf(sum_exps[batch_idx]);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads = 256;\n    int shared_mem_size = 2 * batch_size * sizeof(float);\n    \n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize by computing max and sum-reduction in parallel using warp-level primitives and shared memory, then broadcasting results for log-softmax calculation.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n#include <cub/cub.cuh>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Step 1: Find max value in the row\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    \n    float max_val = thread_max;\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        max_val = fmaxf(max_val, __shfl_down_sync(0xFFFFFFFF, max_val, offset));\n    }\n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n    \n    // Step 2: Compute sum of exp(x - max)\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    float sum_exp = thread_sum;\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        sum_exp += __shfl_down_sync(0xFFFFFFFF, sum_exp, offset);\n    }\n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n    \n    // Step 3: Compute log softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    int threads = min(1024, ((dim_size + 31) / 32) * 32);\n    log_softmax_kernel<<<batch_size, threads, 0>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{The optimized algorithm uses parallel reduction for max and sum computations, followed by a separate kernel for log softmax calculation, to minimize redundant computations and improve memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\n__global__ void log_softmax_reduce_kernel(const float* input, float* max_vals, float* sum_vals, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    float* max_shared = shared_mem;\n    float* sum_shared = &shared_mem[blockDim.x];\n\n    int batch_idx = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Load data and find local max\n    float local_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = input[batch_idx * dim_size + i];\n        local_max = fmaxf(local_max, val);\n    }\n    max_shared[tid] = local_max;\n\n    // Parallel reduction for max\n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        __syncthreads();\n        if (tid < stride) {\n            max_shared[tid] = fmaxf(max_shared[tid], max_shared[tid + stride]);\n        }\n    }\n\n    // Broadcast max value\n    __syncthreads();\n    float max_val = max_shared[0];\n\n    // Compute sum of exp(x - max)\n    float local_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        local_sum += expf(input[batch_idx * dim_size + i] - max_val);\n    }\n    sum_shared[tid] = local_sum;\n\n    // Parallel reduction for sum\n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        __syncthreads();\n        if (tid < stride) {\n            sum_shared[tid] += sum_shared[tid + stride];\n        }\n    }\n\n    // Store results\n    if (tid == 0) {\n        max_vals[batch_idx] = max_val;\n        sum_vals[batch_idx] = sum_shared[0];\n    }\n}\n\n__global__ void log_softmax_final_kernel(const float* input, float* output, const float* max_vals, const float* sum_vals, int batch_size, int dim_size) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < batch_size * dim_size) {\n        int batch_idx = index / dim_size;\n        int dim_idx = index % dim_size;\n        float max_val = max_vals[batch_idx];\n        float sum_val = sum_vals[batch_idx];\n        output[index] = input[index] - max_val - logf(sum_val);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    // Allocate temporary buffers\n    auto max_vals = torch::empty({batch_size}, input.options());\n    auto sum_vals = torch::empty({batch_size}, input.options());\n\n    // First kernel: compute max and sum per batch\n    int reduce_threads = 256;\n    log_softmax_reduce_kernel<<<batch_size, reduce_threads, 2 * reduce_threads * sizeof(float)>>>(\n        input.data_ptr<float>(),\n        max_vals.data_ptr<float>(),\n        sum_vals.data_ptr<float>(),\n        batch_size,\n        dim_size\n    );\n\n    // Second kernel: compute final log softmax\n    int threads_per_block = 256;\n    int blocks = (batch_size * dim_size + threads_per_block - 1) / threads_per_block;\n    log_softmax_final_kernel<<<blocks, threads_per_block>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        max_vals.data_ptr<float>(),\n        sum_vals.data_ptr<float>(),\n        batch_size,\n        dim_size\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize by parallelizing max reduction and sum-exp calculations across threads within each batch, then computing log-softmax values using shared memory for efficient data sharing.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Load input into shared memory\n    float *shared_input = shared_mem;\n    if (tid < dim_size) {\n        shared_input[tid] = input[batch_index * dim_size + tid];\n    }\n    __syncthreads();\n\n    // Parallel max reduction\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        max_val = fmaxf(max_val, shared_input[i]);\n    }\n    max_val = blockReduceMax(max_val);\n    __syncthreads();\n\n    // Parallel sum-exp\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        sum_exp += expf(shared_input[i] - max_val);\n    }\n    sum_exp = blockReduceSum(sum_exp);\n    __syncthreads();\n\n    // Compute log softmax\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = shared_input[tid] - max_val - logf(sum_exp);\n    }\n}\n\n__device__ float blockReduceMax(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    val = warpReduceMax(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : -FLT_MAX;\n    if (wid == 0) val = warpReduceMax(val);\n    return val;\n}\n\n__device__ float blockReduceSum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    val = warpReduceSum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) val = warpReduceSum(val);\n    return val;\n}\n\n__device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    return val;\n}\n\n__device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    dim3 blocks(batch_size);\n    int threads = min(1024, ((dim_size + 31) / 32) * 32);\n    size_t shared_mem_size = dim_size * sizeof(float);\n\n    log_softmax_kernel<<<blocks, threads, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{The optimized algorithm computes the max and sum-exp values per batch using parallel reduction, then applies the log-softmax formula to each element with proper thread synchronization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\ntemplate <int BLOCK_SIZE>\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    float* max_vals = shared_mem;\n    float* sum_exps = shared_mem + blockDim.x;\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Find max value in the row\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    thread_max = cub::BlockReduce<float, BLOCK_SIZE>(thread_max, cub::Max()).Reduce(thread_max, cub::Max());\n    if (tid == 0) {\n        max_vals[0] = thread_max;\n    }\n    __syncthreads();\n\n    // Compute sum of exp(x_i - max)\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_vals[0]);\n    }\n    thread_sum = cub::BlockReduce<float, BLOCK_SIZE>(thread_sum, cub::Sum()).Reduce(thread_sum, cub::Sum());\n    if (tid == 0) {\n        sum_exps[0] = thread_sum;\n    }\n    __syncthreads();\n\n    // Compute log softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_vals[0] - logf(sum_exps[0]);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    const int BLOCK_SIZE = 256;\n    int shared_mem_size = 2 * BLOCK_SIZE * sizeof(float);\n\n    log_softmax_kernel<BLOCK_SIZE><<<batch_size, BLOCK_SIZE, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimized algorithm: Use parallel reduction to compute max and sum_exp values per batch, then compute log_softmax values in a separate kernel pass with shared memory for efficient data access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\n__global__ void log_softmax_reduce_kernel(const float* input, float* max_vals, float* sum_exps, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    float* max_shared = shared_mem;\n    float* sum_shared = &shared_mem[blockDim.x];\n\n    int batch_idx = blockIdx.x;\n    int tid = threadIdx.x;\n\n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = input[batch_idx * dim_size + i];\n        thread_max = fmaxf(thread_max, val);\n    }\n\n    max_shared[tid] = thread_max;\n    __syncthreads();\n\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            max_shared[tid] = fmaxf(max_shared[tid], max_shared[tid + s]);\n        }\n        __syncthreads();\n    }\n\n    float batch_max = max_shared[0];\n    __syncthreads();\n\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_idx * dim_size + i] - batch_max);\n    }\n\n    sum_shared[tid] = thread_sum;\n    __syncthreads();\n\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sum_shared[tid] += sum_shared[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        max_vals[batch_idx] = batch_max;\n        sum_exps[batch_idx] = sum_shared[0];\n    }\n}\n\n__global__ void log_softmax_compute_kernel(const float* input, float* output, const float* max_vals, const float* sum_exps, int batch_size, int dim_size) {\n    int batch_idx = blockIdx.x;\n    int dim_idx = threadIdx.x;\n\n    if (dim_idx < dim_size) {\n        float max_val = max_vals[batch_idx];\n        float sum_exp = sum_exps[batch_idx];\n        output[batch_idx * dim_size + dim_idx] = input[batch_idx * dim_size + dim_idx] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    auto max_vals = torch::empty({batch_size}, input.options());\n    auto sum_exps = torch::empty({batch_size}, input.options());\n\n    int threads = min(1024, dim_size);\n    int shared_mem_size = 2 * threads * sizeof(float);\n\n    log_softmax_reduce_kernel<<<batch_size, threads, shared_mem_size>>>(\n        input.data_ptr<float>(), \n        max_vals.data_ptr<float>(), \n        sum_exps.data_ptr<float>(), \n        batch_size, \n        dim_size\n    );\n\n    log_softmax_compute_kernel<<<batch_size, dim_size>>>(\n        input.data_ptr<float>(), \n        output.data_ptr<float>(), \n        max_vals.data_ptr<float>(), \n        sum_exps.data_ptr<float>(), \n        batch_size, \n        dim_size\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{The optimized kernel uses parallel reduction for max and sum computations, processes elements in shared memory, and avoids redundant calculations by computing max and sum in a single pass.}",
        "function": "#include <torch/extension.h>\n\ntemplate <int BLOCK_SIZE>\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    float *max_vals = shared_mem;\n    float *sum_exps = shared_mem + blockDim.x;\n\n    int batch_index = blockIdx.x;\n    if (batch_index >= batch_size) return;\n\n    const float *batch_input = input + batch_index * dim_size;\n    float *batch_output = output + batch_index * dim_size;\n\n    // Parallel reduction for max value\n    float thread_max = -FLT_MAX;\n    for (int i = threadIdx.x; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, batch_input[i]);\n    }\n    max_vals[threadIdx.x] = thread_max;\n    __syncthreads();\n\n    for (int s = BLOCK_SIZE / 2; s > 0; s >>= 1) {\n        if (threadIdx.x < s) {\n            max_vals[threadIdx.x] = fmaxf(max_vals[threadIdx.x], max_vals[threadIdx.x + s]);\n        }\n        __syncthreads();\n    }\n    float max_val = max_vals[0];\n    __syncthreads();\n\n    // Parallel reduction for sum of exps\n    float thread_sum = 0.0f;\n    for (int i = threadIdx.x; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(batch_input[i] - max_val);\n    }\n    sum_exps[threadIdx.x] = thread_sum;\n    __syncthreads();\n\n    for (int s = BLOCK_SIZE / 2; s > 0; s >>= 1) {\n        if (threadIdx.x < s) {\n            sum_exps[threadIdx.x] += sum_exps[threadIdx.x + s];\n        }\n        __syncthreads();\n    }\n    float sum_exp = sum_exps[0];\n    __syncthreads();\n\n    // Compute final log softmax\n    for (int i = threadIdx.x; i < dim_size; i += blockDim.x) {\n        batch_output[i] = batch_input[i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    const int BLOCK_SIZE = 256;\n    int shared_mem_size = 2 * BLOCK_SIZE * sizeof(float);\n\n    log_softmax_kernel<BLOCK_SIZE><<<batch_size, BLOCK_SIZE, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize by computing max and sum-reduce in parallel using warp-level primitives and shared memory, then broadcasting results for element-wise log-softmax calculation.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared[];\n    float *max_vals = shared;\n    float *sum_vals = &shared[batch_size];\n    \n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Compute max per batch\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    thread_max = warpReduceMax(thread_max);\n    if (tid % 32 == 0) {\n        max_vals[batch_index] = thread_max;\n    }\n    __syncthreads();\n    \n    // Compute sum_exp per batch\n    float thread_sum = 0.0f;\n    float max_val = max_vals[batch_index];\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    thread_sum = warpReduceSum(thread_sum);\n    if (tid % 32 == 0) {\n        sum_vals[batch_index] = thread_sum;\n    }\n    __syncthreads();\n    \n    // Compute log_softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_vals[batch_index]);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    dim3 blocks(batch_size);\n    int threads = min(1024, ((dim_size + 31) / 32) * 32);\n    size_t shared_mem = batch_size * 2 * sizeof(float);\n    \n    log_softmax_kernel<<<blocks, threads, shared_mem>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{The optimized algorithm computes the maximum value and sum of exponentials in parallel using warp-level reductions, then applies the log-softmax formula to each element with improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_max(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    }\n    return val;\n}\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    const int warp_size = 32;\n    \n    __shared__ float s_max[32];\n    __shared__ float s_sum[32];\n    \n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n    \n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    \n    float max_val = warp_reduce_max(thread_max);\n    if (tid % warp_size == 0) {\n        s_max[tid / warp_size] = max_val;\n    }\n    __syncthreads();\n    \n    if (tid < warp_size) {\n        max_val = (tid < blockDim.x / warp_size) ? s_max[tid] : -FLT_MAX;\n        max_val = warp_reduce_max(max_val);\n    }\n    __syncthreads();\n    \n    max_val = __shfl_sync(0xFFFFFFFF, max_val, 0);\n    \n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    \n    float sum_exp = warp_reduce_sum(thread_sum);\n    if (tid % warp_size == 0) {\n        s_sum[tid / warp_size] = sum_exp;\n    }\n    __syncthreads();\n    \n    if (tid < warp_size) {\n        sum_exp = (tid < blockDim.x / warp_size) ? s_sum[tid] : 0.0f;\n        sum_exp = warp_reduce_sum(sum_exp);\n    }\n    __syncthreads();\n    \n    sum_exp = __shfl_sync(0xFFFFFFFF, sum_exp, 0);\n    \n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    dim3 blocks(batch_size);\n    dim3 threads(256);\n    \n    log_softmax_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{The optimized algorithm computes log softmax by first finding the maximum value per batch using parallel reduction, then computes the sum of exponentials (shifted by max) using another parallel reduction, and finally applies the log softmax formula to each element.}",
        "function": "#include <torch/extension.h>\n\n__device__ void warp_reduce_max(float& val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n}\n\n__device__ void warp_reduce_sum(float& val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n}\n\n__global__ void log_softmax_kernel(const float* input, float* output, int batch_size, int dim_size) {\n    extern __shared__ float shared_mem[];\n    float* max_vals = shared_mem;\n    float* sum_exps = shared_mem + batch_size;\n\n    int batch_idx = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Find max value for this batch\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_idx * dim_size + i]);\n    }\n    thread_max = block_reduce_max(thread_max);\n\n    if (tid == 0) {\n        max_vals[batch_idx] = thread_max;\n    }\n    __syncthreads();\n\n    // Compute sum of exponentials\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_idx * dim_size + i] - max_vals[batch_idx]);\n    }\n    thread_sum = block_reduce_sum(thread_sum);\n\n    if (tid == 0) {\n        sum_exps[batch_idx] = thread_sum;\n    }\n    __syncthreads();\n\n    // Compute log softmax\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_idx * dim_size + i] = input[batch_idx * dim_size + i] - max_vals[batch_idx] - logf(sum_exps[batch_idx]);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads = min(1024, ((dim_size + 31) / 32) * 32);\n    size_t shared_mem_size = batch_size * sizeof(float) * 2;\n\n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize by computing max and sum-exp in parallel using shared memory and warp-level reductions, then apply log-softmax to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__inline__ __device__ float warp_reduce_max(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared[];\n    float *max_vals = shared;\n    float *sum_exps = shared + blockDim.x;\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    float thread_max = -FLT_MAX;\n    float thread_sum = 0.0f;\n\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n\n    float block_max = warp_reduce_max(thread_max);\n    if (tid % 32 == 0) {\n        max_vals[tid / 32] = block_max;\n    }\n    __syncthreads();\n\n    if (tid < 32) {\n        block_max = (tid < blockDim.x / 32) ? max_vals[tid] : -FLT_MAX;\n        block_max = warp_reduce_max(block_max);\n        if (tid == 0) {\n            max_vals[0] = block_max;\n        }\n    }\n    __syncthreads();\n    block_max = max_vals[0];\n\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - block_max);\n    }\n\n    float block_sum = warp_reduce_sum(thread_sum);\n    if (tid % 32 == 0) {\n        sum_exps[tid / 32] = block_sum;\n    }\n    __syncthreads();\n\n    if (tid < 32) {\n        block_sum = (tid < blockDim.x / 32) ? sum_exps[tid] : 0.0f;\n        block_sum = warp_reduce_sum(block_sum);\n        if (tid == 0) {\n            sum_exps[0] = block_sum;\n        }\n    }\n    __syncthreads();\n    block_sum = sum_exps[0];\n\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = input[batch_index * dim_size + tid] - block_max - logf(block_sum);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads_per_block = 256;\n    int shared_mem_size = (threads_per_block / 32 + 1) * sizeof(float) * 2;\n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{The optimized algorithm computes the max and sum-exp values per batch using parallel reduction, then applies the log-softmax formula to each element in parallel.}",
        "function": "#include <torch/extension.h>\n#include <cub/block/block_reduce.cuh>\n\ntemplate <int BLOCK_SIZE>\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    typedef cub::BlockReduce<float, BLOCK_SIZE> BlockReduce;\n    __shared__ typename BlockReduce::TempStorage temp_storage_max;\n    __shared__ typename BlockReduce::TempStorage temp_storage_sum;\n    __shared__ float max_val_shared;\n    __shared__ float sum_exp_shared;\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Compute max value for the batch\n    float max_val = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        max_val = fmaxf(max_val, input[batch_index * dim_size + i]);\n    }\n    max_val = BlockReduce(temp_storage_max).Reduce(max_val, cub::Max());\n    if (tid == 0) {\n        max_val_shared = max_val;\n    }\n    __syncthreads();\n\n    // Compute sum of exp(x - max_val)\n    float sum_exp = 0.0f;\n    for (int i = tid; i < dim_size; i += BLOCK_SIZE) {\n        sum_exp += expf(input[batch_index * dim_size + i] - max_val_shared);\n    }\n    sum_exp = BlockReduce(temp_storage_sum).Reduce(sum_exp, cub::Sum());\n    if (tid == 0) {\n        sum_exp_shared = sum_exp;\n    }\n    __syncthreads();\n\n    // Compute log softmax for each element\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = input[batch_index * dim_size + tid] - max_val_shared - logf(sum_exp_shared);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    const int BLOCK_SIZE = 256;\n    log_softmax_kernel<BLOCK_SIZE><<<batch_size, BLOCK_SIZE>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{The optimized algorithm computes the max and sum-exp in parallel using warp-level reductions within each thread block, then applies the log-softmax formula with improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_max(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val = fmaxf(val, __shfl_down_sync(0xffffffff, val, offset));\n    return val;\n}\n\ntemplate <typename T>\n__device__ __forceinline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    float *max_vals = shared_mem;\n    float *sum_exps = shared_mem + blockDim.x;\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Each thread processes one element to find max\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    thread_max = warp_reduce_max(thread_max);\n    if (tid % 32 == 0) {\n        max_vals[tid / 32] = thread_max;\n    }\n    __syncthreads();\n\n    // First warp reduces all partial max values\n    if (tid < 32) {\n        thread_max = (tid < (blockDim.x + 31) / 32) ? max_vals[tid] : -FLT_MAX;\n        thread_max = warp_reduce_max(thread_max);\n        if (tid == 0) {\n            max_vals[0] = thread_max;\n        }\n    }\n    __syncthreads();\n    float max_val = max_vals[0];\n\n    // Compute sum_exp\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    thread_sum = warp_reduce_sum(thread_sum);\n    if (tid % 32 == 0) {\n        sum_exps[tid / 32] = thread_sum;\n    }\n    __syncthreads();\n\n    // First warp reduces all partial sum values\n    if (tid < 32) {\n        thread_sum = (tid < (blockDim.x + 31) / 32) ? sum_exps[tid] : 0.0f;\n        thread_sum = warp_reduce_sum(thread_sum);\n        if (tid == 0) {\n            sum_exps[0] = thread_sum;\n        }\n    }\n    __syncthreads();\n    float sum_exp = sum_exps[0];\n\n    // Compute final output\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads = 256;\n    int shared_mem_size = 2 * ((threads + 31) / 32) * sizeof(float);\n\n    log_softmax_kernel<<<batch_size, threads, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{The optimized algorithm uses parallel reduction for max and sum computations, splits the work into two kernels (one for max reduction, one for sum reduction and final computation), and employs shared memory for efficient data access.}",
        "function": "#include <torch/extension.h>\n\n__global__ void max_reduce_kernel(const float* input, float* max_vals, int batch_size, int dim_size) {\n    extern __shared__ float shared_max[];\n    int batch_idx = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    float local_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        local_max = fmaxf(local_max, input[batch_idx * dim_size + i]);\n    }\n    \n    shared_max[tid] = local_max;\n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_max[tid] = fmaxf(shared_max[tid], shared_max[tid + s]);\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        max_vals[batch_idx] = shared_max[0];\n    }\n}\n\n__global__ void log_softmax_kernel(const float* input, float* output, const float* max_vals, int batch_size, int dim_size) {\n    extern __shared__ float shared_data[];\n    int batch_idx = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    float max_val = max_vals[batch_idx];\n    float local_sum = 0.0f;\n    \n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        float val = expf(input[batch_idx * dim_size + i] - max_val);\n        shared_data[tid] = val;\n        local_sum += val;\n    }\n    \n    __syncthreads();\n    \n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    float sum_exp = shared_data[0];\n    float log_sum = logf(sum_exp);\n    \n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_idx * dim_size + i] = input[batch_idx * dim_size + i] - max_val - log_sum;\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    auto max_vals = torch::empty({batch_size}, input.options());\n    \n    int threads = 256;\n    max_reduce_kernel<<<batch_size, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), max_vals.data_ptr<float>(), batch_size, dim_size);\n    log_softmax_kernel<<<batch_size, threads, threads * sizeof(float)>>>(input.data_ptr<float>(), output.data_ptr<float>(), max_vals.data_ptr<float>(), batch_size, dim_size);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimize by computing max and sum-exp in parallel using shared memory within thread blocks, then applying log-softmax to each element.}",
        "function": "#include <torch/extension.h>\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    float *max_vals = shared_mem;\n    float *sum_exps = &shared_mem[batch_size];\n    \n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Compute max value for each batch\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    \n    thread_max = blockReduceMax(thread_max);\n    if (tid == 0) {\n        max_vals[batch_index] = thread_max;\n    }\n    __syncthreads();\n    \n    // Compute sum_exp for each batch\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_vals[batch_index]);\n    }\n    \n    thread_sum = blockReduceSum(thread_sum);\n    if (tid == 0) {\n        sum_exps[batch_index] = thread_sum;\n    }\n    __syncthreads();\n    \n    // Compute log softmax for each element\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        output[batch_index * dim_size + i] = input[batch_index * dim_size + i] - max_vals[batch_index] - logf(sum_exps[batch_index]);\n    }\n}\n\n__device__ float blockReduceMax(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n    \n    val = warpReduceMax(val);\n    \n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n    \n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : -FLT_MAX;\n    if (wid == 0) val = warpReduceMax(val);\n    return val;\n}\n\n__device__ float blockReduceSum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n    \n    val = warpReduceSum(val);\n    \n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n    \n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) val = warpReduceSum(val);\n    return val;\n}\n\n__device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n    \n    dim3 blocks(batch_size);\n    dim3 threads(256);\n    size_t shared_mem_size = batch_size * sizeof(float) * 2;\n    \n    log_softmax_kernel<<<blocks, threads, shared_mem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{The optimized algorithm uses parallel reduction within thread blocks to compute the max and sum-exp values efficiently, then applies the log-softmax operation to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__inline__ __device__ float warpReduceMax(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val = fmaxf(val, __shfl_down_sync(0xFFFFFFFF, val, offset));\n    return val;\n}\n\n__inline__ __device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void log_softmax_kernel(const float *input, float *output, int batch_size, int dim_size, int dim) {\n    extern __shared__ float shared_mem[];\n    float *max_vals = shared_mem;\n    float *sum_exps = &shared_mem[batch_size];\n\n    int batch_index = blockIdx.x;\n    int tid = threadIdx.x;\n\n    // Compute max value for this batch\n    float thread_max = -FLT_MAX;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_max = fmaxf(thread_max, input[batch_index * dim_size + i]);\n    }\n    float block_max = warpReduceMax(thread_max);\n    if (tid % 32 == 0) {\n        atomicMaxFloat(max_vals + batch_index, block_max);\n    }\n    __syncthreads();\n\n    float max_val = max_vals[batch_index];\n\n    // Compute sum_exp for this batch\n    float thread_sum = 0.0f;\n    for (int i = tid; i < dim_size; i += blockDim.x) {\n        thread_sum += expf(input[batch_index * dim_size + i] - max_val);\n    }\n    float block_sum = warpReduceSum(thread_sum);\n    if (tid % 32 == 0) {\n        atomicAdd(sum_exps + batch_index, block_sum);\n    }\n    __syncthreads();\n\n    float sum_exp = sum_exps[batch_index];\n\n    // Compute log_softmax for each element\n    if (tid < dim_size) {\n        output[batch_index * dim_size + tid] = input[batch_index * dim_size + tid] - max_val - logf(sum_exp);\n    }\n}\n\ntorch::Tensor log_softmax_cuda(torch::Tensor input, int dim) {\n    auto output = torch::empty_like(input);\n    int batch_size = input.size(0);\n    int dim_size = input.size(1);\n\n    int threads_per_block = 256;\n    int shared_mem_size = 2 * batch_size * sizeof(float);\n\n    log_softmax_kernel<<<batch_size, threads_per_block, shared_mem_size>>>(\n        input.data_ptr<float>(), output.data_ptr<float>(), batch_size, dim_size, dim);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &log_softmax_cuda, \"LogSoftmax activation (CUDA)\");\n}",
        "score": null
    }
]