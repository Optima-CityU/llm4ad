[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the tanh kernel by using faster math operations and improved memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n#include <cmath>\n\n__forceinline__ __device__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f)));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    output[idx] = fast_tanh(input[idx]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads, 0, c10::cuda::getCurrentCUDAStream()>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the Tanh CUDA kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing warp divergence.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__device__ __forceinline__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        output[idx] = fast_tanh(input[idx]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining accuracy through precise range handling and leveraging GPU memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__device__ __forceinline__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        float val = input[idx];\n        output[idx] = (fabsf(val) < 4.97f) ? fast_tanh(val) : copysignf(1.0f, val);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining accuracy through precise bounds checking and leveraging GPU warp-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__device__ __forceinline__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        float val = input[idx];\n        if (val < -4.97f) {\n            output[idx] = -1.0f;\n        } else if (val > 4.97f) {\n            output[idx] = 1.0f;\n        } else {\n            output[idx] = fast_tanh(val);\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining accuracy, unrolling loops, and increasing thread utilization with better block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__forceinline__ __device__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < size; i += stride) {\n        output[i] = fast_tanh(input[i]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    \n    int device_id;\n    cudaGetDevice(&device_id);\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device_id);\n    \n    int threads = min(1024, prop.maxThreadsPerBlock);\n    int blocks = min((size + threads - 1) / threads, prop.multiProcessorCount * 32);\n    \n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining sufficient precision, increasing thread utilization, and minimizing warp divergence.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\n__forceinline__ __device__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        output[idx] = fast_tanh(input[idx]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads, 0, c10::cuda::getCurrentCUDAStream()>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the tanh kernel by unrolling loops, increasing occupancy through better block/thread configuration, and using fast math approximations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n#define UNROLL_FACTOR 4\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * UNROLL_FACTOR;\n  #pragma unroll\n  for (int i = 0; i < UNROLL_FACTOR; ++i) {\n    if (idx + i < size) {\n      output[idx + i] = tanhf(input[idx + i]);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;  // Better occupancy for RTX 4090\n  int blocks = (size + threads * UNROLL_FACTOR - 1) / (threads * UNROLL_FACTOR);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the Tanh kernel by utilizing warp-level parallelism, reducing thread divergence, and using fast math approximations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n#include <cmath>\n\n__forceinline__ __device__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < size; i += stride) {\n        output[i] = fast_tanh(input[i]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int numSMs;\n    cudaDeviceGetAttribute(&numSMs, cudaDevAttrMultiProcessorCount, device);\n    \n    int threads = 256;\n    int blocks = 4 * numSMs;\n    \n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining sufficient precision, unrolling loops, and increasing thread utilization with optimal block and grid sizes.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n#include <cmath>\n\n__forceinline__ __device__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < size; i += stride) {\n        output[i] = fast_tanh(input[i]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    \n    int threads = 256;\n    int blocks = min(65535, (size + threads - 1) / threads);\n    \n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n    tanh_kernel<<<blocks, threads, 0, stream>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining correctness, and ensure proper memory coalescing and block/thread configuration for the RTX 4090 architecture.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    output[idx] = __tanhf(x);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining accuracy, increasing thread count, and ensuring proper memory alignment for coalesced access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    const float x = input[idx];\n    output[idx] = __tanhf(x);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  const int size = input.numel();\n  const int threads = 1024;\n  const int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads, 0, c10::cuda::getCurrentCUDAStream()>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the tanh kernel by using warp-level parallelism and reducing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float val = input[idx];\n    float exp2x = expf(2.0f * val);\n    output[idx] = (exp2x - 1.0f) / (exp2x + 1.0f);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the CUDA kernel by using vectorized memory accesses and loop unrolling to process multiple elements per thread while maintaining correct Tanh computation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  #pragma unroll\n  for (int i = 0; i < VEC_SIZE; ++i) {\n    if (idx + i < size) {\n      output[idx + i] = tanhf(input[idx + i]);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the tanh CUDA kernel by using warp-level parallelism and reducing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < size; i += stride) {\n    output[i] = tanhf(input[i]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  \n  int threads = 256;\n  int blocks = min(65535, (size + threads - 1) / threads);\n  \n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the tanh kernel by using faster math functions and increasing parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n#include <cuda_runtime_api.h>\n#include <cmath>\n\n__forceinline__ __device__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        output[idx] = fast_tanh(input[idx]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the tanh kernel by utilizing warp-level parallelism and minimizing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    float exp2x = __expf(2.0f * x);\n    output[idx] = (exp2x - 1.0f) / (exp2x + 1.0f);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining accuracy through precise computation of tanh, and ensure proper memory coalescing and thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__forceinline__ __device__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        float val = input[idx];\n        output[idx] = (fabsf(val) < 4.97f) ? fast_tanh(val) : copysignf(1.0f, val);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the CUDA kernel by using faster intrinsic functions (__tanhf) and ensuring proper memory coalescing with aligned memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <math_constants.h>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    output[idx] = __tanhf(input[idx]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;  // Better occupancy for RTX 4090\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the CUDA kernel by using vectorized memory access and loop unrolling to process multiple elements per thread, reducing the number of memory transactions and improving throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  #pragma unroll\n  for (int i = 0; i < VEC_SIZE; ++i) {\n    if (idx + i < size) {\n      output[idx + i] = tanhf(input[idx + i]);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining correctness, and ensure proper memory coalescing and block/thread configuration for maximum occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    output[idx] = tanhf(x);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;  // Better for RTX 4090's warp scheduling\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the tanh kernel by using warp-level parallelism, reducing thread divergence, and minimizing redundant computations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < size; i += stride) {\n    output[i] = tanhf(input[i]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = min(65535, (size + threads - 1) / threads);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining correctness, and ensure proper memory coalescing and block/thread configuration for the RTX 4090 architecture.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    output[idx] = tanhf(x);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads, 0, cudaStreamPerThread>>>(\n      input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the tanh kernel by using warp-level primitives and increased occupancy while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    float exp2x = expf(2.0f * x);\n    output[idx] = (exp2x - 1.0f) / (exp2x + 1.0f);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining correctness, and ensure proper memory coalescing and block/thread configuration for the RTX 4090 architecture.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    output[idx] = tanhf(x);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads, 0, cudaStreamPerThread>>>(\n      input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining correctness, and ensure proper memory coalescing and thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    output[idx] = tanhf(x);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the tanh kernel by using warp-level parallelism and reducing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__device__ __forceinline__ float warp_tanh(float x) {\n    float exp2x = __expf(2.0f * x);\n    return (exp2x - 1.0f) / (exp2x + 1.0f);\n}\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int warp_stride = blockDim.x * gridDim.x;\n    for (int i = idx; i < size; i += warp_stride) {\n        output[i] = warp_tanh(input[i]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the CUDA kernel by using vectorized memory accesses and loop unrolling to process multiple elements per thread, while maintaining the same tanh computation for correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  #pragma unroll\n  for (int i = 0; i < VEC_SIZE; ++i) {\n    if (idx + i < size) {\n      output[idx + i] = tanhf(input[idx + i]);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the Tanh CUDA kernel by using warp-level parallelism and minimizing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__device__ __forceinline__ float warp_tanh(float x) {\n    float exp2x = __expf(2.0f * x);\n    return (exp2x - 1.0f) / (exp2x + 1.0f);\n}\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        output[idx] = warp_tanh(input[idx]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the tanh kernel by using warp-level parallelism and reducing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < size; i += stride) {\n    output[i] = tanhf(input[i]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;  // Better occupancy\n  int blocks = min((size + threads - 1) / threads, 2048);  // Cap blocks for better utilization\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the Tanh CUDA kernel by unrolling loops, increasing occupancy with optimal block sizes, and using fast math approximations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < size; i += stride) {\n        float x = input[i];\n        output[i] = tanhf(x);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    \n    int device_id;\n    cudaGetDevice(&device_id);\n    cudaDeviceProp props;\n    cudaGetDeviceProperties(&props, device_id);\n    \n    int threads = props.maxThreadsPerBlock;\n    int blocks = std::min((size + threads - 1) / threads, props.multiProcessorCount * 32);\n    \n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the CUDA kernel by increasing thread utilization through block size tuning and using fast math approximations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    float x2 = x * x;\n    output[idx] = x * (27.0f + x2) / (27.0f + 9.0f * x2);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the CUDA kernel by using warp-level parallelism and reducing thread divergence while maintaining numerical accuracy with single-precision tanhf.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (; idx < size; idx += stride) {\n    output[idx] = tanhf(input[idx]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<std::min(blocks, 65535), threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the Tanh CUDA kernel by using fast math approximations while maintaining numerical accuracy, and ensure proper memory coalescing and thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__forceinline__ __device__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f)));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        output[idx] = fast_tanh(input[idx]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the Tanh CUDA kernel by increasing thread utilization and using faster intrinsic functions while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n#include <cmath>\n\n__forceinline__ __device__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        output[idx] = fast_tanh(input[idx]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the tanh kernel by using warp-level primitives and increased occupancy while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (; idx < size; idx += stride) {\n    output[idx] = tanhf(input[idx]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = std::min((size + threads - 1) / threads, 2048);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the CUDA kernel by increasing thread utilization and reducing warp divergence while maintaining exact single-precision floating-point accuracy for the tanh operation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  #pragma unroll 4\n  for (int i = idx; i < size; i += blockDim.x * gridDim.x) {\n    output[i] = tanhf(input[i]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = min(65535, (size + threads - 1) / threads);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the tanh kernel by using warp-level parallelism and reducing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < size; i += stride) {\n    output[i] = tanhf(input[i]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = min(65535, (size + threads - 1) / threads);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  cudaDeviceSynchronize();\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining correctness, and ensure proper memory coalescing and block/thread configuration for the RTX 4090.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    output[idx] = __tanhf(x);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;  // Better for RTX 4090's warp size\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining accuracy, and ensure proper memory coalescing and block/thread configuration for the RTX 4090.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    float x2 = x * x;\n    output[idx] = x * (27.0f + x2) / (27.0f + 9.0f * x2);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the CUDA kernel by using faster intrinsic functions, increasing occupancy with optimal block sizes, and adding loop unrolling for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <mma.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  #pragma unroll 4\n  for (int i = idx; i < size; i += stride) {\n    output[i] = __tanhf(input[i]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  \n  // Optimal block size for RTX 4090\n  int threads = 256;\n  int blocks = min(65535, (size + threads - 1) / threads);\n  \n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the tanh CUDA kernel by using fast math approximations while maintaining correctness, increasing thread utilization, and minimizing warp divergence.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    const float x = input[idx];\n    output[idx] = __tanhf(x);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  const int size = input.numel();\n  const int threads = 256;\n  const int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads, 0, cudaStreamPerThread>>>(\n    input.data_ptr<float>(), \n    output.data_ptr<float>(), \n    size\n  );\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the CUDA kernel by using warp-level parallelism and minimizing thread divergence while maintaining exact single-precision floating-point accuracy for the tanh operation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__device__ __forceinline__ float warp_tanh(float x) {\n    return tanhf(x);\n}\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        output[idx] = warp_tanh(input[idx]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining accuracy through bounded input ranges and precise computation near zero.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__device__ __forceinline__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f)));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        float x = input[idx];\n        if (x < -4.97f) {\n            output[idx] = -1.0f;\n        } else if (x > 4.97f) {\n            output[idx] = 1.0f;\n        } else if (fabsf(x) < 0.0001f) {\n            output[idx] = x;  // tanh(x) \u2248 x for small x\n        } else {\n            output[idx] = fast_tanh(x);\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 1024;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining accuracy, and ensure proper memory coalescing and block/thread configuration for RTX 4090.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__forceinline__ __device__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        output[idx] = fast_tanh(input[idx]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the tanh kernel by unrolling loops, increasing occupancy with more threads per block, and using fast math approximations while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n  #pragma unroll\n  for (int i = 0; i < 4; ++i) {\n    int current_idx = idx + i * blockDim.x;\n    if (current_idx < size) {\n      float x = input[current_idx];\n      float x2 = x * x;\n      float x3 = x * x2;\n      output[current_idx] = x * (27 + x2) / (27 + 9 * x2);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads * 4 - 1) / (threads * 4);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the tanh CUDA kernel by using warp-level parallelism and reducing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    float exp2x = __expf(2.0f * x);\n    output[idx] = (exp2x - 1.0f) / (exp2x + 1.0f);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimize the tanh kernel by using warp-level primitives and loop unrolling to reduce instruction overhead and improve memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  #pragma unroll 4\n  for (int i = idx; i < size; i += stride) {\n    output[i] = tanhf(input[i]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = min(65535, (size + threads - 1) / threads);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining numerical accuracy, and ensure proper memory coalescing and block/thread configuration for the RTX 4090 architecture.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    float x2 = x * x;\n    float x4 = x2 * x2;\n    float x6 = x4 * x2;\n    output[idx] = x * (1.0f - x2/3.0f + 2.0f*x4/15.0f - 17.0f*x6/315.0f);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize the Tanh CUDA kernel by utilizing warp-level parallelism, reducing thread divergence, and ensuring coalesced memory access while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < size; i += stride) {\n    output[i] = tanhf(input[i]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  \n  int device_id;\n  cudaGetDevice(&device_id);\n  cudaDeviceProp prop;\n  cudaGetDeviceProperties(&prop, device_id);\n  \n  int threads = prop.warpSize * 8;  // 256 threads per block\n  int blocks = min((size + threads - 1) / threads, prop.multiProcessorCount * 8);\n  \n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the Tanh CUDA kernel by using fast math approximations while maintaining correctness through precise bounds checking and avoiding precision-reducing compiler flags.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__device__ __forceinline__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        float x = input[idx];\n        output[idx] = (x < -4.97f) ? -1.0f : \n                     (x > 4.97f) ? 1.0f : \n                     fast_tanh(x);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 1024;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize the CUDA kernel by increasing thread utilization and reducing warp divergence while maintaining exact single-precision floating-point accuracy for the tanh operation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  #pragma unroll 4\n  for (int i = idx; i < size; i += blockDim.x * gridDim.x) {\n    output[i] = tanhf(input[i]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = std::min((size + threads - 1) / threads, 1024);\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimize the tanh kernel by using fast math approximations while maintaining correctness, and ensure proper memory coalescing and thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < size) {\n    float x = input[idx];\n    // Fast tanh approximation that maintains accuracy\n    output[idx] = x * (27.f + x * x) / (27.f + 9.f * x * x);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;  // Better occupancy for RTX 4090\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimize the CUDA kernel by utilizing warp-level parallelism and replacing the standard tanhf with a faster, mathematically equivalent approximation while maintaining full float32 precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__device__ __forceinline__ float fast_tanh(float x) {\n    float x2 = x * x;\n    float a = x * (135135.0f + x2 * (17325.0f + x2 * (378.0f + x2)));\n    float b = 135135.0f + x2 * (62370.0f + x2 * (3150.0f + x2 * 28.0f));\n    return a / b;\n}\n\n__global__ void tanh_kernel(const float* __restrict__ input, float* __restrict__ output, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        output[idx] = fast_tanh(input[idx]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    int size = input.numel();\n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize the tanh kernel by using warp-level parallelism and reducing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void tanh_kernel(const float* input, float* output, int size) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < size; i += stride) {\n    output[i] = tanhf(input[i]);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  auto output = torch::empty_like(input);\n  int size = input.numel();\n  int threads = 256;\n  int blocks = (size + threads - 1) / threads;\n  tanh_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), size);\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"Apply Tanh activation (CUDA)\");\n}",
        "score": null
    }
]