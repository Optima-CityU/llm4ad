[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory access and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void optimized_relu_kernel(const scalar_t* __restrict__ input,\n                                     scalar_t* __restrict__ output,\n                                     size_t numel) {\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n  \n  if (idx < numel) {\n    float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n    float4 out_val;\n    out_val.x = in_val.x > 0 ? in_val.x : 0;\n    out_val.y = in_val.y > 0 ? in_val.y : 0;\n    out_val.z = in_val.z > 0 ? in_val.z : 0;\n    out_val.w = in_val.w > 0 ? in_val.w : 0;\n    *reinterpret_cast<float4*>(&output[idx]) = out_val;\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  const int threads = 1024;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    optimized_relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                       output.data_ptr<scalar_t>(),\n                                                       numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced divergence, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 128);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(\n      input.data_ptr<scalar_t>(),\n      output.data_ptr<scalar_t>(),\n      numel\n    );\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives and shared memory to reduce thread divergence and improve memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  namespace cg = cooperative_groups;\n  cg::thread_block block = cg::this_thread_block();\n  cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int warp_idx = idx / warp.size();\n  int lane = idx % warp.size();\n\n  #pragma unroll\n  for (int i = warp_idx; i * warp.size() < numel; i += gridDim.x * blockDim.x / warp.size()) {\n    int element_idx = i * warp.size() + lane;\n    if (element_idx < numel) {\n      scalar_t val = input[element_idx];\n      output[element_idx] = (val > 0) ? val : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = (numel + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                               output.data_ptr<scalar_t>(),\n                                               numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory access and warp-level primitives to reduce memory transactions and improve instruction throughput while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    using Vec = typename torch::scalar_traits<scalar_t>::vec_t;\n    Vec in_vec = *reinterpret_cast<const Vec*>(&input[idx]);\n    Vec out_vec;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      out_vec[i] = (in_vec[i] > static_cast<scalar_t>(0)) ? in_vec[i] : static_cast<scalar_t>(0);\n    }\n    *reinterpret_cast<Vec*>(&output[idx]) = out_vec;\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int vec_size = 4;\n  const int threads = 256;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, vec_size><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                         output.data_ptr<scalar_t>(),\n                                                         numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives and ensuring memory coalescing for better performance while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 128);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                               output.data_ptr<scalar_t>(),\n                                               numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                           scalar_t* __restrict__ output,\n                           size_t numel) {\n  const int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    scalar_t vals[VEC_SIZE];\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        scalar_t val = input[idx + i];\n        vals[i] = (val > 0) ? val : static_cast<scalar_t>(0);\n      }\n    }\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        output[idx + i] = vals[i];\n      }\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int vec_size = 4;\n  const int threads = 256;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, vec_size><<<blocks, threads>>>(\n        input.data_ptr<scalar_t>(),\n        output.data_ptr<scalar_t>(),\n        numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n  \n  if (idx + vec_size - 1 < numel) {\n    float4 in = *reinterpret_cast<const float4*>(&input[idx]);\n    float4 out;\n    out.x = in.x > 0 ? in.x : 0;\n    out.y = in.y > 0 ? in.y : 0;\n    out.z = in.z > 0 ? in.z : 0;\n    out.w = in.w > 0 ? in.w : 0;\n    *reinterpret_cast<float4*>(&output[idx]) = out;\n  } else {\n    for (int i = 0; i < vec_size && idx + i < numel; ++i) {\n      scalar_t val = input[idx + i];\n      output[idx + i] = val > 0 ? val : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = max(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = min((numel + threads - 1) / threads, 2048);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory access and warp-level primitives to reduce memory transactions and improve instruction throughput while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n  \n  if (idx + vec_size - 1 < numel) {\n    float4 in = *reinterpret_cast<const float4*>(&input[idx]);\n    float4 out;\n    out.x = (in.x > 0) ? in.x : static_cast<scalar_t>(0);\n    out.y = (in.y > 0) ? in.y : static_cast<scalar_t>(0);\n    out.z = (in.z > 0) ? in.z : static_cast<scalar_t>(0);\n    out.w = (in.w > 0) ? in.w : static_cast<scalar_t>(0);\n    *reinterpret_cast<float4*>(&output[idx]) = out;\n  } else {\n    for (int i = 0; i < vec_size && idx + i < numel; ++i) {\n      scalar_t val = input[idx + i];\n      output[idx + i] = (val > 0) ? val : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    scalar_t vals[VEC_SIZE];\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        scalar_t val = input[idx + i];\n        vals[i] = (val > 0) ? val : static_cast<scalar_t>(0);\n      }\n    }\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        output[idx + i] = vals[i];\n      }\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int vec_size = 4;\n  const int threads = 1024;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, vec_size><<<blocks, threads>>>(\n      input.data_ptr<scalar_t>(),\n      output.data_ptr<scalar_t>(),\n      numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 1024);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branch divergence, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 1024);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                               output.data_ptr<scalar_t>(),\n                                               numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory access and loop unrolling to process multiple elements per thread, while maintaining correctness for all floating-point types.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                           scalar_t* __restrict__ output,\n                           size_t numel) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE && (idx + i) < numel; ++i) {\n      scalar_t val = input[idx + i];\n      output[idx + i] = (val > 0) ? val : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int VEC_SIZE = 4;\n  const int threads = 1024;\n  const int blocks = (numel + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, VEC_SIZE><<<blocks, threads>>>(\n      input.data_ptr<scalar_t>(),\n      output.data_ptr<scalar_t>(),\n      numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimized the ReLU kernel by using warp-level primitives for reduced divergence and improved memory coalescing while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = min(blocks, 128);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve instruction throughput while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    scalar_t vals[VEC_SIZE];\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        vals[i] = input[idx + i];\n        output[idx + i] = (vals[i] > 0) ? vals[i] : static_cast<scalar_t>(0);\n      }\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int VEC_SIZE = 4;\n  const int threads = 256;\n  const int blocks = (numel + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, VEC_SIZE><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                         output.data_ptr<scalar_t>(),\n                                                         numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 1024);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = max(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = min(blocks, 1024);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and loop unrolling to process multiple elements per thread, while maintaining correctness through proper type handling.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    scalar_t vals[VEC_SIZE];\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        scalar_t val = input[idx + i];\n        vals[i] = (val > 0) ? val : static_cast<scalar_t>(0);\n      }\n    }\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        output[idx + i] = vals[i];\n      }\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int VEC_SIZE = 4;\n  const int threads = 1024;\n  const int blocks = (numel + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, VEC_SIZE><<<blocks, threads>>>(\n      input.data_ptr<scalar_t>(),\n      output.data_ptr<scalar_t>(),\n      numel\n    );\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = max(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = min((numel + threads - 1) / threads, 256);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(\n      input.data_ptr<scalar_t>(),\n      output.data_ptr<scalar_t>(),\n      numel\n    );\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branch divergence.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = fmaxf(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branch divergence.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 1024);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives and loop unrolling to reduce thread divergence and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  #pragma unroll 4\n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = (numel + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced thread divergence, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 1024);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branch divergence, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = max(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = min((numel + threads - 1) / threads, 256);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(\n      input.data_ptr<scalar_t>(),\n      output.data_ptr<scalar_t>(),\n      numel\n    );\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the ReLU kernel by utilizing warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = max(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = min((numel + threads - 1) / threads, 256);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branch divergence while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = fmaxf(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = (numel + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reducing thread divergence with predicated execution.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 1024);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n  \n  if (idx < numel) {\n    float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n    float4 out_val;\n    \n    #pragma unroll\n    for (int i = 0; i < vec_size; ++i) {\n      scalar_t val = reinterpret_cast<scalar_t*>(&in_val)[i];\n      reinterpret_cast<scalar_t*>(&out_val)[i] = (val > 0) ? val : static_cast<scalar_t>(0);\n    }\n    \n    *reinterpret_cast<float4*>(&output[idx]) = out_val;\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  const int threads = 1024;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    scalar_t vals[VEC_SIZE];\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        vals[i] = input[idx + i];\n        output[idx + i] = (vals[i] > 0) ? vals[i] : static_cast<scalar_t>(0);\n      }\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int vec_size = 4;\n  const int threads = 256;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, vec_size><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                         output.data_ptr<scalar_t>(),\n                                                         numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimized the ReLU kernel by using vectorized memory accesses and loop unrolling to process multiple elements per thread, while maintaining correctness through template specialization for different floating-point types.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel_vec(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               size_t numel) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    scalar_t vals[VEC_SIZE];\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        vals[i] = input[idx + i];\n        output[idx + i] = (vals[i] > 0) ? vals[i] : static_cast<scalar_t>(0);\n      }\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int vec_size = 4;\n  const int threads = 1024;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel_vec<scalar_t, vec_size><<<blocks, threads>>>(\n        input.data_ptr<scalar_t>(),\n        output.data_ptr<scalar_t>(),\n        numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branch divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = std::min((numel + threads - 1) / threads, 1024);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced thread divergence while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = (val > static_cast<scalar_t>(0)) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 128);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = max(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = min((numel + threads - 1) / threads, 128);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory access and warp-level primitives to reduce memory transactions and improve instruction throughput while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                           scalar_t* __restrict__ output,\n                           size_t numel) {\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    scalar_t vals[VEC_SIZE];\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        vals[i] = input[idx + i];\n        output[idx + i] = (vals[i] > static_cast<scalar_t>(0)) ? vals[i] : static_cast<scalar_t>(0);\n      }\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int threads = 256;\n  constexpr int vec_size = 4;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, vec_size><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                         output.data_ptr<scalar_t>(),\n                                                         numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory access and warp-level primitives to reduce memory transactions and improve instruction throughput while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    scalar_t vals[VEC_SIZE];\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (idx + i < numel) {\n        vals[i] = input[idx + i];\n        output[idx + i] = (vals[i] > 0) ? vals[i] : static_cast<scalar_t>(0);\n      }\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int vec_size = 4;\n  const int threads = 1024;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, vec_size><<<blocks, threads>>>(\n        input.data_ptr<scalar_t>(),\n        output.data_ptr<scalar_t>(),\n        numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the ReLU kernel by increasing thread utilization through warp-level operations and reducing branch divergence with predicated execution.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < numel) {\n    scalar_t val = input[idx];\n    output[idx] = val * (val > static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = (numel + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimized the ReLU kernel by using warp-level primitives for coalesced memory access and reduced thread divergence with a grid-stride loop.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = (val > 0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = min(blocks, 1024);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives and shared memory to reduce memory latency and improve thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  namespace cg = cooperative_groups;\n  cg::thread_block block = cg::this_thread_block();\n  cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n  \n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int warp_idx = idx / warp.size();\n  int lane = idx % warp.size();\n  \n  #pragma unroll\n  for (int i = warp_idx; i * warp.size() < numel; i += gridDim.x * blockDim.x / warp.size()) {\n    int element_idx = i * warp.size() + lane;\n    if (element_idx < numel) {\n      scalar_t val = input[element_idx];\n      output[element_idx] = (val > 0) ? val : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 1024;\n  const int blocks = (numel + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimized the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve instruction throughput while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx + VEC_SIZE <= numel) {\n    scalar_t vals[VEC_SIZE];\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      vals[i] = input[idx + i];\n    }\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      output[idx + i] = vals[i] > 0 ? vals[i] : static_cast<scalar_t>(0);\n    }\n  } else {\n    for (int i = idx; i < numel; ++i) {\n      output[i] = input[i] > 0 ? input[i] : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int vec_size = 4;\n  const int threads = 1024;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, vec_size><<<blocks, threads>>>(\n        input.data_ptr<scalar_t>(),\n        output.data_ptr<scalar_t>(),\n        numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives and shared memory to reduce memory latency and improve thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = (val > 0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 128);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                               output.data_ptr<scalar_t>(),\n                                               numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives and ensuring memory coalescing while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = val > static_cast<scalar_t>(0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 1024);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives and shared memory to reduce memory transactions and improve thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = (val > 0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int device;\n  cudaGetDevice(&device);\n  cudaDeviceProp prop;\n  cudaGetDeviceProperties(&prop, device);\n  \n  const int threads = prop.maxThreadsPerBlock;\n  const int blocks = prop.multiProcessorCount * 4;\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = fmaxf(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 128);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimized the ReLU kernel by using vectorized memory access and maximizing memory throughput while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n  \n  if (idx + vec_size - 1 < numel) {\n    float4 in = *reinterpret_cast<const float4*>(&input[idx]);\n    float4 out;\n    out.x = in.x > 0 ? in.x : 0;\n    out.y = in.y > 0 ? in.y : 0;\n    out.z = in.z > 0 ? in.z : 0;\n    out.w = in.w > 0 ? in.w : 0;\n    *reinterpret_cast<float4*>(&output[idx]) = out;\n  } else {\n    for (int i = 0; i < vec_size && idx + i < numel; ++i) {\n      scalar_t val = input[idx + i];\n      output[idx + i] = val > 0 ? val : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n  \n  if (idx + vec_size - 1 < numel) {\n    float4 in = *reinterpret_cast<const float4*>(&input[idx]);\n    float4 out;\n    out.x = (in.x > 0) ? in.x : static_cast<scalar_t>(0);\n    out.y = (in.y > 0) ? in.y : static_cast<scalar_t>(0);\n    out.z = (in.z > 0) ? in.z : static_cast<scalar_t>(0);\n    out.w = (in.w > 0) ? in.w : static_cast<scalar_t>(0);\n    *reinterpret_cast<float4*>(&output[idx]) = out;\n  } else {\n    for (int i = 0; i < vec_size && idx + i < numel; ++i) {\n      scalar_t val = input[idx + i];\n      output[idx + i] = (val > 0) ? val : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve instruction throughput while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n  if (idx < numel) {\n    float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n    float4 out_val;\n    out_val.x = (in_val.x > 0) ? in_val.x : static_cast<scalar_t>(0);\n    out_val.y = (in_val.y > 0) ? in_val.y : static_cast<scalar_t>(0);\n    out_val.z = (in_val.z > 0) ? in_val.z : static_cast<scalar_t>(0);\n    out_val.w = (in_val.w > 0) ? in_val.w : static_cast<scalar_t>(0);\n    *reinterpret_cast<float4*>(&output[idx]) = out_val;\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int threads = 256;\n  const int vec_numel = numel / (sizeof(float4) / sizeof(float));\n  const int blocks = (vec_numel + threads - 1) / threads;\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory access and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n  \n  if (idx + vec_size - 1 < numel) {\n    float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n    float4 out_val;\n    out_val.x = (in_val.x > 0) ? in_val.x : static_cast<scalar_t>(0);\n    out_val.y = (in_val.y > 0) ? in_val.y : static_cast<scalar_t>(0);\n    out_val.z = (in_val.z > 0) ? in_val.z : static_cast<scalar_t>(0);\n    out_val.w = (in_val.w > 0) ? in_val.w : static_cast<scalar_t>(0);\n    *reinterpret_cast<float4*>(&output[idx]) = out_val;\n  } else {\n    for (int i = 0; i < vec_size && idx + i < numel; ++i) {\n      scalar_t val = input[idx + i];\n      output[idx + i] = (val > 0) ? val : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int threads = 256;\n  constexpr int vec_size = sizeof(float4) / sizeof(float);\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = fmaxf(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = (numel + threads - 1) / threads;\n  blocks = std::min(blocks, 128);\n  \n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n  \n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = max(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = min((numel + threads - 1) / threads, 256);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives and shared memory to reduce thread divergence and improve memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = (val > 0) ? val : static_cast<scalar_t>(0);\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int threads = 256;\n  int blocks = std::min((numel + threads - 1) / threads, 1024);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve instruction throughput while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n  \n  if (idx + vec_size - 1 < numel) {\n    float4 in = *reinterpret_cast<const float4*>(&input[idx]);\n    float4 out;\n    out.x = in.x > 0 ? in.x : 0;\n    out.y = in.y > 0 ? in.y : 0;\n    out.z = in.z > 0 ? in.z : 0;\n    out.w = in.w > 0 ? in.w : 0;\n    *reinterpret_cast<float4*>(&output[idx]) = out;\n  } else {\n    for (int i = 0; i < vec_size && idx + i < numel; ++i) {\n      scalar_t val = input[idx + i];\n      output[idx + i] = val > 0 ? val : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  const int threads = 256;\n  const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\ntemplate <typename scalar_t>\n__global__ void optimized_relu_kernel(const scalar_t* __restrict__ input,\n                                     scalar_t* __restrict__ output,\n                                     size_t numel) {\n  const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n  \n  for (int i = idx; i < numel; i += stride) {\n    scalar_t val = input[i];\n    output[i] = max(val, static_cast<scalar_t>(0));\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  \n  int device;\n  cudaGetDevice(&device);\n  cudaDeviceProp props;\n  cudaGetDeviceProperties(&props, device);\n  \n  const int threads = props.maxThreadsPerBlock;\n  const int blocks = std::min(props.multiProcessorCount * 16, \n                             (int)((numel + threads - 1) / threads));\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    optimized_relu_kernel<scalar_t><<<blocks, threads>>>(\n      input.data_ptr<scalar_t>(),\n      output.data_ptr<scalar_t>(),\n      numel\n    );\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimize the ReLU kernel by using warp-level primitives and vectorized memory access to reduce memory transactions and improve thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  const int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n  if (idx < numel) {\n    const int remaining = min(static_cast<int>(numel - idx), VEC_SIZE);\n    scalar_t in_vals[VEC_SIZE];\n    scalar_t out_vals[VEC_SIZE];\n    \n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (i < remaining) {\n        in_vals[i] = input[idx + i];\n        out_vals[i] = (in_vals[i] > 0) ? in_vals[i] : static_cast<scalar_t>(0);\n      }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n      if (i < remaining) {\n        output[idx + i] = out_vals[i];\n      }\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int vec_size = 4;\n  const int threads = 1024;\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t, vec_size><<<blocks, threads>>>(\n      input.data_ptr<scalar_t>(),\n      output.data_ptr<scalar_t>(),\n      numel\n    );\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize the ReLU kernel by using vectorized memory accesses and warp-level primitives to reduce memory transactions and improve instruction throughput while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void relu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            size_t numel) {\n  constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n  \n  if (idx + vec_size - 1 < numel) {\n    float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n    float4 out_val;\n    out_val.x = (in_val.x > 0) ? in_val.x : static_cast<scalar_t>(0);\n    out_val.y = (in_val.y > 0) ? in_val.y : static_cast<scalar_t>(0);\n    out_val.z = (in_val.z > 0) ? in_val.z : static_cast<scalar_t>(0);\n    out_val.w = (in_val.w > 0) ? in_val.w : static_cast<scalar_t>(0);\n    *reinterpret_cast<float4*>(&output[idx]) = out_val;\n  } else {\n    for (int i = 0; i < vec_size && idx + i < numel; ++i) {\n      scalar_t val = input[idx + i];\n      output[idx + i] = (val > 0) ? val : static_cast<scalar_t>(0);\n    }\n  }\n}\n\ntorch::Tensor forward(torch::Tensor input) {\n  TORCH_CHECK(input.is_cuda(), \"input must be a CUDA tensor\");\n  auto output = torch::empty_like(input);\n  const size_t numel = input.numel();\n  constexpr int threads = 256;\n  constexpr int vec_size = sizeof(float4) / sizeof(float);\n  const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"relu_forward_cuda\", ([&] {\n    relu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                              output.data_ptr<scalar_t>(),\n                                              numel);\n  }));\n\n  return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &forward, \"ReLU forward (CUDA)\");\n}",
        "score": null
    }
]