[
    {
        "sample_order": 1,
        "algorithm": "{Optimize by exploiting lower triangular structure to reduce redundant computations and improve memory access patterns through loop unrolling and shared memory tiling.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + blockDim.y - 1) / blockDim.y; ++tile) {\n        int k = tile * blockDim.y + threadIdx.y;\n        \n        if (row < N && k < N && k <= row) {\n            sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n        } else {\n            sA[threadIdx.x * blockDim.y + threadIdx.y] = 0.0f;\n        }\n\n        if (k < N && col < N) {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = B[k * N + col];\n        } else {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < blockDim.y && (tile * blockDim.y + i) <= row; ++i) {\n            sum += sA[threadIdx.x * blockDim.y + i] * sB[i * blockDim.x + threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B, reducing global memory accesses, and using thread coarsening to increase arithmetic intensity while maintaining correctness for lower triangular matrices.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = (tile_col <= row) ? A[row * N + tile_col] : 0.0f;\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = (tile_row >= col) ? B[tile_row * N + col] : 0.0f;\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize by exploiting the lower triangular structure to reduce redundant computations and improve memory access patterns, while maintaining correctness through proper bounds checking.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col <= row) {\n        float sum = 0.0f;\n        int upper_k = min(row, col);\n        for (int k = 0; k <= upper_k; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(32, 32);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize by exploiting lower triangular matrix structure to reduce redundant computations and improve memory access patterns, while maintaining correctness through proper boundary conditions.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col <= row) {  // Only compute for lower triangular part\n        float sum = 0.0f;\n        int k_start = max(col - (row - col), 0);  // Lower bound from B's triangular structure\n        for (int k = k_start; k <= col; ++k) {    // Upper bound from A's triangular structure\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(32, 32);  // Increased block size for better occupancy\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize by reducing redundant computations in the lower triangular matrix multiplication through thread coarsening and shared memory utilization, while maintaining correctness by only computing necessary elements where row >= col.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= row; tile += blockDim.y) {\n            int k = tile + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n            \n            int end = min(blockDim.y, row - tile + 1);\n            for (int i = 0; i < end; ++i) {\n                sum += sA[threadIdx.x * blockDim.y + i] * sB[i * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    \n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize by exploiting lower triangular structure to reduce computation and memory accesses, using shared memory for data reuse, and adjusting thread block configuration for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    if (row < N && col < N && row >= col) {\n        for (int k_start = 0; k_start <= row; k_start += blockDim.y) {\n            int k = k_start + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n\n            int k_end = min(k_start + blockDim.y, row + 1);\n            for (int kk = k_start; kk < k_end; ++kk) {\n                sum += sA[threadIdx.x * blockDim.y + (kk - k_start)] * sB[(kk - k_start) * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize by leveraging the lower triangular property to reduce computation and memory access, using shared memory for block-level matrix multiplication, and ensuring only necessary elements are computed.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* As = shared_mem;\n    float* Bs = shared_mem + blockDim.x * blockDim.y;\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + blockDim.x - 1) / blockDim.x; ++tile) {\n        int tile_row = tile * blockDim.x + threadIdx.x;\n        int tile_col = tile * blockDim.y + threadIdx.y;\n        \n        if (row < N && tile_col < N) {\n            As[threadIdx.y * blockDim.x + threadIdx.x] = (tile_col <= row) ? A[row * N + tile_col] : 0.0f;\n        } else {\n            As[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n        \n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y * blockDim.x + threadIdx.x] = (tile_row <= col) ? B[tile_row * N + col] : 0.0f;\n        } else {\n            Bs[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        if (row < N && col < N && row >= col) {\n            for (int k = 0; k < blockDim.x; ++k) {\n                sum += As[threadIdx.y * blockDim.x + k] * Bs[k * blockDim.y + threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize by exploiting the lower triangular structure to reduce redundant computations and memory accesses, while maintaining correctness through proper bounds checking.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col <= row) {\n        float sum = 0.0f;\n        int start_k = max(col - (row - col), 0);\n        for (int k = start_k; k <= col; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize by computing only the lower triangular elements and using shared memory to cache tiles of A and B for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col <= row) {\n        float sum = 0.0f;\n        \n        for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n            int tile_row = tile * TILE_SIZE + threadIdx.y;\n            int tile_col = tile * TILE_SIZE + threadIdx.x;\n            \n            if (row < N && tile_col < N) {\n                As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n            \n            if (tile_row < N && col < N) {\n                Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n            \n            __syncthreads();\n            \n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n            __syncthreads();\n        }\n        \n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimized algorithm computes lower triangular matrix multiplication by only iterating over relevant elements (k \u2264 row) and using shared memory to cache tiles of A and B for faster access.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    if (row < N && col < N) {\n        for (int tile = 0; tile <= (row / TILE_SIZE); ++tile) {\n            int tile_row = threadIdx.x;\n            int tile_col = threadIdx.y;\n            \n            // Load A tile\n            int A_row = row;\n            int A_col = tile * TILE_SIZE + tile_col;\n            if (A_col <= row && A_col < N) {\n                sA[tile_row][tile_col] = A[A_row * N + A_col];\n            } else {\n                sA[tile_row][tile_col] = 0.0f;\n            }\n            \n            // Load B tile\n            int B_row = tile * TILE_SIZE + tile_row;\n            int B_col = col;\n            if (B_row < N && B_col < N) {\n                sB[tile_row][tile_col] = B[B_row * N + B_col];\n            } else {\n                sB[tile_row][tile_col] = 0.0f;\n            }\n            \n            __syncthreads();\n            \n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += sA[tile_row][k] * sB[k][tile_col];\n            }\n            \n            __syncthreads();\n        }\n\n        if (row >= col) {\n            C[row * N + col] = sum;\n        } else {\n            C[row * N + col] = 0.0f;\n        }\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimized algorithm that leverages the lower triangular property to reduce computation by only calculating necessary elements and using shared memory for faster data access.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col < N && row >= col) {\n        float sum = 0.0f;\n        for (int k = col; k <= row; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize by leveraging shared memory for coalesced memory access and reducing redundant computations in the lower triangular matrix multiplication, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            sA[threadIdx.y][threadIdx.x] = (tile_col <= row) ? A[row * N + tile_col] : 0.0f;\n        } else {\n            sA[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            sB[threadIdx.y][threadIdx.x] = (col <= tile_row) ? B[tile_row * N + col] : 0.0f;\n        } else {\n            sB[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += sA[threadIdx.y][k] * sB[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize by leveraging the lower triangular property to reduce computation and memory accesses, using shared memory for data reuse, and adjusting thread block configuration for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= row; tile += blockDim.y) {\n            int k = tile + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n\n            int end = min(blockDim.y, row - tile + 1);\n            for (int i = 0; i < end; ++i) {\n                sum += sA[threadIdx.x * blockDim.y + i] * sB[i * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimized algorithm computes only the lower triangular portion of the result matrix by restricting the column range for each row and using shared memory to cache tiles of A and B for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    if (row < N && col <= row) {\n        for (int tile = 0; tile < (row + blockDim.y - 1) / blockDim.y; ++tile) {\n            int tile_col = tile * blockDim.y + threadIdx.y;\n            if (tile_col <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + tile_col];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[tile_col * N + col];\n            }\n            __syncthreads();\n\n            for (int k = 0; k < blockDim.y; ++k) {\n                int tile_col = tile * blockDim.y + k;\n                if (tile_col <= row) {\n                    sum += sA[threadIdx.x * blockDim.y + k] * sB[k * blockDim.y + threadIdx.y];\n                }\n            }\n            __syncthreads();\n        }\n\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(\n        A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize by exploiting lower triangular structure to reduce computation and memory access, using shared memory for data reuse and thread coarsening to increase arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    extern __shared__ float smem[];\n    float* sA = smem;\n    float* sB = smem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile <= (row + blockDim.x - 1) / blockDim.x; ++tile) {\n        int k = tile * blockDim.x + threadIdx.y;\n        if (k <= row && threadIdx.x < blockDim.y) {\n            sA[threadIdx.y * blockDim.x + threadIdx.x] = (row < N && k < N) ? A[row * N + k] : 0.0f;\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = (k < N && col < N) ? B[k * N + col] : 0.0f;\n        }\n        __syncthreads();\n\n        if (row < N && col < N) {\n            for (int k_tile = 0; k_tile < blockDim.x && (tile * blockDim.x + k_tile) <= row; ++k_tile) {\n                sum += sA[threadIdx.x * blockDim.x + k_tile] * sB[k_tile * blockDim.y + threadIdx.y];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(32, 32);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize by exploiting the lower triangular structure to reduce redundant computations and improve memory access patterns, while maintaining correctness through proper boundary conditions.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col < N) {\n        float sum = 0.0f;\n        int k_start = max(0, col - (N - row - 1));\n        int k_end = min(row, col);\n        \n        for (int k = k_start; k <= k_end; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize by reducing redundant computations in the lower triangular matrix multiplication through thread coarsening and shared memory utilization, while maintaining correctness by preserving the tril condition.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shmem[];\n    float* sA = shmem;\n    float* sB = shmem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + blockDim.x - 1) / blockDim.x; ++tile) {\n        int k = tile * blockDim.x + threadIdx.x;\n        if (row < N && k < N) {\n            sA[threadIdx.y * blockDim.x + threadIdx.x] = (k <= row) ? A[row * N + k] : 0.0f;\n        } else {\n            sA[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n\n        k = tile * blockDim.y + threadIdx.y;\n        if (col < N && k < N) {\n            sB[threadIdx.x * blockDim.y + threadIdx.y] = (k <= col) ? B[k * N + col] : 0.0f;\n        } else {\n            sB[threadIdx.x * blockDim.y + threadIdx.y] = 0.0f;\n        }\n        __syncthreads();\n\n        for (int k = 0; k < blockDim.x; ++k) {\n            sum += sA[threadIdx.y * blockDim.x + k] * sB[k * blockDim.y + threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shmem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shmem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize by exploiting the lower triangular property to reduce redundant computations and memory accesses, using shared memory for data reuse and thread coarsening to increase arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= row; tile += blockDim.y) {\n            int k = tile + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n\n            for (int k_tile = 0; k_tile < blockDim.y && tile + k_tile <= row; ++k_tile) {\n                sum += sA[threadIdx.x * blockDim.y + k_tile] * sB[k_tile * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    const int BLOCK_SIZE = 16;\n    dim3 block_size(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    size_t shared_mem_size = 2 * BLOCK_SIZE * BLOCK_SIZE * sizeof(float);\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize by exploiting the lower triangular structure to reduce redundant computations and improve memory access patterns, while maintaining correctness through proper bounds checking.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col <= row) {  // Only compute lower triangular part\n        float sum = 0.0f;\n        int start_k = max(col - (row - col), 0);  // Further restrict k range\n        for (int k = start_k; k <= min(row, col); ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(32, 32);  // Increased block size for better occupancy\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize by exploiting lower triangular structure to reduce computation and memory access, using shared memory for data reuse and thread coarsening to increase arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (row + blockDim.y - 1) / blockDim.y; ++tile) {\n        int k = tile * blockDim.y + threadIdx.y;\n        if (k <= row && k < N && row < N) {\n            sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n        }\n        if (k < N && col < N) {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = B[k * N + col];\n        }\n        __syncthreads();\n\n        for (int k_tile = 0; k_tile < blockDim.y && (tile * blockDim.y + k_tile) <= row; ++k_tile) {\n            sum += sA[threadIdx.x * blockDim.y + k_tile] * sB[k_tile * blockDim.x + threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        if (row >= col) {\n            C[row * N + col] = sum;\n        } else {\n            C[row * N + col] = 0.0f;\n        }\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize by computing only the lower triangular elements of the result matrix, leveraging shared memory for faster access to A and B tiles, and unrolling the inner loop for better performance.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int A_col = tile_offset + threadIdx.y;\n        int B_row = tile_offset + threadIdx.x;\n\n        if (row < N && A_col < N && A_col <= row) {\n            As[threadIdx.x][threadIdx.y] = A[row * N + A_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (B_row < N && col < N && B_row >= col) {\n            Bs[threadIdx.x][threadIdx.y] = B[B_row * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        if (row >= col) {\n            C[row * N + col] = sum;\n        } else {\n            C[row * N + col] = 0.0f;\n        }\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize by exploiting lower triangular structure to reduce computation and memory accesses, using shared memory for data reuse and limiting computation to the lower triangular region only.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= row; tile += blockDim.y) {\n            int k = tile + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n\n            int end = min(blockDim.y, row - tile + 1);\n            for (int kk = 0; kk < end; ++kk) {\n                sum += sA[threadIdx.x * blockDim.y + kk] * sB[kk * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(\n        A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize by exploiting the lower triangular property to reduce computation and memory access, using shared memory for data reuse, and adjusting thread block configuration for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= row; tile += blockDim.y) {\n            int k = tile + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n            \n            int end = min(blockDim.y, row - tile + 1);\n            for (int i = 0; i < end; ++i) {\n                sum += sA[threadIdx.x * blockDim.y + i] * sB[i * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n    \n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n    \n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize by exploiting the lower triangular structure to reduce redundant computations and improve memory access patterns, while maintaining correctness through proper bounds checking.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col < N && row >= col) {\n        float sum = 0.0f;\n        int start_k = col;  // A is lower triangular\n        int end_k = row;    // B is lower triangular\n        for (int k = start_k; k <= end_k; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimized algorithm computes the lower triangular matrix multiplication by only processing elements where row >= col and k <= min(row, col), reducing unnecessary computations and memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col < N && row >= col) {\n        float sum = 0.0f;\n        for (int k = col; k <= row; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize by exploiting lower triangular structure to reduce computation and memory access, using shared memory for data reuse, and adjusting thread block configuration for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= row; tile += blockDim.y) {\n            int k = tile + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n\n            int end = min(blockDim.y, row - tile + 1);\n            for (int i = 0; i < end; ++i) {\n                sum += sA[threadIdx.x * blockDim.y + i] * sB[i * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(32, 8);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    \n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize by exploiting lower triangular structure to reduce redundant computations and improve memory access patterns through tiling and loop unrolling.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col <= row) {  // Only compute lower triangular part\n        float sum = 0.0f;\n        int k = 0;\n        \n        // Process tiles with loop unrolling\n        for (; k <= (row - UNROLL_FACTOR + 1); k += UNROLL_FACTOR) {\n            sum += A[row * N + k] * B[k * N + col];\n            sum += A[row * N + k + 1] * B[(k + 1) * N + col];\n            sum += A[row * N + k + 2] * B[(k + 2) * N + col];\n            sum += A[row * N + k + 3] * B[(k + 3) * N + col];\n        }\n        \n        // Process remaining elements\n        for (; k <= row; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize by exploiting lower triangular structure to reduce redundant computations and improve memory access patterns, while maintaining correctness through proper bounds checking.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col < N && row >= col) {\n        float sum = 0.0f;\n        for (int k = col; k <= row; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize by exploiting the lower triangular property to reduce redundant computations, using shared memory for data reuse, and adjusting thread block configuration for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= row; tile += blockDim.y) {\n            int k = tile + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n\n            for (int k_tile = 0; k_tile < blockDim.y && (tile + k_tile) <= row; ++k_tile) {\n                sum += sA[threadIdx.x * blockDim.y + k_tile] * sB[k_tile * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize by reducing redundant computations in the lower triangular matrix multiplication through loop unrolling, shared memory caching, and minimizing thread divergence with early termination for upper triangular elements.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row >= N || col >= N || row < col) {\n        if (row < N && col < N) {\n            C[row * N + col] = 0.0f;\n        }\n        return;\n    }\n\n    __shared__ float sA[16][16];\n    __shared__ float sB[16][16];\n\n    float sum = 0.0f;\n    for (int tile = 0; tile < (N + 15) / 16; ++tile) {\n        int k = tile * 16 + threadIdx.y;\n        if (k <= row) {\n            sA[threadIdx.x][threadIdx.y] = A[row * N + k];\n        } else {\n            sA[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        k = tile * 16 + threadIdx.x;\n        if (k < N) {\n            sB[threadIdx.x][threadIdx.y] = B[k * N + col];\n        } else {\n            sB[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < 16; ++k) {\n            sum += sA[threadIdx.x][k] * sB[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    C[row * N + col] = sum;\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the kernel by reducing redundant computations in the lower triangular matrix multiplication through thread coarsening and loop unrolling while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    const int row = blockIdx.x * blockDim.x + threadIdx.x;\n    const int col = blockIdx.y * blockDim.y * 4 + threadIdx.y;\n\n    if (row < N && col < N) {\n        float sum[4] = {0.0f, 0.0f, 0.0f, 0.0f};\n        const int k_end = min(row + 1, N);\n        \n        #pragma unroll 4\n        for (int k = 0; k < k_end; ++k) {\n            float a = A[row * N + k];\n            sum[0] += a * B[k * N + col];\n            if (col + blockDim.y < N) sum[1] += a * B[k * N + col + blockDim.y];\n            if (col + 2 * blockDim.y < N) sum[2] += a * B[k * N + col + 2 * blockDim.y];\n            if (col + 3 * blockDim.y < N) sum[3] += a * B[k * N + col + 3 * blockDim.y];\n        }\n\n        if (row >= col) C[row * N + col] = sum[0];\n        else C[row * N + col] = 0.0f;\n        \n        if (col + blockDim.y < N) {\n            if (row >= col + blockDim.y) C[row * N + col + blockDim.y] = sum[1];\n            else C[row * N + col + blockDim.y] = 0.0f;\n        }\n        \n        if (col + 2 * blockDim.y < N) {\n            if (row >= col + 2 * blockDim.y) C[row * N + col + 2 * blockDim.y] = sum[2];\n            else C[row * N + col + 2 * blockDim.y] = 0.0f;\n        }\n        \n        if (col + 3 * blockDim.y < N) {\n            if (row >= col + 3 * blockDim.y) C[row * N + col + 3 * blockDim.y] = sum[3];\n            else C[row * N + col + 3 * blockDim.y] = 0.0f;\n        }\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + 4 * block_size.y - 1) / (4 * block_size.y));\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize by exploiting the lower triangular structure to reduce redundant computations and improve memory access patterns, while maintaining correctness through proper bounds checking.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col < N && row >= col) {\n        float sum = 0.0f;\n        int k_start = col;  // A is lower triangular\n        int k_end = row;     // B is lower triangular\n        for (int k = k_start; k <= k_end; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize by exploiting lower triangular structure to reduce redundant computations and improve memory access patterns, while maintaining correctness through proper bounds checking.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col < N && row >= col) {\n        float sum = 0.0f;\n        for (int k = col; k <= row; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize by computing only the lower triangular portion of the result matrix and using shared memory to reduce global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row >= N || col > row) return;\n\n    float sum = 0.0f;\n    for (int k_start = 0; k_start <= row; k_start += blockDim.y) {\n        int k = k_start + threadIdx.y;\n        if (k <= row) {\n            sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n            sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n        }\n        __syncthreads();\n\n        int k_end = min(k_start + blockDim.y, row + 1);\n        for (int kk = k_start; kk < k_end; ++kk) {\n            sum += sA[threadIdx.x * blockDim.y + (kk - k_start)] * \n                   sB[(kk - k_start) * blockDim.y + threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    C[row * N + col] = sum;\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B, and only compute the lower triangular portion while reducing redundant computations through loop unrolling and increased thread utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        sA[threadIdx.y][threadIdx.x] = (row < N && tile_col < N) ? A[row * N + tile_col] : 0.0f;\n        sB[threadIdx.y][threadIdx.x] = (tile_row < N && col < N) ? B[tile_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        if (row < N && col <= row) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += sA[threadIdx.y][k] * sB[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize by reducing redundant computations in lower triangular matrix multiplication through thread coarsening and shared memory utilization, while maintaining correctness by only computing necessary elements in the lower triangular region.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= (row / blockDim.y); ++tile) {\n            int k = tile * blockDim.y + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n\n            if (k <= row) {\n                for (int i = 0; i < blockDim.y && (tile * blockDim.y + i) <= row; ++i) {\n                    sum += sA[threadIdx.x * blockDim.y + i] * sB[i * blockDim.y + threadIdx.y];\n                }\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize by leveraging the lower triangular property to reduce computation and memory access, using shared memory for data reuse, and adjusting thread block configuration for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= row; tile += blockDim.y) {\n            int k = tile + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n\n            for (int i = 0; i < blockDim.y && (tile + i) <= row; ++i) {\n                sum += sA[threadIdx.x * blockDim.y + i] * sB[i * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the kernel by reducing redundant computations in the lower triangular matrix multiplication through loop unrolling, shared memory caching, and minimizing thread divergence.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = (tile_col <= row) ? A[row * N + tile_col] : 0.0f;\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = (tile_row >= col) ? B[tile_row * N + col] : 0.0f;\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the kernel by computing only the lower triangular elements and using shared memory to cache tiles of A and B for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= (row / blockDim.y); ++tile) {\n            int tile_col = tile * blockDim.y + threadIdx.y;\n            if (tile_col <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + tile_col];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[tile_col * N + col];\n            }\n            __syncthreads();\n\n            int k_end = min(blockDim.y, row - tile * blockDim.y + 1);\n            for (int k = 0; k < k_end && (tile * blockDim.y + k) <= row; ++k) {\n                sum += sA[threadIdx.x * blockDim.y + k] * sB[k * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize by computing only the lower triangular elements and using shared memory to cache tiles of A and B for faster access.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= (row / blockDim.y); ++tile) {\n            int k = tile * blockDim.y + threadIdx.y;\n            if (k <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[k * N + col];\n            }\n            __syncthreads();\n\n            int upper = min(blockDim.y, row - tile * blockDim.y + 1);\n            for (int kk = 0; kk < upper; ++kk) {\n                sum += sA[threadIdx.x * blockDim.y + kk] * sB[kk * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(\n        A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimized algorithm computes the lower triangular matrix multiplication by only iterating over relevant elements (k <= row and k >= col) and uses shared memory to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col < N && row >= col) {\n        float sum = 0.0f;\n        for (int k = col; k <= row; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the kernel by reducing redundant computations in the inner loop for lower triangular matrices and improve memory access patterns by utilizing shared memory and loop unrolling.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = (tile_col <= row) ? A[row * N + tile_col] : 0.0f;\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = (tile_row >= col) ? B[tile_row * N + col] : 0.0f;\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            sum += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            sum += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            sum += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize by leveraging the lower triangular property to reduce redundant computations and improve memory access patterns, while maintaining correctness through proper bounds checking.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col <= row) {\n        float sum = 0.0f;\n        int upper_k = min(row, col);\n        for (int k = col; k <= upper_k; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n    else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize by reducing redundant computations in the inner loop and leveraging shared memory for faster access to frequently used elements of A and B, while maintaining correctness for lower triangular matrices.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col < N) {\n        float sum = 0.0f;\n        int k_start = (row < col) ? 0 : col;\n        int k_end = min(row, N-1);\n\n        for (int k_block = 0; k_block <= k_end; k_block += blockDim.y) {\n            int k = k_block + threadIdx.y;\n            if (k <= k_end && threadIdx.x == 0) {\n                sB[threadIdx.y] = (k < N && col < N) ? B[k * N + col] : 0.0f;\n            }\n            __syncthreads();\n\n            if (k <= k_end && row < N) {\n                sum += A[row * N + k] * sB[threadIdx.y];\n            }\n            __syncthreads();\n        }\n\n        if (row >= col) {\n            C[row * N + col] = sum;\n        } else {\n            C[row * N + col] = 0.0f;\n        }\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize by leveraging the lower triangular property to reduce computation and memory access, using shared memory for data reuse and thread coarsening to increase occupancy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    extern __shared__ float smem[];\n    float* sA = smem;\n    float* sB = smem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + blockDim.x - 1) / blockDim.x; ++tile) {\n        int tile_row = tile * blockDim.x + threadIdx.x;\n        int tile_col = tile * blockDim.y + threadIdx.y;\n        \n        if (row < N && tile_col < N) {\n            sA[threadIdx.y * blockDim.x + threadIdx.x] = (tile_col <= row) ? A[row * N + tile_col] : 0.0f;\n        } else {\n            sA[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n        \n        if (tile_row < N && col < N) {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = (tile_row >= col) ? B[tile_row * N + col] : 0.0f;\n        } else {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        if (row < N && col < N && row >= col) {\n            for (int k = 0; k < blockDim.x; ++k) {\n                sum += sA[threadIdx.y * blockDim.x + k] * sB[k * blockDim.y + threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize by reducing redundant computations in the lower triangular matrix multiplication through thread coarsening and loop unrolling, while maintaining correctness by only computing necessary elements.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y * 4 + threadIdx.y;\n\n    if (row < N) {\n        #pragma unroll\n        for (int c = 0; c < 4; ++c) {\n            int current_col = col + c * blockDim.y;\n            if (current_col < N) {\n                float sum = 0.0f;\n                int upper_bound = min(row, current_col);\n                for (int k = 0; k <= upper_bound; ++k) {\n                    sum += A[row * N + k] * B[k * N + current_col];\n                }\n                C[row * N + current_col] = (row >= current_col) ? sum : 0.0f;\n            }\n        }\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(32, 8);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y * 4 - 1) / (block_size.y * 4));\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimized algorithm computes the lower triangular matrix multiplication by only iterating up to min(row, col) in the inner loop and using shared memory to cache tiles of A and B for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    if (row < N && col < N) {\n        for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n            int tile_row = tile * TILE_SIZE + threadIdx.y;\n            if (row < N && tile_row < N) {\n                sA[threadIdx.x][threadIdx.y] = A[row * N + tile_row];\n            } else {\n                sA[threadIdx.x][threadIdx.y] = 0.0f;\n            }\n\n            int tile_col = tile * TILE_SIZE + threadIdx.x;\n            if (col < N && tile_col < N) {\n                sB[threadIdx.x][threadIdx.y] = B[tile_col * N + col];\n            } else {\n                sB[threadIdx.x][threadIdx.y] = 0.0f;\n            }\n            __syncthreads();\n\n            int upper = min(TILE_SIZE, N - tile * TILE_SIZE);\n            for (int k = 0; k < upper; ++k) {\n                if (k + tile * TILE_SIZE <= min(row, col)) {\n                    sum += sA[threadIdx.x][k] * sB[k][threadIdx.y];\n                }\n            }\n            __syncthreads();\n        }\n\n        if (row >= col) {\n            C[row * N + col] = sum;\n        } else {\n            C[row * N + col] = 0.0f;\n        }\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize by computing only the lower triangular elements and using shared memory to cache tiles of A and B for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    if (row < N && col <= row) {\n        for (int tile = 0; tile <= (row / blockDim.y); ++tile) {\n            int tile_col = tile * blockDim.y + threadIdx.y;\n            if (tile_col <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + tile_col];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[tile_col * N + col];\n            }\n            __syncthreads();\n\n            int upper = min(blockDim.y, row - tile * blockDim.y + 1);\n            for (int k = 0; k < upper; ++k) {\n                sum += sA[threadIdx.x * blockDim.y + k] * sB[k * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize by reducing redundant computations in the lower triangular matrix multiplication through thread coarsening and loop unrolling, while maintaining correctness by only computing necessary elements.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < N && col <= row) {\n        float sum = 0.0f;\n        #pragma unroll 4\n        for (int k = 0; k <= row; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(32, 32);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize by exploiting lower triangular structure to reduce computation and memory access, using shared memory for data reuse, and adjusting thread block configuration for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void tril_matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        sA[threadIdx.y][threadIdx.x] = (row < N && tile_col < N && tile_col <= row) ? A[row * N + tile_col] : 0.0f;\n        sB[threadIdx.y][threadIdx.x] = (tile_row < N && col < N && tile_row >= col) ? B[tile_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += sA[threadIdx.y][k] * sB[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(TILE_SIZE, TILE_SIZE);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n\n    tril_matmul_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize by computing only the lower triangular elements, using shared memory for data reuse, and unrolling the inner loop to reduce memory accesses and improve parallelism.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    if (row < N && col <= row) {\n        for (int tile = 0; tile < (row + blockDim.y - 1) / blockDim.y; ++tile) {\n            int tile_col = tile * blockDim.y + threadIdx.y;\n            if (tile_col <= row) {\n                sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + tile_col];\n                sB[threadIdx.x * blockDim.y + threadIdx.y] = B[tile_col * N + col];\n            }\n            __syncthreads();\n\n            for (int k = 0; k < blockDim.y && (tile * blockDim.y + k) <= row; ++k) {\n                sum += sA[threadIdx.x * blockDim.y + k] * sB[k * blockDim.y + threadIdx.y];\n            }\n            __syncthreads();\n        }\n\n        C[row * N + col] = sum;\n    } else if (row < N && col < N) {\n        C[row * N + col] = 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimize by reducing redundant computations in lower triangular matrix multiplication through thread coarsening and shared memory utilization, while maintaining correctness by only computing necessary elements.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (row + blockDim.y - 1) / blockDim.y; ++tile) {\n        int k = tile * blockDim.y + threadIdx.y;\n        if (row < N && k < N && k <= row) {\n            sA[threadIdx.x * blockDim.y + threadIdx.y] = A[row * N + k];\n        } else {\n            sA[threadIdx.x * blockDim.y + threadIdx.y] = 0.0f;\n        }\n\n        k = tile * blockDim.x + threadIdx.x;\n        if (k < N && col < N) {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = B[k * N + col];\n        } else {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n        __syncthreads();\n\n        for (int k_tile = 0; k_tile < blockDim.y; ++k_tile) {\n            int k = tile * blockDim.y + k_tile;\n            if (k <= row) {\n                sum += sA[threadIdx.x * blockDim.y + k_tile] * sB[k_tile * blockDim.x + threadIdx.y];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        if (row >= col) {\n            C[row * N + col] = sum;\n        } else {\n            C[row * N + col] = 0.0f;\n        }\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimized algorithm computes only the lower triangular part of the result matrix by restricting the inner loop to k <= min(row, col) and using shared memory for faster access to A and B tiles.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + blockDim.x - 1) / blockDim.x; ++tile) {\n        int tile_row = tile * blockDim.x + threadIdx.x;\n        int tile_col = tile * blockDim.y + threadIdx.y;\n\n        if (row < N && tile_col < N) {\n            sA[threadIdx.y * blockDim.x + threadIdx.x] = (tile_col <= row) ? A[row * N + tile_col] : 0.0f;\n        } else {\n            sA[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = (tile_row <= col) ? B[tile_row * N + col] : 0.0f;\n        } else {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (row < N && col < N && row >= col) {\n            for (int k = 0; k < blockDim.x; ++k) {\n                sum += sA[k * blockDim.y + threadIdx.y] * sB[threadIdx.x * blockDim.y + k];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize by reducing redundant computations in the inner loop through thread coarsening and leveraging shared memory for data reuse, while maintaining correctness by only computing necessary elements for the lower triangular result.}",
        "function": "#include <torch/extension.h>\n\n__global__ void tril_matmul_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.x * blockDim.y;\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n    \n    for (int tile = 0; tile <= (N + blockDim.x - 1) / blockDim.x; ++tile) {\n        int k = tile * blockDim.x + threadIdx.x;\n        \n        if (row < N && k < N && k <= row) {\n            sA[threadIdx.y * blockDim.x + threadIdx.x] = A[row * N + k];\n        } else {\n            sA[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && k < N) {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = B[k * N + col];\n        } else {\n            sB[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n\n        for (int i = 0; i < blockDim.x && (tile * blockDim.x + i) <= row; ++i) {\n            sum += sA[threadIdx.y * blockDim.x + i] * sB[i * blockDim.y + threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = (row >= col) ? sum : 0.0f;\n    }\n}\n\ntorch::Tensor tril_matmul_forward(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 block_size(16, 16);\n    dim3 grid_size((N + block_size.x - 1) / block_size.x, (N + block_size.y - 1) / block_size.y);\n    size_t shared_mem_size = 2 * block_size.x * block_size.y * sizeof(float);\n\n    tril_matmul_kernel<<<grid_size, block_size, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &tril_matmul_forward, \"Matrix multiplication of lower triangular matrices A and B.\");\n}",
        "score": null
    }
]