[
    {
        "algorithm": "{Optimized implementation using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < m && tiled_k < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < n && tiled_k < k) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + tiled_k * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.6619679987430572
    },
    {
        "algorithm": "{Optimized batched matrix multiplication using shared memory for data reuse and thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && a_col < k) ? A_batch[row * k + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < k && col < n) ? B_batch[b_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.6625376045703888
    },
    {
        "algorithm": "{Optimized batched matrix multiplication using shared memory tiling to reduce global memory accesses and improve memory coalescing, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int bx = blockIdx.x, by = blockIdx.y, bz = blockIdx.z;\n    int tx = threadIdx.x, ty = threadIdx.y;\n\n    int row = by * TILE_SIZE + ty;\n    int col = bx * TILE_SIZE + tx;\n    int batch = bz;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + tx;\n        int B_row = tiled_k + ty;\n\n        if (row < m && A_col < k) {\n            As[ty][tx] = A_batch[row * k + A_col];\n        } else {\n            As[ty][tx] = 0.0f;\n        }\n\n        if (B_row < k && col < n) {\n            Bs[ty][tx] = B_batch[B_row * n + col];\n        } else {\n            Bs[ty][tx] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[ty][i] * Bs[i][tx];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.6634207963943481
    },
    {
        "algorithm": "{Optimized batched matrix multiplication using shared memory for data reuse and increased thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < m && a_col < k) {\n            As[threadIdx.y][threadIdx.x] = A_batch[row * k + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B_batch[b_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.780268794298172
    },
    {
        "algorithm": "{Optimized batched matrix multiplication using shared memory for data reuse and increased occupancy with larger block sizes.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tile from A\n        int load_row = row;\n        int load_col = tiled_k + threadIdx.x;\n        if (load_row < m && load_col < k) {\n            As[threadIdx.y][threadIdx.x] = A_batch[load_row * k + load_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        // Load tile from B\n        load_row = tiled_k + threadIdx.y;\n        load_col = col;\n        if (load_row < k && load_col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B_batch[load_row * n + load_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.7808703899383544
    }
]