[
    {
        "sample_order": 1,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of A and B matrices for each thread block, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n\n        if (row < m && A_col < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + B_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimized batched matrix multiplication using shared memory for data reuse and increased occupancy with larger block sizes.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tile from A\n        int load_row = row;\n        int load_col = tiled_k + threadIdx.x;\n        if (load_row < m && load_col < k) {\n            As[threadIdx.y][threadIdx.x] = A_batch[load_row * k + load_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        // Load tile from B\n        load_row = tiled_k + threadIdx.y;\n        load_col = col;\n        if (load_row < k && load_col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B_batch[load_row * n + load_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.7808703899383544
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimized implementation using shared memory for tiling to reduce global memory accesses and improve memory coalescing, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                   const float* __restrict__ B,\n                                   float* __restrict__ C,\n                                   int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < m && a_col < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < n && b_row < k) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + b_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimized implementation using shared memory tiling to reduce global memory accesses and increase memory bandwidth utilization while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < m && a_col < k) {\n            As[threadIdx.y][threadIdx.x] = A_batch[row * k + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B_batch[b_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimized batched matrix multiplication using shared memory tiling and loop unrolling to improve memory access patterns and reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                   const float* __restrict__ B,\n                                   float* __restrict__ C,\n                                   int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && a_col < k) ? A_batch[row * k + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < k && col < n) ? B_batch[b_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the batched matrix multiplication by using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < m && tiled_k < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < n && tiled_k < k) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + tiled_k * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimized implementation using shared memory tiling and loop unrolling to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                   const float* __restrict__ B,\n                                   float* __restrict__ C,\n                                   int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && a_col < k) ? A_batch[row * k + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (col < n && b_row < k) ? B_batch[b_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the batched matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < m && a_col < k) {\n            As[threadIdx.y][threadIdx.x] = A_batch[row * k + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B_batch[b_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimized implementation using shared memory for tiling and loop unrolling to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_row = row;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n        int B_col = col;\n\n        if (A_row < m && A_col < k) {\n            As[threadIdx.y][threadIdx.x] = A_batch[A_row * k + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < k && B_col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B_batch[B_row * n + B_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimized the batched matrix multiplication by using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n\n        if (row < m && A_col < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + B_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimized implementation using shared memory tiling to reduce global memory accesses and improve memory coalescing, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_row = row;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n        int B_col = col;\n\n        if (A_row < m && A_col < k) {\n            As[threadIdx.y][threadIdx.x] = A_batch[A_row * k + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < k && B_col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B_batch[B_row * n + B_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimized batched matrix multiplication using shared memory tiling and loop unrolling to improve memory access patterns and reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && A_col < k) ? A_batch[row * k + A_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (B_row < k && col < n) ? B_batch[B_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the batched matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n\n        if (row < m && A_col < k) {\n            As[threadIdx.y][threadIdx.x] = A_batch[row * k + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B_batch[B_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\n// The rest of the code remains the same as in the original implementation",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimized batched matrix multiplication using shared memory for data reuse and thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && a_col < k) ? A_batch[row * k + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < k && col < n) ? B_batch[b_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.6625376045703888
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimized implementation using shared memory for tiling and increased block size to improve memory access patterns and occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define TILE_SIZE 32\n#define BLOCK_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && A_col < k) ? A_batch[row * k + A_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (B_row < k && col < n) ? B_batch[B_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the batched matrix multiplication by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < m && a_col < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + b_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimized implementation using shared memory tiling to reduce global memory accesses and increase memory bandwidth utilization while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                   const float* __restrict__ B,\n                                   float* __restrict__ C,\n                                   int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (k + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_col = tile_offset + threadIdx.x;\n        int b_row = tile_offset + threadIdx.y;\n\n        if (row < m && a_col < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + b_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < m && tiled_k < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < n && tiled_k < k) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + tiled_k * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.6619679987430572
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimized implementation using shared memory for tiling to reduce global memory accesses and improve memory coalescing, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < m && a_col < k) {\n            As[threadIdx.y][threadIdx.x] = A_batch[row * k + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B_batch[b_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimized implementation using shared memory for tile-based matrix multiplication with increased block size for better utilization of RTX 4090's resources.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && A_col < k) ? A_batch[row * k + A_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (B_row < k && col < n) ? B_batch[B_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimized batched matrix multiplication using shared memory for data reuse and increased thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < m && a_col < k) {\n            As[threadIdx.y][threadIdx.x] = A_batch[row * k + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B_batch[b_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.780268794298172
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the batched matrix multiplication by utilizing shared memory for block-level matrix multiplication and increasing thread block utilization with larger BLOCK_SIZE.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (k + BLOCK_SIZE - 1) / BLOCK_SIZE; ++tile) {\n        int tile_offset = tile * BLOCK_SIZE;\n        int a_col = tile_offset + threadIdx.x;\n        int b_row = tile_offset + threadIdx.y;\n\n        if (row < m && a_col < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < k && col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + b_row * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < BLOCK_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimized batched matrix multiplication using shared memory tiling to reduce global memory accesses and improve memory coalescing, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int bx = blockIdx.x, by = blockIdx.y, bz = blockIdx.z;\n    int tx = threadIdx.x, ty = threadIdx.y;\n\n    int row = by * TILE_SIZE + ty;\n    int col = bx * TILE_SIZE + tx;\n    int batch = bz;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + tx;\n        int B_row = tiled_k + ty;\n\n        if (row < m && A_col < k) {\n            As[ty][tx] = A_batch[row * k + A_col];\n        } else {\n            As[ty][tx] = 0.0f;\n        }\n\n        if (B_row < k && col < n) {\n            Bs[ty][tx] = B_batch[B_row * n + col];\n        } else {\n            Bs[ty][tx] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[ty][i] * Bs[i][tx];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.6634207963943481
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the batched matrix multiplication by using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                   const float* __restrict__ B,\n                                   float* __restrict__ C,\n                                   int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && a_col < k) ? A_batch[row * k + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < k && col < n) ? B_batch[b_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\n// Keep the rest of the code (batchedMatMulLauncher, forward, PYBIND11_MODULE) the same",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the batched matrix multiplication by utilizing shared memory for block-level matrix multiplication and increasing thread block utilization for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < m && tiled_k < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < n && tiled_k < k) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + tiled_k * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.6620416045188904
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_row = row;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n        int B_col = col;\n\n        if (A_row < m && A_col < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + A_row * k + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < k && B_col < n) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + B_row * n + B_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimized batched matrix multiplication using warp-level tiling and register accumulation to reduce shared memory usage and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    const int row = (blockIdx.y * WARPS_PER_BLOCK + warp_id) * 2 + (lane_id / 16);\n    const int col = blockIdx.x * TILE_SIZE + (lane_id % 16) * 2;\n    const int batch = blockIdx.z;\n\n    float sum[2][2] = {{0.0f, 0.0f}, {0.0f, 0.0f}};\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < k; ++t) {\n        float a_val[2];\n        float b_val[2];\n        \n        if (row < m && t < k) {\n            a_val[0] = A_batch[row * k + t];\n            a_val[1] = (row + 1) < m ? A_batch[(row + 1) * k + t] : 0.0f;\n        } else {\n            a_val[0] = a_val[1] = 0.0f;\n        }\n\n        if (t < k && col < n) {\n            b_val[0] = B_batch[t * n + col];\n            b_val[1] = (col + 1) < n ? B_batch[t * n + (col + 1)] : 0.0f;\n        } else {\n            b_val[0] = b_val[1] = 0.0f;\n        }\n\n        sum[0][0] += a_val[0] * b_val[0];\n        sum[0][1] += a_val[0] * b_val[1];\n        sum[1][0] += a_val[1] * b_val[0];\n        sum[1][1] += a_val[1] * b_val[1];\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum[0][0];\n        if (col + 1 < n) C[batch * m * n + row * n + (col + 1)] = sum[0][1];\n    }\n    if (row + 1 < m && col < n) {\n        C[batch * m * n + (row + 1) * n + col] = sum[1][0];\n        if (col + 1 < n) C[batch * m * n + (row + 1) * n + (col + 1)] = sum[1][1];\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(WARP_SIZE * WARPS_PER_BLOCK);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + (2 * WARPS_PER_BLOCK) - 1) / (2 * WARPS_PER_BLOCK),\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimized batched matrix multiplication using warp-level tiling and register accumulation to reduce shared memory usage and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define WARP_TILES 4\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    const int warp_id = threadIdx.y;\n    const int lane_id = threadIdx.x;\n    const int row = blockIdx.y * TILE_SIZE + warp_id * WARP_TILES;\n    const int col = blockIdx.x * TILE_SIZE + lane_id;\n    const int batch = blockIdx.z;\n\n    float accum[WARP_TILES] = {0.0f};\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        const int tiled_k = t * TILE_SIZE;\n        \n        float a_frag[WARP_TILES];\n        float b_frag;\n\n        #pragma unroll\n        for (int i = 0; i < WARP_TILES; ++i) {\n            int load_row = row + i;\n            int load_col = tiled_k + lane_id;\n            a_frag[i] = (load_row < m && load_col < k) ? A_batch[load_row * k + load_col] : 0.0f;\n        }\n\n        #pragma unroll\n        for (int j = 0; j < TILE_SIZE; j += WARP_SIZE) {\n            int load_row = tiled_k + j + lane_id;\n            int load_col = col;\n            b_frag = (load_row < k && load_col < n) ? B_batch[load_row * n + load_col] : 0.0f;\n\n            #pragma unroll\n            for (int i = 0; i < WARP_TILES; ++i) {\n                accum[i] += a_frag[i] * b_frag;\n            }\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < WARP_TILES; ++i) {\n        int write_row = row + i;\n        if (write_row < m && col < n) {\n            C[batch * m * n + write_row * n + col] = accum[i];\n        }\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(WARP_SIZE, WARP_SIZE / WARP_TILES);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimized batched matrix multiplication using warp-level tiling and register accumulation to reduce shared memory usage and improve instruction-level parallelism, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x % (WARP_SIZE / 4);\n    const int warp_id = threadIdx.x / (WARP_SIZE / 4);\n\n    const int row = blockIdx.y * TILE_SIZE + warp_row * 4;\n    const int col = blockIdx.x * TILE_SIZE + warp_id * (WARP_SIZE / 4) + warp_col;\n    const int batch = blockIdx.z;\n\n    float sum[4] = {0.0f, 0.0f, 0.0f, 0.0f};\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < k; t += WARP_SIZE) {\n        float a_reg[4];\n        float b_reg[4];\n\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            if (row + i < m && t + threadIdx.x < k) {\n                a_reg[i] = A_batch[(row + i) * k + t + threadIdx.x];\n            } else {\n                a_reg[i] = 0.0f;\n            }\n        }\n\n        if (t + threadIdx.x < k && col < n) {\n            b_reg[0] = B_batch[(t + threadIdx.x) * n + col];\n        } else {\n            b_reg[0] = 0.0f;\n        }\n\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            #pragma unroll\n            for (int j = 0; j < 4; ++j) {\n                sum[i] += a_reg[j] * __shfl_sync(0xffffffff, b_reg[0], j * (WARP_SIZE / 4) + warp_col);\n            }\n        }\n    }\n\n    if (row < m && col < n) {\n        for (int i = 0; i < 4; ++i) {\n            if (row + i < m) {\n                C[batch * m * n + (row + i) * n + col] = sum[i];\n            }\n        }\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(WARP_SIZE, 8);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimized batched matrix multiplication using warp-level tiling and register accumulation to minimize shared memory usage and maximize instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define WARP_TILES 4\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x;\n    const int warp_id = threadIdx.y * WARP_SIZE + threadIdx.x;\n    const int warp_tile_row = warp_id / (TILE_SIZE / WARP_TILES);\n    const int warp_tile_col = warp_id % (TILE_SIZE / WARP_TILES);\n\n    const int row = blockIdx.y * TILE_SIZE + warp_tile_row * WARP_TILES + warp_row;\n    const int col = blockIdx.x * TILE_SIZE + warp_tile_col * WARP_TILES + warp_col;\n    const int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        const int tiled_k = t * TILE_SIZE;\n        const int a_col = tiled_k + warp_tile_col * WARP_TILES + warp_col;\n        const int b_row = tiled_k + warp_tile_row * WARP_TILES + warp_row;\n\n        float a_val = 0.0f;\n        float b_val = 0.0f;\n\n        if (row < m && a_col < k) {\n            a_val = A_batch[row * k + a_col];\n        }\n        if (b_row < k && col < n) {\n            b_val = B_batch[b_row * n + col];\n        }\n\n        for (int i = 0; i < WARP_TILES; ++i) {\n            sum += a_val * b_val;\n        }\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(WARP_SIZE, WARP_TILES);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimized batched matrix multiplication using register tiling to reduce shared memory usage and increase thread-level parallelism while maintaining data reuse.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 8\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y * TILE_SIZE;\n    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x * TILE_SIZE;\n    int batch = blockIdx.z;\n\n    float sum[TILE_SIZE][TILE_SIZE] = {0.0f};\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + BLOCK_SIZE - 1) / BLOCK_SIZE; ++t) {\n        int tiled_k = t * BLOCK_SIZE;\n        \n        float A_tile[TILE_SIZE];\n        float B_tile[TILE_SIZE];\n\n        for (int tile_row = 0; tile_row < TILE_SIZE; ++tile_row) {\n            int global_row = row + tile_row;\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                int global_col = tiled_k + threadIdx.x * TILE_SIZE + i;\n                A_tile[i] = (global_row < m && global_col < k) ? \n                            A_batch[global_row * k + global_col] : 0.0f;\n            }\n\n            for (int tile_col = 0; tile_col < TILE_SIZE; ++tile_col) {\n                int global_col = col + tile_col;\n                for (int i = 0; i < TILE_SIZE; ++i) {\n                    int global_row = tiled_k + threadIdx.y * TILE_SIZE + i;\n                    B_tile[i] = (global_row < k && global_col < n) ? \n                               B_batch[global_row * n + global_col] : 0.0f;\n                }\n\n                for (int i = 0; i < TILE_SIZE; ++i) {\n                    sum[tile_row][tile_col] += A_tile[i] * B_tile[i];\n                }\n            }\n        }\n    }\n\n    for (int tile_row = 0; tile_row < TILE_SIZE; ++tile_row) {\n        for (int tile_col = 0; tile_col < TILE_SIZE; ++tile_col) {\n            int global_row = row + tile_row;\n            int global_col = col + tile_col;\n            if (global_row < m && global_col < n) {\n                C[batch * m * n + global_row * n + global_col] = sum[tile_row][tile_col];\n            }\n        }\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(BLOCK_SIZE/TILE_SIZE, BLOCK_SIZE/TILE_SIZE);\n    dim3 grid((n + BLOCK_SIZE - 1) / BLOCK_SIZE,\n              (m + BLOCK_SIZE - 1) / BLOCK_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimized batched matrix multiplication using register tiling to reduce shared memory usage and increase arithmetic intensity, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_K 4\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    float regA[TILE_K], regB[TILE_K];\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < k; t += TILE_K) {\n        #pragma unroll\n        for (int i = 0; i < TILE_K; ++i) {\n            int k_idx = t + i;\n            regA[i] = (row < m && k_idx < k) ? A_batch[row * k + k_idx] : 0.0f;\n            regB[i] = (k_idx < k && col < n) ? B_batch[k_idx * n + col] : 0.0f;\n        }\n\n        #pragma unroll\n        for (int i = 0; i < TILE_K; ++i) {\n            sum += regA[i] * regB[i];\n        }\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 grid((n + BLOCK_SIZE - 1) / BLOCK_SIZE,\n              (m + BLOCK_SIZE - 1) / BLOCK_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -1.0715968132019043
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimized batched matrix multiplication using shared memory with double buffering to overlap memory transfers with computation for improved performance.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 16\n#define TILE_SIZE 16\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    int load_phase = 0;\n    int compute_phase = 0;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Load next tile while computing current tile\n        if (row < m && a_col < k) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A_batch[row * k + a_col];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < k && col < n) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B_batch[b_row * n + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute with previous tile\n        if (t > 0) {\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                sum += As[compute_phase][threadIdx.y][i] * Bs[compute_phase][i][threadIdx.x];\n            }\n        }\n\n        compute_phase = load_phase;\n        load_phase = 1 - load_phase;\n        __syncthreads();\n    }\n\n    // Compute last tile\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        sum += As[compute_phase][threadIdx.y][i] * Bs[compute_phase][i][threadIdx.x];\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.7147552073001862
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication and register tiling to further reduce shared memory usage and improve occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 8\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    const int warp_row = threadIdx.y / 4;\n    const int warp_col = threadIdx.x / 4;\n    const int lane_row = threadIdx.y % 4;\n    const int lane_col = threadIdx.x % 4;\n\n    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum[TILE_SIZE][TILE_SIZE] = {0.0f};\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        float a_frag[TILE_SIZE];\n        float b_frag[TILE_SIZE];\n\n        int tiled_k = t * TILE_SIZE;\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            int k_idx = tiled_k + i;\n            if (row < m && k_idx < k) {\n                a_frag[i] = A[batch * m * k + row * k + k_idx];\n            } else {\n                a_frag[i] = 0.0f;\n            }\n\n            if (col < n && k_idx < k) {\n                b_frag[i] = B[batch * k * n + k_idx * n + col];\n            } else {\n                b_frag[i] = 0.0f;\n            }\n        }\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            for (int j = 0; j < TILE_SIZE; ++j) {\n                sum[i][j] += a_frag[i] * b_frag[j];\n            }\n        }\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum[lane_row][lane_col];\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 grid((n + BLOCK_SIZE - 1) / BLOCK_SIZE,\n              (m + BLOCK_SIZE - 1) / BLOCK_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimized batched matrix multiplication using shared memory with loop unrolling and increased block size for better memory access patterns and higher occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int bx = blockIdx.x, by = blockIdx.y, bz = blockIdx.z;\n    int tx = threadIdx.x, ty = threadIdx.y;\n\n    int row = by * TILE_SIZE + ty;\n    int col = bx * TILE_SIZE + tx;\n    int batch = bz;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + tx;\n        int B_row = tiled_k + ty;\n\n        if (row < m && A_col < k) {\n            As[ty][tx] = A_batch[row * k + A_col];\n        } else {\n            As[ty][tx] = 0.0f;\n        }\n\n        if (B_row < k && col < n) {\n            Bs[ty][tx] = B_batch[B_row * n + col];\n        } else {\n            Bs[ty][tx] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[ty][i] * Bs[i][tx];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimized batched matrix multiplication using shared memory with improved memory access patterns and loop unrolling for better performance.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && a_col < k) ? A_batch[row * k + a_col] : 0.0f;\n        Bs[threadIdx.x][threadIdx.y] = (b_row < k && col < n) ? B_batch[b_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[threadIdx.x][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -2.1700320482254027
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimized batched matrix multiplication using shared memory with increased tile size and loop unrolling for better memory access patterns and reduced synchronization overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && a_col < k) ? A_batch[row * k + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < k && col < n) ? B_batch[b_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": -0.7829279959201813
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimized batched matrix multiplication using shared memory tiling with loop unrolling and increased register usage to reduce shared memory bank conflicts and improve instruction-level parallelism, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 8\n#define UNROLL_FACTOR 4\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE*UNROLL_FACTOR];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE*UNROLL_FACTOR];\n\n    int bx = blockIdx.x, by = blockIdx.y, bz = blockIdx.z;\n    int tx = threadIdx.x, ty = threadIdx.y;\n\n    int row = by * BLOCK_SIZE + ty;\n    int col = bx * BLOCK_SIZE + tx;\n    int batch = bz;\n\n    float sum[UNROLL_FACTOR][UNROLL_FACTOR] = {{0.0f}};\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; t += UNROLL_FACTOR) {\n        #pragma unroll\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            int tiled_k = (t + u) * TILE_SIZE;\n            int A_col = tiled_k + tx;\n            int B_row = tiled_k + ty;\n\n            if (row < m && A_col < k) {\n                As[ty][tx + u*TILE_SIZE] = A_batch[row * k + A_col];\n            } else {\n                As[ty][tx + u*TILE_SIZE] = 0.0f;\n            }\n\n            if (B_row < k && col < n) {\n                Bs[ty][tx + u*TILE_SIZE] = B_batch[B_row * n + col];\n            } else {\n                Bs[ty][tx + u*TILE_SIZE] = 0.0f;\n            }\n        }\n        __syncthreads();\n\n        #pragma unroll\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            #pragma unroll\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                #pragma unroll\n                for (int v = 0; v < UNROLL_FACTOR; ++v) {\n                    sum[u][v] += As[ty][i + u*TILE_SIZE] * Bs[i][tx + v*TILE_SIZE];\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        float final_sum = 0.0f;\n        #pragma unroll\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            #pragma unroll\n            for (int v = 0; v < UNROLL_FACTOR; ++v) {\n                final_sum += sum[u][v];\n            }\n        }\n        C[batch * m * n + row * n + col] = final_sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 grid((n + BLOCK_SIZE - 1) / BLOCK_SIZE,\n              (m + BLOCK_SIZE - 1) / BLOCK_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimized implementation using shared memory with increased tile size (32x32) and loop unrolling for better memory access patterns and reduced synchronization overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < m && tiled_k < k) {\n            As[threadIdx.y][threadIdx.x] = A[batch * m * k + row * k + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < n && tiled_k < k) {\n            Bs[threadIdx.y][threadIdx.x] = B[batch * k * n + tiled_k * n + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimized batched matrix multiplication using shared memory with increased tile size (32x32) and loop unrolling for better memory access patterns and reduced synchronization overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && a_col < k) ? A_batch[row * k + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < k && col < n) ? B_batch[b_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimized batched matrix multiplication using shared memory with increased tile size (32x32) and loop unrolling for better memory access patterns and reduced synchronization overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int batch = blockIdx.z;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < m && a_col < k) ? A_batch[row * k + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < k && col < n) ? B_batch[b_row * n + col] : 0.0f;\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimized batched matrix multiplication using shared memory tiling with increased tile size (32x32) and loop unrolling for better memory utilization and reduced synchronization overhead, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int bx = blockIdx.x, by = blockIdx.y, bz = blockIdx.z;\n    int tx = threadIdx.x, ty = threadIdx.y;\n\n    int row = by * TILE_SIZE + ty;\n    int col = bx * TILE_SIZE + tx;\n    int batch = bz;\n\n    float sum = 0.0f;\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + tx;\n        int B_row = tiled_k + ty;\n\n        if (row < m && A_col < k) {\n            As[ty][tx] = A_batch[row * k + A_col];\n        } else {\n            As[ty][tx] = 0.0f;\n        }\n\n        if (B_row < k && col < n) {\n            Bs[ty][tx] = B_batch[B_row * n + col];\n        } else {\n            Bs[ty][tx] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[ty][i] * Bs[i][tx];\n        }\n        __syncthreads();\n    }\n\n    if (row < m && col < n) {\n        C[batch * m * n + row * n + col] = sum;\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Implement a warp-level tiled matrix multiplication kernel that utilizes warp shuffles for efficient data sharing between threads within a warp, reducing shared memory usage and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int row = (blockIdx.y * TILE_SIZE) + (warp_id * (TILE_SIZE / WARP_SIZE)) + (lane_id / 4);\n    int col = (blockIdx.x * TILE_SIZE) + (lane_id % 4) * 8;\n    int batch = blockIdx.z;\n\n    float sum[8] = {0.0f};\n\n    for (int t = 0; t < (k + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + lane_id;\n        float a_val = 0.0f;\n        if (row < m && tiled_k < k) {\n            a_val = A[batch * m * k + row * k + tiled_k];\n        }\n\n        for (int i = 0; i < 8; ++i) {\n            int b_col = col + i;\n            float b_val = 0.0f;\n            if (b_col < n && tiled_k < k) {\n                b_val = B[batch * k * n + tiled_k * n + b_col];\n            }\n            sum[i] += a_val * b_val;\n        }\n    }\n\n    for (int i = 0; i < 8; ++i) {\n        if (row < m && (col + i) < n) {\n            C[batch * m * n + row * n + (col + i)] = sum[i];\n        }\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(128);  // 4 warps per block\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimized batched matrix multiplication using warp-level tiling and register accumulation to maximize memory throughput and minimize shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    \n    const int row = (blockIdx.y * WARPS_PER_BLOCK + warp_id) * TILE_SIZE + (lane_id / 4);\n    const int col = blockIdx.x * TILE_SIZE + (lane_id % 4) * 8;\n    const int batch = blockIdx.z;\n\n    float sum[8] = {0.0f};\n    \n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < k; t += 4) {\n        float a_frag[4];\n        float b_frag[8];\n        \n        if (row < m && t + (lane_id % 4) < k) {\n            a_frag[lane_id % 4] = A_batch[row * k + t + (lane_id % 4)];\n        } else {\n            a_frag[lane_id % 4] = 0.0f;\n        }\n        \n        if (t + (lane_id / 8) < k && col + (lane_id % 8) < n) {\n            b_frag[lane_id % 8] = B_batch[(t + (lane_id / 8)) * n + col + (lane_id % 8)];\n        } else {\n            b_frag[lane_id % 8] = 0.0f;\n        }\n\n        for (int i = 0; i < 4; ++i) {\n            for (int j = 0; j < 8; ++j) {\n                sum[j] += a_frag[i] * b_frag[j];\n            }\n        }\n    }\n\n    if (row < m && col < n) {\n        for (int j = 0; j < 8; ++j) {\n            if (col + j < n) {\n                C[batch * m * n + row * n + col + j] = sum[j];\n            }\n        }\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(WARPS_PER_BLOCK * WARP_SIZE);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK),\n              batch_size);\n\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::empty({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimized batched matrix multiplication using warp-level tiling and register accumulation to minimize shared memory usage and maximize thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <vector>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void batchedMatMulKernel(const float* __restrict__ A,\n                                    const float* __restrict__ B,\n                                    float* __restrict__ C,\n                                    int m, int n, int k) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x;\n    const int warp_id = threadIdx.y * blockDim.x + threadIdx.x;\n    const int warp_per_block = blockDim.x * blockDim.y / WARP_SIZE;\n\n    const int row = blockIdx.y * TILE_SIZE + warp_row * (TILE_SIZE / WARP_SIZE);\n    const int col = blockIdx.x * TILE_SIZE + (warp_id % warp_per_block) * (TILE_SIZE / warp_per_block);\n    const int batch = blockIdx.z;\n\n    float sum[TILE_SIZE / WARP_SIZE][TILE_SIZE / warp_per_block] = {0.0f};\n\n    const float* A_batch = A + batch * m * k;\n    const float* B_batch = B + batch * k * n;\n\n    for (int t = 0; t < k; ++t) {\n        float a_val = (row < m && t < k) ? A_batch[row * k + t] : 0.0f;\n        float b_val = (t < k && col < n) ? B_batch[t * n + col] : 0.0f;\n\n        for (int i = 0; i < TILE_SIZE / WARP_SIZE; ++i) {\n            for (int j = 0; j < TILE_SIZE / warp_per_block; ++j) {\n                sum[i][j] += a_val * b_val;\n            }\n        }\n    }\n\n    if (row < m && col < n) {\n        for (int i = 0; i < TILE_SIZE / WARP_SIZE; ++i) {\n            for (int j = 0; j < TILE_SIZE / warp_per_block; ++j) {\n                int out_row = row + i;\n                int out_col = col + j;\n                if (out_row < m && out_col < n) {\n                    atomicAdd(&C[batch * m * n + out_row * n + out_col], sum[i][j]);\n                }\n            }\n        }\n    }\n}\n\nvoid batchedMatMulLauncher(const at::Tensor A, const at::Tensor B, at::Tensor C) {\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    dim3 block(32, 8);\n    dim3 grid((n + TILE_SIZE - 1) / TILE_SIZE,\n              (m + TILE_SIZE - 1) / TILE_SIZE,\n              batch_size);\n\n    C.fill_(0);\n    batchedMatMulKernel<<<grid, block>>>(A.data_ptr<float>(),\n                                        B.data_ptr<float>(),\n                                        C.data_ptr<float>(),\n                                        m, n, k);\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(\"CUDA kernel failed: \" + std::string(cudaGetErrorString(err)));\n    }\n}\n\nat::Tensor forward(at::Tensor A, at::Tensor B) {\n    if (!A.is_cuda() || !B.is_cuda()) {\n        throw std::runtime_error(\"Input tensors must be CUDA tensors\");\n    }\n\n    const auto batch_size = A.size(0);\n    const auto m = A.size(1);\n    const auto k = A.size(2);\n    const auto n = B.size(2);\n\n    auto C = at::zeros({batch_size, m, n}, A.options());\n    batchedMatMulLauncher(A, B, C);\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Batched Matrix Multiplication (CUDA)\");",
        "score": null
    }
]