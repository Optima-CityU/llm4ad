[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and leveraging the symmetric property to halve the computation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses, and maintaining correctness for symmetric matrices.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize by utilizing shared memory for blocking matrix multiplication and leveraging symmetry of input matrices to reduce memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and leveraging the symmetric property of the inputs.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B matrices, reducing global memory accesses, and using register tiling for better instruction-level parallelism while maintaining correctness for symmetric matrices.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_col = t * TILE_SIZE + threadIdx.x;\n        int tiled_row = t * TILE_SIZE + threadIdx.y;\n\n        if (row < N && tiled_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && tiled_row < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of matrices A and B, reducing global memory accesses, and using a block size that maximizes occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_row = t * TILE_SIZE + threadIdx.y;\n        int tiled_col = t * TILE_SIZE + threadIdx.x;\n\n        As[threadIdx.y][threadIdx.x] = (row < N && tiled_col < N) ? A[row * N + tiled_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (tiled_row < N && col < N) ? B[tiled_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n        \n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of matrices A and B, and utilizing the symmetric property to reduce memory accesses while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B, and using register accumulation to reduce global memory accesses while maintaining correctness for symmetric matrices.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int k = 0; k < N; k += TILE_SIZE) {\n        if (row < N && (k + threadIdx.x) < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && (k + threadIdx.y) < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of A and B, reducing global memory accesses, and leveraging the symmetry property to halve the computation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n        \n        if (row < N && tile_col < N) As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        else As[threadIdx.y][threadIdx.x] = 0.0f;\n        \n        if (tile_row < N && col < N) Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        else Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    \n    if (row < N && col < N) C[row * N + col] = sum;\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and leveraging the symmetric property to halve memory loads.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int k = 0; k < N; k += TILE_SIZE) {\n        if (row < N && (k + threadIdx.x) < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + k + threadIdx.x];\n        }\n        if (col < N && (k + threadIdx.y) < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(k + threadIdx.y) * N + col];\n        }\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE && (k + i) < N; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tRow = t * TILE_SIZE + threadIdx.y;\n        int tCol = t * TILE_SIZE + threadIdx.x;\n        \n        if (row < N && tCol < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tCol];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (tRow < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tRow * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and leveraging the symmetric property of the input matrices.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of symmetric matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int k = 0; k < N; k += TILE_SIZE) {\n        if (row < N && (k + threadIdx.x) < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && (k + threadIdx.y) < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B matrices, reducing global memory accesses while maintaining correctness for symmetric matrices.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n        \n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B matrices, reducing global memory accesses while maintaining correctness for symmetric matrices.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses, and maintaining the same computation pattern to ensure correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_col = tile_offset + threadIdx.x;\n        int b_row = tile_offset + threadIdx.y;\n\n        if (row < N && a_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to reduce global memory accesses and utilizing the symmetry property of input matrices to reduce computation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && tile_row < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B matrices, reducing global memory accesses, and using 32x32 thread blocks for better occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of symmetric matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int k = 0; k < N; k += TILE_SIZE) {\n        if (row < N && (k + threadIdx.x) < N) {\n            sA[threadIdx.y][threadIdx.x] = A[row * N + k + threadIdx.x];\n        } else {\n            sA[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if ((k + threadIdx.y) < N && col < N) {\n            sB[threadIdx.y][threadIdx.x] = B[(k + threadIdx.y) * N + col];\n        } else {\n            sB[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += sA[threadIdx.y][i] * sB[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int k = 0; k < (N + TILE_SIZE - 1) / TILE_SIZE; ++k) {\n        int a_col = k * TILE_SIZE + threadIdx.x;\n        int b_row = k * TILE_SIZE + threadIdx.y;\n\n        if (row < N && a_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to reduce global memory accesses and using tiling with symmetric matrix properties for better cache utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_col = tile_offset + threadIdx.x;\n        int b_row = tile_offset + threadIdx.y;\n\n        if (row < N && a_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to reduce global memory accesses and utilizing the symmetry property of input matrices to reduce computations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses, and maintaining correctness for symmetric matrices.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n        \n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of symmetric matrices A and B, reducing global memory accesses and exploiting matrix symmetry to halve computation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int load_row = row;\n        int load_col = tile_offset + threadIdx.x;\n        \n        if (load_row < N && load_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * N + load_col];\n            Bs[threadIdx.y][threadIdx.x] = B[load_col * N + load_row]; // Transpose B\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[threadIdx.x][k]; // B is transposed\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B matrices, reducing global memory accesses while maintaining correctness for symmetric matrices.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of symmetric matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int k = 0; k < N; k += TILE_SIZE) {\n        // Load tiles into shared memory\n        if (row < N && (k + threadIdx.x) < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((k + threadIdx.y) < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial sum\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of matrices A and B, reducing global memory accesses, and using 32x32 thread blocks for better occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int A_col = t * TILE_SIZE + threadIdx.x;\n        int B_row = t * TILE_SIZE + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < N && A_col < N) ? A[row * N + A_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (B_row < N && col < N) ? B[B_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of symmetric matrices A and B, reducing global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_col = t * TILE_SIZE + threadIdx.x;\n        int tiled_row = t * TILE_SIZE + threadIdx.y;\n\n        if (row < N && tiled_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tiled_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of symmetric matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to reduce global memory accesses and utilize the symmetry of input matrices to halve computation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_col = tile_offset + threadIdx.x;\n        int b_row = tile_offset + threadIdx.y;\n\n        if (row < N && a_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and leveraging the symmetric property to potentially reduce computations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of matrices A and B, leveraging the symmetry property to reduce memory accesses, with careful attention to thread synchronization and bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tRow = t * TILE_SIZE + threadIdx.y;\n        int tCol = t * TILE_SIZE + threadIdx.x;\n\n        As[threadIdx.y][threadIdx.x] = (row < N && tCol < N) ? A[row * N + tCol] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (tRow < N && col < N) ? B[tRow * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of symmetric matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int k = 0; k < (N + TILE_SIZE - 1) / TILE_SIZE; ++k) {\n        int A_col = k * TILE_SIZE + threadIdx.x;\n        int B_row = k * TILE_SIZE + threadIdx.y;\n        \n        if (row < N && A_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to reduce global memory accesses and utilizing the symmetry property of input matrices to halve the computation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_row = t * TILE_SIZE + threadIdx.y;\n        int tiled_col = t * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tiled_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tiled_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize matrix multiplication using shared memory to cache tiles of symmetric matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int t_col = t * TILE_SIZE + threadIdx.x;\n        int t_row = t * TILE_SIZE + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < N && t_col < N) ? A[row * N + t_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (t_row < N && col < N) ? B[t_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to reduce global memory accesses and using a tiling approach to improve data locality.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to reduce global memory accesses and utilizing the symmetry property of input matrices to minimize redundant computations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_row = tile * TILE_SIZE + threadIdx.y;\n        int tiled_col = tile * TILE_SIZE + threadIdx.x;\n        \n        if (row < N && tiled_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (tiled_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_row = t * TILE_SIZE + threadIdx.y;\n        int tiled_col = t * TILE_SIZE + threadIdx.x;\n        \n        if (row < N && tiled_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tiled_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of the symmetric matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tRow = t * TILE_SIZE + threadIdx.y;\n        int tCol = t * TILE_SIZE + threadIdx.x;\n        \n        if (row < N && tCol < N) As[threadIdx.y][threadIdx.x] = A[row * N + tCol];\n        else As[threadIdx.y][threadIdx.x] = 0.0f;\n        \n        if (tRow < N && col < N) Bs[threadIdx.y][threadIdx.x] = B[tRow * N + col];\n        else Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        \n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to reduce global memory accesses and utilizing the symmetry property of input matrices to reduce computation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of symmetric matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize matrix multiplication by leveraging shared memory for tile-based computation and loop unrolling to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory to cache tiles of matrices A and B, reducing global memory accesses, and using register tiling to improve arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_row = t * TILE_SIZE + threadIdx.y;\n        int tiled_col = t * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tiled_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && tiled_row < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled = t * TILE_SIZE;\n        int A_col = tiled + threadIdx.x;\n        int B_row = tiled + threadIdx.y;\n        \n        if (row < N && A_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (B_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of symmetric matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tiled_k = tile * TILE_SIZE + threadIdx.x;\n        if (row < N && tiled_k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = tile * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of the symmetric matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled = t * TILE_SIZE;\n        \n        // Load tiles into shared memory\n        if (row < N && (tiled + threadIdx.x) < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tiled + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled + threadIdx.y) < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial sum\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimize the matrix multiplication by leveraging shared memory for block-level tiling and utilizing the symmetry property of input matrices to reduce memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_col = tile_offset + threadIdx.x;\n        int b_row = tile_offset + threadIdx.y;\n\n        if (row < N && a_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n                \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize the matrix multiplication by utilizing shared memory to cache tiles of the symmetric matrices A and B, reducing global memory accesses and improving memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int k = 0; k < N; k += TILE_SIZE) {\n        if (row < N && (k + threadIdx.x) < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && (k + threadIdx.y) < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    TORCH_CHECK(A.is_cuda(), \"A must be a CUDA tensor\");\n    TORCH_CHECK(B.is_cuda(), \"B must be a CUDA tensor\");\n    TORCH_CHECK(A.dim() == 2 && B.dim() == 2, \"Inputs must be 2D matrices\");\n    TORCH_CHECK(A.size(0) == A.size(1) && B.size(0) == B.size(1),\n              \"Input matrices must be square\");\n\n    int N = A.size(0);\n    auto C = torch::zeros({N, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (N + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication of symmetric matrices (CUDA)\");\n}",
        "score": null
    }
]