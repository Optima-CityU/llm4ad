[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the matrix multiplication kernel using shared memory for data reuse and tiling to reduce global memory accesses, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        \n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    \n    torch::Tensor C = torch::zeros({M, N}, A.options());\n    \n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n    \n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.623523163795471
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with improved memory access patterns and thread utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int a_col = tiledK + threadIdx.x;\n        int b_row = tiledK + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.6268031358718873
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimized implementation using shared memory tiling to reduce global memory accesses and improve memory coalescing, with block size tuning for RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.676940846443176
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 16x16 thread blocks and proper memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.6320863723754884
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimized implementation using tiled matrix multiplication with shared memory to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.628611159324646
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimized implementation using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.6317792177200316
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimized implementation using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.6295071840286255
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 16x16 thread blocks and proper memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.6274783849716186
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with register tiling and cooperative loading to maximize memory throughput and minimize shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define SUB_TILE 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) / WARP_SIZE;\n    const int warp_col = blockIdx.x * blockDim.x + threadIdx.x;\n    const int lane_id = threadIdx.y * blockDim.x + threadIdx.x;\n\n    float accum[SUB_TILE][SUB_TILE] = {0.0f};\n\n    for (int t = 0; t < K; t += TILE_SIZE) {\n        __shared__ float As[TILE_SIZE][TILE_SIZE];\n        __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n        for (int i = 0; i < TILE_SIZE; i += SUB_TILE) {\n            int load_row = warp_row * TILE_SIZE + threadIdx.y + i;\n            int load_col = t + threadIdx.x;\n            if (load_row < M && load_col < K) {\n                As[threadIdx.y + i][threadIdx.x] = A[load_row * K + load_col];\n            } else {\n                As[threadIdx.y + i][threadIdx.x] = 0.0f;\n            }\n\n            load_row = t + threadIdx.y + i;\n            load_col = warp_col * TILE_SIZE + threadIdx.x;\n            if (load_row < K && load_col < N) {\n                Bs[threadIdx.y + i][threadIdx.x] = B[load_row * N + load_col];\n            } else {\n                Bs[threadIdx.y + i][threadIdx.x] = 0.0f;\n            }\n        }\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            for (int i = 0; i < SUB_TILE; ++i) {\n                for (int j = 0; j < SUB_TILE; ++j) {\n                    accum[i][j] += As[threadIdx.y + i][k] * Bs[k][threadIdx.x + j];\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    for (int i = 0; i < SUB_TILE; ++i) {\n        for (int j = 0; j < SUB_TILE; ++j) {\n            int row = warp_row * TILE_SIZE + threadIdx.y + i;\n            int col = warp_col * TILE_SIZE + threadIdx.x + j;\n            if (row < M && col < N) {\n                C[row * N + col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(8, 32);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with register tiling and cooperative fetching to maximize memory throughput and arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x;\n    const int warp_id = warp_row * blockDim.x + warp_col;\n    const int lane_id = warp_id % WARP_SIZE;\n    \n    const int row = blockIdx.y * (blockDim.y * TILE_SIZE) + warp_row * TILE_SIZE;\n    const int col = blockIdx.x * (blockDim.x * TILE_SIZE) + warp_col * TILE_SIZE;\n    \n    float reg_a[TILE_SIZE][TILE_SIZE] = {0.0f};\n    float reg_b[TILE_SIZE][TILE_SIZE] = {0.0f};\n    float reg_c[TILE_SIZE][TILE_SIZE] = {0.0f};\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            int a_row = row + i;\n            int a_col = tiled_k + lane_id;\n            if (a_row < M && a_col < K) {\n                reg_a[i][lane_id] = A[a_row * K + a_col];\n            }\n            \n            int b_row = tiled_k + lane_id;\n            int b_col = col + i;\n            if (b_row < K && b_col < N) {\n                reg_b[lane_id][i] = B[b_row * N + b_col];\n            }\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            #pragma unroll\n            for (int j = 0; j < TILE_SIZE; ++j) {\n                reg_c[i][j] += reg_a[i][lane_id] * reg_b[lane_id][j];\n            }\n        }\n    }\n    \n    #pragma unroll\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        #pragma unroll\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int c_row = row + i;\n            int c_col = col + j;\n            if (c_row < M && c_col < N) {\n                C[c_row * N + c_col] = reg_c[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(4, 4); // 4 warps per block (128 threads)\n    dim3 grid((N + (block.x * TILE_SIZE) - 1) / (block.x * TILE_SIZE),\n             (M + (block.y * TILE_SIZE) - 1) / (block.y * TILE_SIZE));\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with register tiling and cooperative fetching to maximize memory throughput and arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define THREADS_PER_BLOCK 256\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    const int warp_row = warp_id / (TILE_SIZE / WARP_SIZE);\n    const int warp_col = warp_id % (TILE_SIZE / WARP_SIZE);\n\n    const int global_row = blockIdx.y * TILE_SIZE + warp_row * WARP_SIZE + lane_id;\n    const int global_col = blockIdx.x * TILE_SIZE + warp_col * WARP_SIZE + lane_id;\n\n    float reg_A[TILE_SIZE / WARP_SIZE];\n    float reg_B[TILE_SIZE / WARP_SIZE];\n    float accum[TILE_SIZE / WARP_SIZE][TILE_SIZE / WARP_SIZE] = {0.0f};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        for (int i = 0; i < TILE_SIZE / WARP_SIZE; ++i) {\n            int load_row = global_row;\n            int load_col = tiled_k + lane_id + i * WARP_SIZE;\n            if (load_row < M && load_col < K) {\n                reg_A[i] = A[load_row * K + load_col];\n            } else {\n                reg_A[i] = 0.0f;\n            }\n\n            load_row = tiled_k + lane_id + i * WARP_SIZE;\n            load_col = global_col;\n            if (load_row < K && load_col < N) {\n                reg_B[i] = B[load_row * N + load_col];\n            } else {\n                reg_B[i] = 0.0f;\n            }\n        }\n\n        for (int i = 0; i < TILE_SIZE / WARP_SIZE; ++i) {\n            for (int j = 0; j < TILE_SIZE / WARP_SIZE; ++j) {\n                accum[i][j] += reg_A[i] * reg_B[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < TILE_SIZE / WARP_SIZE; ++i) {\n        for (int j = 0; j < TILE_SIZE / WARP_SIZE; ++j) {\n            int out_row = global_row + i * WARP_SIZE;\n            int out_col = global_col + j * WARP_SIZE;\n            if (out_row < M && out_col < N) {\n                C[out_row * N + out_col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(THREADS_PER_BLOCK);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with register tiling and cooperative fetching to maximize memory bandwidth utilization and minimize shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define BLOCK_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = threadIdx.y / (WARP_SIZE / BLOCK_SIZE);\n    const int warp_col = threadIdx.x / (WARP_SIZE / BLOCK_SIZE);\n    const int lane_row = threadIdx.y % (WARP_SIZE / BLOCK_SIZE);\n    const int lane_col = threadIdx.x % (WARP_SIZE / BLOCK_SIZE);\n\n    const int row = blockIdx.y * BLOCK_SIZE + warp_row * (BLOCK_SIZE / 2) + lane_row;\n    const int col = blockIdx.x * BLOCK_SIZE + warp_col * (BLOCK_SIZE / 2) + lane_col;\n\n    float accum[2][2] = {0};\n\n    for (int t = 0; t < K; t += TILE_SIZE) {\n        float a_tile[2];\n        float b_tile[2];\n\n        for (int i = 0; i < 2; ++i) {\n            int a_row = row + i * (BLOCK_SIZE / 2);\n            int a_col = t + lane_col;\n            a_tile[i] = (a_row < M && a_col < K) ? A[a_row * K + a_col] : 0.0f;\n\n            int b_row = t + lane_row;\n            int b_col = col + i * (BLOCK_SIZE / 2);\n            b_tile[i] = (b_row < K && b_col < N) ? B[b_row * N + b_col] : 0.0f;\n        }\n\n        for (int i = 0; i < 2; ++i) {\n            for (int j = 0; j < 2; ++j) {\n                accum[i][j] += a_tile[i] * b_tile[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < 2; ++i) {\n        for (int j = 0; j < 2; ++j) {\n            int c_row = row + i * (BLOCK_SIZE / 2);\n            int c_col = col + j * (BLOCK_SIZE / 2);\n            if (c_row < M && c_col < N) {\n                C[c_row * N + c_col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{New implementation using warp-level matrix multiply-accumulate operations with shared memory tiling and double buffering to hide memory latency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int warp_id = threadIdx.y;\n    int lane_id = threadIdx.x;\n    int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n\n    float accum[TILE_SIZE / WARPS_PER_BLOCK] = {0.0f};\n    int stage = 0;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Double buffering: load next tile while computing current\n        int a_col = tiled_k + lane_id;\n        int b_row = tiled_k + lane_id;\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            int a_row = row + i;\n            if (a_row < M && a_col < K) {\n                As[stage ^ 1][warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + i][lane_id] = A[a_row * K + a_col];\n            } else {\n                As[stage ^ 1][warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + i][lane_id] = 0.0f;\n            }\n            \n            if (b_row < K && col + i < N) {\n                Bs[stage ^ 1][lane_id][warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + i] = B[b_row * N + col + i];\n            } else {\n                Bs[stage ^ 1][lane_id][warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + i] = 0.0f;\n            }\n        }\n        __syncthreads();\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            #pragma unroll\n            for (int j = 0; j < TILE_SIZE / WARPS_PER_BLOCK; ++j) {\n                accum[j] += As[stage][warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + j][i] * Bs[stage][i][lane_id];\n            }\n        }\n        stage ^= 1;\n        __syncthreads();\n    }\n\n    #pragma unroll\n    for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n        if (row + i < M && col < N) {\n            C[(row + i) * N + col] = accum[i];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(THREADS_PER_WARP, WARPS_PER_BLOCK);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{New implementation using a different tiling approach with thread coarsening to process multiple elements per thread and reduce the number of blocks needed.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define COARSE_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = (blockIdx.y * blockDim.y + threadIdx.y) * COARSE_FACTOR;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float values[COARSE_FACTOR] = {0.0f};\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load A tiles (coalesced)\n        for (int c = 0; c < COARSE_FACTOR; ++c) {\n            int a_col = tiled_k + threadIdx.x;\n            if (row + c < M && a_col < K) {\n                As[threadIdx.y * COARSE_FACTOR + c][threadIdx.x] = A[(row + c) * K + a_col];\n            } else {\n                As[threadIdx.y * COARSE_FACTOR + c][threadIdx.x] = 0.0f;\n            }\n        }\n        \n        // Load B tile (coalesced)\n        int b_row = tiled_k + threadIdx.y;\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial results\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            for (int c = 0; c < COARSE_FACTOR; ++c) {\n                values[c] += As[threadIdx.y * COARSE_FACTOR + c][k] * Bs[k][threadIdx.x];\n            }\n        }\n        \n        __syncthreads();\n    }\n    \n    // Store results\n    for (int c = 0; c < COARSE_FACTOR; ++c) {\n        if (row + c < M && col < N) {\n            C[(row + c) * N + col] = values[c];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    \n    torch::Tensor C = torch::zeros({M, N}, A.options());\n    \n    dim3 block(TILE_SIZE, TILE_SIZE / COARSE_FACTOR);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y * COARSE_FACTOR - 1) / (block.y * COARSE_FACTOR));\n    \n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 32x32 thread blocks and double buffering to hide memory latency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int a_col = tiledK + threadIdx.x;\n        int b_row = tiledK + threadIdx.y;\n\n        // Load next tile while computing current tile (double buffering)\n        int next_t = t + 1;\n        int next_tiledK = next_t * TILE_SIZE;\n        int next_a_col = next_tiledK + threadIdx.x;\n        int next_b_row = next_tiledK + threadIdx.y;\n\n        if (t < (K + TILE_SIZE - 1) / TILE_SIZE - 1) {\n            As[(t+1)%2][threadIdx.y][threadIdx.x] = (row < M && next_a_col < K) ? A[row * K + next_a_col] : 0.0f;\n            Bs[(t+1)%2][threadIdx.y][threadIdx.x] = (next_b_row < K && col < N) ? B[next_b_row * N + col] : 0.0f;\n        }\n\n        if (t == 0) {\n            As[t%2][threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n            Bs[t%2][threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[t%2][threadIdx.y][i] * Bs[t%2][i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.768188810348511
    },
    {
        "sample_order": 16,
        "algorithm": "{New implementation using a larger tile size (32x32) with warp-level optimizations and double buffering to better utilize the RTX 4090's resources.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    int load_stage = 0;\n    int compute_stage = 0;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        \n        // Double buffering: load next tile while computing current\n        As[load_stage][threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[load_stage][threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[compute_stage][threadIdx.y][i] * Bs[compute_stage][i][threadIdx.x];\n        }\n        __syncthreads();\n        \n        // Swap stages\n        compute_stage = load_stage;\n        load_stage = 1 - load_stage;\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimized implementation using tiled matrix multiplication with shared memory and loop unrolling to reduce instruction overhead and improve memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1]; // +1 for bank conflict avoidance\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < K; t += TILE_SIZE * UNROLL_FACTOR) {\n        #pragma unroll\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            int tiled_k = t + u * TILE_SIZE;\n            \n            if (tiled_k + threadIdx.x < K && row < M) {\n                As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            if (tiled_k + threadIdx.y < K && col < N) {\n                Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            __syncthreads();\n\n            #pragma unroll\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n            }\n\n            __syncthreads();\n        }\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimized implementation using tiled matrix multiplication with shared memory and loop unrolling to reduce instruction overhead and improve memory access efficiency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n            value += As[threadIdx.y][i+1] * Bs[i+1][threadIdx.x];\n            value += As[threadIdx.y][i+2] * Bs[i+2][threadIdx.x];\n            value += As[threadIdx.y][i+3] * Bs[i+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimized implementation using shared memory with double buffering to overlap memory transfers and computation, using 32x32 thread blocks for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Load next tile while computing current tile (double buffering)\n        As[load_phase][threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[load_phase][threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n        __syncthreads();\n\n        if (t > 0) {\n            // Compute previous tile\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                value += As[1-load_phase][threadIdx.y][i] * Bs[1-load_phase][i][threadIdx.x];\n            }\n        }\n        load_phase = 1 - load_phase;\n        __syncthreads();\n    }\n\n    // Compute last tile\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        value += As[1-load_phase][threadIdx.y][i] * Bs[1-load_phase][i][threadIdx.x];\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.699388837814331
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimized implementation using tiled matrix multiplication with shared memory and loop unrolling to reduce instruction overhead and improve memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1]; // +1 for bank conflict avoidance\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < K; t += TILE_SIZE * UNROLL_FACTOR) {\n        #pragma unroll\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            int tiled_k = t + u * TILE_SIZE;\n            int a_col = tiled_k + threadIdx.x;\n            int b_row = tiled_k + threadIdx.y;\n\n            if (row < M && a_col < K) {\n                As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            if (b_row < K && col < N) {\n                Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            __syncthreads();\n\n            #pragma unroll\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n            }\n\n            __syncthreads();\n        }\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 32x32 thread blocks and proper memory coalescing, doubling the tile size for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.678044819831848
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 32x32 thread blocks and proper memory coalescing, doubling the tile size for better utilization of GPU resources.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.675856041908264
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 32x32 thread blocks and proper memory coalescing, increasing tile size for better memory reuse.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.676352071762085
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimized implementation using tiled matrix multiplication with shared memory and increased tile size (32x32) to better utilize GPU resources and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.6775488376617433
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with 32x8 thread blocks and register tiling for better utilization of Tensor Cores on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x;\n    \n    const int row = blockIdx.y * WARP_SIZE + warp_row;\n    const int col = blockIdx.x * TILE_SIZE * WARP_SIZE + warp_col * TILE_SIZE;\n    \n    float accum[TILE_SIZE] = {0.0f};\n    \n    for (int t = 0; t < K; t += WARP_SIZE) {\n        float a_frag = (row < M && t + warp_col < K) ? A[row * K + t + warp_col] : 0.0f;\n        \n        float b_frag[TILE_SIZE];\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            b_frag[i] = (t + warp_row < K && col + i < N) ? B[(t + warp_row) * N + col + i] : 0.0f;\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            accum[i] += a_frag * b_frag[i];\n        }\n    }\n    \n    if (row < M) {\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            if (col + i < N) {\n                C[row * N + col + i] = accum[i];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE, WARP_SIZE);\n    dim3 grid((N + (TILE_SIZE * WARP_SIZE) - 1) / (TILE_SIZE * WARP_SIZE), \n              (M + WARP_SIZE - 1) / WARP_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with 32x8 thread blocks and register tiling to reduce shared memory usage and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x % (WARP_SIZE / 4);\n    const int warp_id = threadIdx.x / (WARP_SIZE / 4);\n\n    const int row = blockIdx.y * (WARP_SIZE * 4) + warp_id * WARP_SIZE + warp_row;\n    const int col = blockIdx.x * (WARP_SIZE / 4) + warp_col;\n\n    float accum[4][4] = {0.0f};\n\n    for (int t = 0; t < K; t += TILE_K) {\n        float a_frag[4];\n        float b_frag[4];\n\n        for (int i = 0; i < TILE_K; ++i) {\n            int k = t + i;\n            if (k < K) {\n                a_frag[i] = A[row * K + k];\n                b_frag[i] = B[k * N + col];\n            } else {\n                a_frag[i] = 0.0f;\n                b_frag[i] = 0.0f;\n            }\n        }\n\n        for (int i = 0; i < 4; ++i) {\n            for (int j = 0; j < 4; ++j) {\n                for (int k = 0; k < TILE_K; ++k) {\n                    accum[i][j] += a_frag[k] * b_frag[k];\n                }\n            }\n        }\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = accum[0][0];\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE, 4);\n    dim3 grid((N + (WARP_SIZE / 4) - 1) / (WARP_SIZE / 4), \n             (M + (WARP_SIZE * 4) - 1) / (WARP_SIZE * 4));\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with 32x8 thread blocks and register tiling to reduce shared memory usage and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define THREADS_PER_ROW 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int row = blockIdx.y * blockDim.y + threadIdx.y;\n    const int col = blockIdx.x * (WARP_SIZE * THREADS_PER_ROW) + threadIdx.x;\n    \n    float accum[THREADS_PER_ROW] = {0.0f};\n\n    if (row < M) {\n        for (int k = 0; k < K; ++k) {\n            float a_val = A[row * K + k];\n            #pragma unroll\n            for (int t = 0; t < THREADS_PER_ROW; ++t) {\n                int c_col = col + t * WARP_SIZE;\n                if (c_col < N) {\n                    accum[t] += a_val * B[k * N + c_col];\n                }\n            }\n        }\n    }\n\n    #pragma unroll\n    for (int t = 0; t < THREADS_PER_ROW; ++t) {\n        int c_col = col + t * WARP_SIZE;\n        if (row < M && c_col < N) {\n            C[row * N + c_col] = accum[t];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE, THREADS_PER_ROW);\n    dim3 grid((N + WARP_SIZE * THREADS_PER_ROW - 1) / (WARP_SIZE * THREADS_PER_ROW), (M + THREADS_PER_ROW - 1) / THREADS_PER_ROW);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with 32x8 thread blocks and register tiling to reduce shared memory usage and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define BLOCK_ROWS 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int row = blockIdx.y * BLOCK_ROWS + threadIdx.y;\n    const int col = (blockIdx.x * WARP_SIZE) + (threadIdx.x % WARP_SIZE);\n    \n    float value = 0.0f;\n    \n    if (row < M && col < N) {\n        for (int i = 0; i < K; ++i) {\n            value += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE, BLOCK_ROWS);\n    dim3 grid((N + WARP_SIZE - 1) / WARP_SIZE, (M + BLOCK_ROWS - 1) / BLOCK_ROWS);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 32x32 thread blocks and double buffering to hide memory latency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Load next tile while computing current tile\n        int next_t = t + 1;\n        int next_tiled_k = next_t * TILE_SIZE;\n        int next_a_col = next_tiled_k + threadIdx.x;\n        int next_b_row = next_tiled_k + threadIdx.y;\n\n        if (t < (K + TILE_SIZE - 1) / TILE_SIZE - 1) {\n            As[(t+1)%2][threadIdx.y][threadIdx.x] = (row < M && next_a_col < K) ? A[row * K + next_a_col] : 0.0f;\n            Bs[(t+1)%2][threadIdx.y][threadIdx.x] = (next_b_row < K && col < N) ? B[next_b_row * N + col] : 0.0f;\n        }\n\n        if (t == 0) {\n            As[t%2][threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n            Bs[t%2][threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[t%2][threadIdx.y][i] * Bs[t%2][i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.7692064523696898
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 32x32 thread blocks and double buffering to hide memory latency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        int buf_idx = t % 2;\n        \n        if (row < M && a_col < K) {\n            As[buf_idx][threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[buf_idx][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[buf_idx][threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[buf_idx][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[1-buf_idx][threadIdx.y][i] * Bs[1-buf_idx][i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{New implementation using warp-level matrix operations with 32x32 tiles and warp shuffles for reduced shared memory usage.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n\n    int warp_id = threadIdx.y / 32;\n    int lane_id = threadIdx.y % 32;\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[warp_id][threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[warp_id][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[warp_id][threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[warp_id][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[warp_id][threadIdx.y][i] * Bs[warp_id][i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimized implementation using shared memory with double buffering to overlap memory transfers with computation, using 32x32 thread blocks for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    int loadA = threadIdx.x;\n    int loadB = threadIdx.y;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        \n        // Load next tile while computing current tile (double buffering)\n        int a_col = tiledK + loadA;\n        int b_row = tiledK + loadB;\n        \n        As[(t+1)%2][threadIdx.y][loadA] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[(t+1)%2][loadB][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n        \n        __syncthreads();\n        \n        // Compute current tile\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[t%2][threadIdx.y][i] * Bs[t%2][i][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimized implementation using shared memory with double buffering to overlap memory transfers and computation, using 32x32 thread blocks for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Preload next tile while computing current tile\n        As[load_phase][threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[load_phase][threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[compute_phase][threadIdx.y][i] * Bs[compute_phase][i][threadIdx.x];\n        }\n        __syncthreads();\n\n        // Swap phases\n        int temp = load_phase;\n        load_phase = compute_phase;\n        compute_phase = temp;\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimized implementation using tiled matrix multiplication with shared memory and loop unrolling to reduce instruction overhead and improve memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1]; // +1 to avoid bank conflicts\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < K; t += TILE_SIZE * UNROLL_FACTOR) {\n        #pragma unroll\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            int tiled_k = t + u * TILE_SIZE;\n            int a_col = tiled_k + threadIdx.x;\n            int b_row = tiled_k + threadIdx.y;\n\n            if (row < M && a_col < K) {\n                As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            if (b_row < K && col < N) {\n                Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            __syncthreads();\n\n            #pragma unroll\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n            }\n\n            __syncthreads();\n        }\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the matrix multiplication kernel using shared memory with double buffering to overlap memory transfers with computation, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    int load_phase = 0;\n    int compute_phase = 1;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        \n        // Preload next tile while computing current tile\n        if (row < M && a_col + TILE_SIZE < K) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * K + a_col + TILE_SIZE];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (b_row + TILE_SIZE < K && col < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[(b_row + TILE_SIZE) * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n        }\n        \n        __syncthreads();\n        \n        // Swap phases\n        int temp = load_phase;\n        load_phase = compute_phase;\n        compute_phase = temp;\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    \n    torch::Tensor C = torch::zeros({M, N}, A.options());\n    \n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n    \n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimized implementation using tiled matrix multiplication with shared memory and loop unrolling to improve instruction-level parallelism and reduce synchronization overhead.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n            value += As[threadIdx.y][i+1] * Bs[i+1][threadIdx.x];\n            value += As[threadIdx.y][i+2] * Bs[i+2][threadIdx.x];\n            value += As[threadIdx.y][i+3] * Bs[i+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimized implementation using tiled matrix multiplication with shared memory and increased tile size (32x32) to better utilize GPU resources and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the matrix multiplication kernel using shared memory with larger tile size (32x32) for better memory access patterns and increased thread utilization, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        \n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    \n    torch::Tensor C = torch::zeros({M, N}, A.options());\n    \n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n    \n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimized implementation using shared memory with increased tile size (32x32) for better memory access efficiency and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int a_col = tiledK + threadIdx.x;\n        int b_row = tiledK + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.6770240306854247
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 32x32 thread blocks and proper memory coalescing, increasing the tile size for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.6773568391799927
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with 32x8 thread blocks and register tiling for improved memory efficiency and reduced shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x % WARP_SIZE;\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    \n    const int row = blockIdx.y * (blockDim.y * 2) + threadIdx.y;\n    const int col = blockIdx.x * (blockDim.x / WARP_SIZE * 4) + warp_id * 4 + (warp_col / 8);\n    \n    float accum[4][2] = {0.0f};\n    float a_frag[2][TILE_K];\n    float b_frag[2][TILE_K];\n    \n    for (int t = 0; t < K; t += TILE_K) {\n        for (int i = 0; i < TILE_K; ++i) {\n            int a_row = row + (i < TILE_K/2 ? 0 : blockDim.y);\n            int a_col = t + i;\n            int b_row = t + i;\n            int b_col = col * 2 + (i < TILE_K/2 ? 0 : 1);\n            \n            a_frag[0][i] = (a_row < M && a_col < K) ? A[a_row * K + a_col] : 0.0f;\n            b_frag[0][i] = (b_row < K && b_col < N) ? B[b_row * N + b_col] : 0.0f;\n            \n            a_frag[1][i] = (a_row + blockDim.y < M && a_col < K) ? A[(a_row + blockDim.y) * K + a_col] : 0.0f;\n            b_frag[1][i] = (b_row < K && b_col + 1 < N) ? B[b_row * N + b_col + 1] : 0.0f;\n        }\n        \n        for (int i = 0; i < TILE_K; ++i) {\n            for (int j = 0; j < 2; ++j) {\n                for (int k = 0; k < 4; ++k) {\n                    accum[k][j] += a_frag[j][i] * b_frag[0][i];\n                }\n            }\n        }\n    }\n    \n    if (row < M && col * 2 < N) {\n        C[row * N + col * 2] = accum[0][0];\n        if (col * 2 + 1 < N) {\n            C[row * N + col * 2 + 1] = accum[0][1];\n        }\n        if (row + blockDim.y < M) {\n            C[(row + blockDim.y) * N + col * 2] = accum[1][0];\n            if (col * 2 + 1 < N) {\n                C[(row + blockDim.y) * N + col * 2 + 1] = accum[1][1];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(128, 8); // 4 warps x 32 threads, 8 rows\n    dim3 grid((N + 7) / 8, (M + 15) / 16);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize matrix multiplication using register tiling with warp-level operations and shared memory to maximize memory throughput and minimize bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    \n    int warp_id = threadIdx.y;\n    int lane_id = threadIdx.x;\n    int row = blockIdx.y * TILE_SIZE * WARPS_PER_BLOCK + warp_id * TILE_SIZE;\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float accum[TILE_SIZE] = {0.0f};\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load A tile\n        for (int i = 0; i < TILE_SIZE; i += THREADS_PER_WARP) {\n            int load_row = row + i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK));\n            int load_col = tiled_k + (lane_id % (TILE_SIZE / WARPS_PER_BLOCK));\n            if (load_row < M && load_col < K) {\n                As[warp_id][i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = A[load_row * K + load_col];\n            } else {\n                As[warp_id][i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = 0.0f;\n            }\n        }\n        \n        // Load B tile\n        for (int j = 0; j < TILE_SIZE; j += THREADS_PER_WARP) {\n            int load_row = tiled_k + j + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK));\n            int load_col = col + (lane_id % (TILE_SIZE / WARPS_PER_BLOCK));\n            if (load_row < K && load_col < N) {\n                Bs[warp_id][j + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = B[load_row * N + load_col];\n            } else {\n                Bs[warp_id][j + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = 0.0f;\n            }\n        }\n        \n        __syncthreads();\n        \n        // Compute\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            float a = As[warp_id][threadIdx.y][k];\n            float b = Bs[warp_id][k][threadIdx.x];\n            accum[threadIdx.y] += a * b;\n        }\n        \n        __syncthreads();\n    }\n    \n    // Store results\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        int write_row = row + i;\n        if (write_row < M && col < N) {\n            C[write_row * N + col] = accum[i];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n    \n    torch::Tensor C = torch::zeros({M, N}, A.options());\n    \n    dim3 block(THREADS_PER_WARP, WARPS_PER_BLOCK);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK));\n    \n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with 32x8 thread blocks and register tiling to reduce shared memory usage and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x % (WARP_SIZE / 4);\n    const int warp_id = threadIdx.x / (WARP_SIZE / 4);\n\n    const int row = blockIdx.y * (WARP_SIZE * 4) + warp_id * WARP_SIZE + warp_row;\n    const int col = blockIdx.x * (WARP_SIZE / 4) + warp_col;\n\n    float accum[4][4] = {0.0f};\n\n    for (int t = 0; t < K; t += TILE_K) {\n        float a_frag[4];\n        float b_frag[4];\n\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            int a_row = row + i * WARP_SIZE;\n            int a_col = t + warp_col;\n            a_frag[i] = (a_row < M && a_col < K) ? A[a_row * K + a_col] : 0.0f;\n        }\n\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            int b_row = t + warp_row;\n            int b_col = col + i * (WARP_SIZE / 4);\n            b_frag[i] = (b_row < K && b_col < N) ? B[b_row * N + b_col] : 0.0f;\n        }\n\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            #pragma unroll\n            for (int j = 0; j < 4; ++j) {\n                accum[i][j] += a_frag[i] * b_frag[j];\n            }\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int c_row = row + i * WARP_SIZE;\n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            int c_col = col + j * (WARP_SIZE / 4);\n            if (c_row < M && c_col < N) {\n                C[c_row * N + c_col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(32, 8);\n    dim3 grid((N + (WARP_SIZE / 4) - 1) / (WARP_SIZE / 4), (M + (WARP_SIZE * 4) - 1) / (WARP_SIZE * 4));\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimized implementation using warp-level matrix multiplication with 32x8 tile size and register tiling to reduce shared memory usage and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_M 32\n#define TILE_N 8\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = blockIdx.y * TILE_M;\n    const int warp_col = blockIdx.x * TILE_N;\n    \n    const int lane_id = threadIdx.x % WARP_SIZE;\n    const int warp_row_in_tile = lane_id / (TILE_N / 8);\n    const int warp_col_in_tile = lane_id % (TILE_N / 8) * 8;\n    \n    float accum[TILE_M / WARP_SIZE][TILE_N / 8] = {0.0f};\n    \n    for (int k = 0; k < K; k += TILE_K) {\n        float a_frag[TILE_M / WARP_SIZE][TILE_K];\n        float b_frag[TILE_K][TILE_N / 8];\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_M / WARP_SIZE; ++i) {\n            int row = warp_row + i * WARP_SIZE + warp_row_in_tile;\n            #pragma unroll\n            for (int j = 0; j < TILE_K; ++j) {\n                int col = k + j;\n                a_frag[i][j] = (row < M && col < K) ? A[row * K + col] : 0.0f;\n            }\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_K; ++i) {\n            int row = k + i;\n            #pragma unroll\n            for (int j = 0; j < TILE_N / 8; ++j) {\n                int col = warp_col + warp_col_in_tile + j * 8;\n                b_frag[i][j] = (row < K && col < N) ? B[row * N + col] : 0.0f;\n            }\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_M / WARP_SIZE; ++i) {\n            #pragma unroll\n            for (int j = 0; j < TILE_N / 8; ++j) {\n                #pragma unroll\n                for (int t = 0; t < TILE_K; ++t) {\n                    accum[i][j] += a_frag[i][t] * b_frag[t][j];\n                }\n            }\n        }\n    }\n    \n    #pragma unroll\n    for (int i = 0; i < TILE_M / WARP_SIZE; ++i) {\n        int row = warp_row + i * WARP_SIZE + warp_row_in_tile;\n        #pragma unroll\n        for (int j = 0; j < TILE_N / 8; ++j) {\n            int col = warp_col + warp_col_in_tile + j * 8;\n            if (row < M && col < N) {\n                C[row * N + col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE);\n    dim3 grid((N + TILE_N - 1) / TILE_N, (M + TILE_M - 1) / TILE_M);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 32x32 thread blocks and double buffering to overlap memory transfers with computation.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 0;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Load next tile while computing current tile\n        As[load_phase][threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[load_phase][threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[compute_phase][threadIdx.y][i] * Bs[compute_phase][i][threadIdx.x];\n        }\n        __syncthreads();\n\n        load_phase = 1 - load_phase;\n        compute_phase = 1 - compute_phase;\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.674185633659363
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with 32x32 thread blocks and double buffering to hide memory latency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int a_col = tiledK + threadIdx.x;\n        int b_row = tiledK + threadIdx.y;\n\n        // Load next tile while computing current tile (double buffering)\n        int next_t = t + 1;\n        int next_tiledK = next_t * TILE_SIZE;\n        int next_a_col = next_tiledK + threadIdx.x;\n        int next_b_row = next_tiledK + threadIdx.y;\n\n        As[t%2][threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[t%2][threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        if (next_t < (K + TILE_SIZE - 1) / TILE_SIZE) {\n            As[(t+1)%2][threadIdx.y][threadIdx.x] = (row < M && next_a_col < K) ? A[row * K + next_a_col] : 0.0f;\n            Bs[(t+1)%2][threadIdx.y][threadIdx.x] = (next_b_row < K && col < N) ? B[next_b_row * N + col] : 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[t%2][threadIdx.y][i] * Bs[t%2][i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    torch::Tensor C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel (C = A * B)\");\n}",
        "score": -2.835228776931763
    }
]