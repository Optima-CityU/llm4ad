[
    {
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and minimizing redundant calculations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0.0f) ? x : alpha * (expf(x) - 1.0f);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;  // Reduced for better occupancy with vectorization\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.023971199989318848
    },
    {
        "algorithm": "{Optimize the ELU kernel by using vectorized memory accesses, loop unrolling, and minimizing redundant calculations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    const int stride = blockDim.x * gridDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; idx < N; idx += stride) {\n        float x = input[idx];\n        output[idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = min((N + threads - 1) / threads, 1024);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.025619200244545937
    },
    {
        "algorithm": "{Optimize the ELU kernel by using vectorized memory accesses, loop unrolling, and minimizing warp divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04129599891602993
    },
    {
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and minimizing redundant calculations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    if (idx < N) {\n        float4 in = *reinterpret_cast<float4*>(&input[idx]);\n        float4 out;\n        \n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            float x = ((float*)&in)[i];\n            ((float*)&out)[i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = out;\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;  // Reduced for better occupancy with vectorized loads\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.043785599246621135
    },
    {
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and minimizing expensive operations like expf while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename scalar_t>\n__global__ void elu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    int N,\n    float alpha) {\n    \n    constexpr int VEC_SIZE = 4;\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            scalar_t x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    constexpr int threads = 256;\n    int blocks = (N + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"elu_forward\", [&] {\n        elu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            N,\n            alpha\n        );\n    });\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04457279965281487
    }
]