[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory accesses, loop unrolling, and minimizing redundant calculations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    const int stride = blockDim.x * gridDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; idx < N; idx += stride) {\n        float x = input[idx];\n        output[idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = min((N + threads - 1) / threads, 1024);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.025619200244545937
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and minimizing warp divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.023852800391614436
    }
]