[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory accesses, loop unrolling, and minimizing redundant calculations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    const int stride = blockDim.x * gridDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; idx < N; idx += stride) {\n        float x = input[idx];\n        output[idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = min((N + threads - 1) / threads, 1024);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.025619200244545937
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the ELU kernel by using warp-level primitives for coalesced memory access and leveraging fast math operations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        float x = input[idx];\n        output[idx] = (x >= 0.0f) ? x : alpha * (__expf(x) - 1.0f);\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;  // Better occupancy\n    int blocks = (N + threads - 1) / threads;\n\n    elu_kernel<<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n        input.data_ptr<float>(), \n        output.data_ptr<float>(), \n        N, \n        alpha\n    );\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and minimizing warp divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel_optimized(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward_optimized(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;  // Reduced for better occupancy with vectorization\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel_optimized<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward_optimized\", &elu_forward_optimized, \"Optimized ELU activation forward\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and minimizing expensive operations like expf while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename scalar_t>\n__global__ void elu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    int N,\n    float alpha) {\n    \n    constexpr int VEC_SIZE = 4;\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            scalar_t x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    constexpr int threads = 256;\n    int blocks = (N + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"elu_forward\", [&] {\n        elu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            N,\n            alpha\n        );\n    });\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04457279965281487
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory accesses, loop unrolling, and minimizing warp divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04129599891602993
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and fused multiply-add operations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : __fmul_rn(alpha, __expm1f(x));\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the ELU kernel by using faster exponential approximation, loop unrolling, and increased parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__device__ __forceinline__ float fast_expf(float x) {\n    x = 1.0f + x / 1024.0f;\n    x *= x; x *= x; x *= x; x *= x;\n    x *= x; x *= x; x *= x; x *= x;\n    x *= x; x *= x;\n    return x;\n}\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        if (idx < N) {\n            float x = input[idx];\n            output[idx] = (x >= 0) ? x : alpha * (fast_expf(x) - alpha;\n            idx += blockDim.x;\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * 4 - 1) / (threads * 4);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the ELU kernel by using warp-level primitives for coalesced memory access and reducing thread divergence with predicated execution.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        float x = input[idx];\n        float exp_val = expf(x);\n        output[idx] = (x >= 0.0f) ? x : alpha * (exp_val - 1.0f);\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads - 1) / threads;\n\n    elu_kernel<<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n        input.data_ptr<float>(), \n        output.data_ptr<float>(), \n        N, \n        alpha\n    );\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and minimizing warp divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.023852800391614436
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and minimizing redundant calculations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0.0f) ? x : alpha * (expf(x) - 1.0f);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;  // Reduced for better occupancy with vectorization\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.023971199989318848
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the ELU kernel by using faster exponential approximation, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\n__forceinline__ __device__ float fast_expf(float x) {\n    x = 1.0f + x / 1024.0f;\n    x *= x; x *= x; x *= x; x *= x;\n    x *= x; x *= x; x *= x; x *= x;\n    x *= x; x *= x;\n    return x;\n}\n\n__global__ void elu_kernel_optimized(float* input, float* output, int N, float alpha) {\n    const int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int current_idx = idx + i * blockDim.x;\n        if (current_idx < N) {\n            float x = input[current_idx];\n            output[current_idx] = (x >= 0) ? x : alpha * (fast_expf(x) - 1.0f);\n        }\n    }\n}\n\ntorch::Tensor elu_forward_optimized(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * 4 - 1) / (threads * 4);\n\n    elu_kernel_optimized<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward_optimized\", &elu_forward_optimized, \"Optimized ELU activation forward\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and minimizing branch divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04185919985175133
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory access, loop unrolling, and minimizing redundant calculations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    if (idx < N) {\n        float4 in = *reinterpret_cast<float4*>(&input[idx]);\n        float4 out;\n        \n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            float x = ((float*)&in)[i];\n            ((float*)&out)[i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = out;\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;  // Reduced for better occupancy with vectorized loads\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.043785599246621135
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the ELU kernel by using warp-level programming with cooperative groups to reduce overhead and improve memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    cg::thread_block tb = cg::this_thread_block();\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(tb);\n    \n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * tile.size();\n    idx += tile.thread_rank();\n    \n    if (idx < N) {\n        float x = input[idx];\n        output[idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * 32 - 1) / (threads * 32);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the ELU kernel by using warp-level programming with cooperative groups to reduce overhead and improve memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    auto warp = cg::tiled_partition<32>(cg::this_thread_block());\n    int lane = warp.thread_rank();\n    int warp_id = (blockIdx.x * blockDim.x + threadIdx.x) / 32;\n    int warps_per_grid = (blockDim.x * gridDim.x) / 32;\n    \n    for (int idx = warp_id * 32 + lane; idx < N; idx += warps_per_grid * 32) {\n        float x = input[idx];\n        output[idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = min((N + threads - 1) / threads, 1024);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the ELU kernel by using warp-level primitives to reduce warp divergence and shared memory for cooperative data loading, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nconstexpr int TILE_SIZE = 128;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    namespace cg = cooperative_groups;\n    cg::thread_block tile = cg::this_thread_block();\n    __shared__ float smem[TILE_SIZE];\n    \n    int idx = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int tid = threadIdx.x;\n    \n    if (idx < N) {\n        smem[tid] = input[idx];\n    }\n    tile.sync();\n    \n    if (idx < N) {\n        float x = smem[tid];\n        output[idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + TILE_SIZE - 1) / TILE_SIZE;\n    \n    elu_kernel<<<blocks, TILE_SIZE>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the ELU kernel by using warp-level primitives for cooperative execution, shared memory for data reuse, and minimizing branch divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void elu_warp_optimized(float* input, float* output, int N, float alpha) {\n    extern __shared__ float smem[];\n    const int warp_size = 32;\n    int tid = threadIdx.x;\n    int warp_id = tid / warp_size;\n    int lane_id = tid % warp_size;\n    int warp_offset = blockIdx.x * (blockDim.x / warp_size) + warp_id;\n    \n    for (int i = warp_offset; i * warp_size < N; i += gridDim.x * (blockDim.x / warp_size)) {\n        int idx = i * warp_size + lane_id;\n        float x = (idx < N) ? input[idx] : 0.0f;\n        smem[lane_id] = x;\n        __syncwarp();\n        \n        float val = smem[lane_id];\n        float res = (val >= 0) ? val : alpha * (expf(val) - 1);\n        \n        if (idx < N) {\n            output[idx] = res;\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    constexpr int threads = 256;\n    int warps_per_block = threads / 32;\n    int blocks = min((N + warps_per_block * 32 - 1) / (warps_per_block * 32), 1024);\n    \n    elu_warp_optimized<<<blocks, threads, 32*sizeof(float)>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        N,\n        alpha\n    );\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the ELU kernel by processing multiple elements per thread using warp-level primitives and shared memory to reduce global memory accesses while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nconstexpr int VEC_SIZE = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    namespace cg = cooperative_groups;\n    cg::thread_block cta = cg::this_thread_block();\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cta);\n\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    float vals[VEC_SIZE];\n    \n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            vals[i] = input[idx + i];\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = vals[i];\n            output[idx + i] = (x >= 0.0f) ? x : alpha * (expf(x) - 1.0f);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04555519931018352
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the ELU kernel by using warp-level primitives and shared memory to reduce global memory accesses and improve thread utilization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void elu_kernel_warp_optimized(\n    const float* __restrict__ input,\n    float* __restrict__ output,\n    int N,\n    float alpha) {\n    \n    constexpr int WARP_SIZE = 32;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int lane = threadIdx.x % WARP_SIZE;\n    int warp_id = threadIdx.x / WARP_SIZE;\n    \n    __shared__ float smem[WARP_SIZE];\n    \n    for (; idx < N; idx += blockDim.x * gridDim.x) {\n        float x = input[idx];\n        float res = (x >= 0) ? x : alpha * (expf(x) - 1);\n        \n        // Warp-level reduction for coalesced writes\n        if (lane == 0) {\n            smem[warp_id] = res;\n        }\n        __syncthreads();\n        \n        if (warp_id == 0) {\n            output[idx] = smem[lane];\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    constexpr int threads = 256;\n    int blocks = (N + threads - 1) / threads;\n    \n    elu_kernel_warp_optimized<<<blocks, threads>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        N,\n        alpha\n    );\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the ELU kernel by processing multiple elements per thread using warp-level primitives and shared memory to reduce global memory accesses while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nconstexpr int VEC_SIZE = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    namespace cg = cooperative_groups;\n    cg::thread_block tb = cg::this_thread_block();\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(tb);\n\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    float4 in_vec;\n    float4 out_vec;\n\n    if (idx + VEC_SIZE - 1 < N) {\n        in_vec = *reinterpret_cast<float4*>(&input[idx]);\n        out_vec.x = (in_vec.x >= 0) ? in_vec.x : alpha * (expf(in_vec.x) - 1);\n        out_vec.y = (in_vec.y >= 0) ? in_vec.y : alpha * (expf(in_vec.y) - 1);\n        out_vec.z = (in_vec.z >= 0) ? in_vec.z : alpha * (expf(in_vec.z) - 1);\n        out_vec.w = (in_vec.w >= 0) ? in_vec.w : alpha * (expf(in_vec.w) - 1);\n        *reinterpret_cast<float4*>(&output[idx]) = out_vec;\n    } else {\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (idx + i < N) {\n                float x = input[idx + i];\n                output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n            }\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04353599958121777
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the ELU kernel by processing multiple elements per thread using warp-level primitives and shared memory for better memory coalescing and reduced global memory access.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    __shared__ float smem[THREADS_PER_BLOCK * VEC_SIZE];\n    \n    int tid = threadIdx.x;\n    int idx = (blockIdx.x * THREADS_PER_BLOCK + tid) * VEC_SIZE;\n    \n    if (idx < N) {\n        float4 in = *reinterpret_cast<float4*>(&input[idx]);\n        *reinterpret_cast<float4*>(&smem[tid * VEC_SIZE]) = in;\n    }\n    __syncthreads();\n    \n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        int pos = tid * VEC_SIZE + i;\n        if (idx + i < N) {\n            float x = smem[pos];\n            smem[pos] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n    __syncthreads();\n    \n    if (idx < N) {\n        float4 out = *reinterpret_cast<float4*>(&smem[tid * VEC_SIZE]);\n        *reinterpret_cast<float4*>(&output[idx]) = out;\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.02494400031864643
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the ELU kernel by using warp-level programming to reduce divergence and improve memory coalescing, while maintaining numerical accuracy through exact computation.}",
        "function": "#include <torch/extension.h>\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x);\n    int warp_id = idx / 32;\n    int lane_id = idx % 32;\n    \n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int element_idx = warp_id * 128 + lane_id + i * 32;\n        if (element_idx < N) {\n            float x = input[element_idx];\n            output[element_idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * 4 - 1) / (threads * 4);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04300479926168919
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the ELU kernel by processing multiple elements per thread using warp-level primitives and shared memory to reduce global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nconstexpr int ELEMENTS_PER_THREAD = 8;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    namespace cg = cooperative_groups;\n    cg::thread_block tile = cg::this_thread_block();\n    int tid = tile.thread_rank();\n    int stride = tile.size() * ELEMENTS_PER_THREAD;\n    int start = blockIdx.x * stride + tid * ELEMENTS_PER_THREAD;\n\n    float vals[ELEMENTS_PER_THREAD];\n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int idx = start + i;\n        if (idx < N) {\n            float x = input[idx];\n            vals[i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int idx = start + i;\n        if (idx < N) {\n            output[idx] = vals[i];\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * ELEMENTS_PER_THREAD - 1) / (threads * ELEMENTS_PER_THREAD);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04315199963748455
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the ELU kernel by processing multiple elements per thread using shared memory for coalesced global memory access and reducing thread divergence.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int ELEMENTS_PER_THREAD = 8;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    __shared__ float smem[ELEMENTS_PER_THREAD * 32];\n    \n    int tid = threadIdx.x;\n    int idx = blockIdx.x * (blockDim.x * ELEMENTS_PER_THREAD) + tid;\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int load_idx = idx + i * blockDim.x;\n        if (load_idx < N) {\n            smem[tid + i * blockDim.x] = input[load_idx];\n        }\n    }\n    __syncthreads();\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int elem_idx = tid + i * blockDim.x;\n        if (idx + i * blockDim.x < N) {\n            float x = smem[elem_idx];\n            smem[elem_idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n    __syncthreads();\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int store_idx = idx + i * blockDim.x;\n        if (store_idx < N) {\n            output[store_idx] = smem[tid + i * blockDim.x];\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 32;\n    int blocks = (N + threads * ELEMENTS_PER_THREAD - 1) / (threads * ELEMENTS_PER_THREAD);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04753279983997345
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the ELU kernel by processing multiple elements per thread using warp-level primitives and shared memory to reduce global memory accesses while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nconstexpr int ELEMENTS_PER_THREAD = 8;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    namespace cg = cooperative_groups;\n    cg::thread_block tile = cg::this_thread_block();\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x * ELEMENTS_PER_THREAD;\n    \n    for (int i = tid * ELEMENTS_PER_THREAD; i < N; i += stride) {\n        #pragma unroll\n        for (int j = 0; j < ELEMENTS_PER_THREAD; ++j) {\n            int idx = i + j;\n            if (idx < N) {\n                float x = input[idx];\n                output[idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n            }\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * ELEMENTS_PER_THREAD - 1) / (THREADS_PER_BLOCK * ELEMENTS_PER_THREAD);\n    blocks = min(blocks, 128);\n    \n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.02434880044311285
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the ELU kernel by using larger vector size (8 elements per thread), increased thread block size (512 threads), and more aggressive loop unrolling while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 8;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 512;\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04456639960408211
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the ELU kernel by increasing the vector size to 8, using 128 threads per block, and maintaining numerical accuracy with proper boundary checks.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 8;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 128;\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.044339199736714366
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the ELU kernel by using larger vector size (8 elements per thread), increased thread block size (512 threads), and full loop unrolling while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 8;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    #pragma unroll\n    for (int i = 0; i < VEC_SIZE; ++i) {\n        if (idx + i < N) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 512;\n    int blocks = (N + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04332799948751927
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the ELU kernel by processing 8 elements per thread using vectorized loads/stores and warp-level operations while maintaining correctness and avoiding shared memory overhead.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 8;\nconstexpr int THREADS_PER_BLOCK = 128;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int tid = threadIdx.x;\n    int idx = (blockIdx.x * THREADS_PER_BLOCK + tid) * VEC_SIZE;\n    \n    if (idx + VEC_SIZE <= N) {\n        float4 in1 = *reinterpret_cast<float4*>(&input[idx]);\n        float4 in2 = *reinterpret_cast<float4*>(&input[idx + 4]);\n        \n        float4 out1, out2;\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            float x = reinterpret_cast<float*>(&in1)[i];\n            reinterpret_cast<float*>(&out1)[i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            float x = reinterpret_cast<float*>(&in2)[i];\n            reinterpret_cast<float*>(&out2)[i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = out1;\n        *reinterpret_cast<float4*>(&output[idx + 4]) = out2;\n    } else {\n        for (int i = 0; i < VEC_SIZE && idx + i < N; ++i) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04224639981985092
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the ELU kernel by using warp-level cooperative groups for efficient SIMD execution and reduced divergence, while processing multiple elements per thread with register-level operations to minimize memory traffic.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\nconstexpr int ELEMENTS_PER_THREAD = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cg::this_thread_block());\n    int thread_idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int global_idx = thread_idx * ELEMENTS_PER_THREAD;\n    \n    float reg[ELEMENTS_PER_THREAD];\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        if (global_idx + i < N) {\n            reg[i] = input[global_idx + i];\n        }\n    }\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        if (global_idx + i < N) {\n            float x = reg[i];\n            reg[i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        if (global_idx + i < N) {\n            output[global_idx + i] = reg[i];\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads * ELEMENTS_PER_THREAD - 1) / (threads * ELEMENTS_PER_THREAD);\n    \n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the ELU kernel by using warp-level cooperative groups for efficient execution and reducing thread divergence, while processing multiple elements per thread with direct global memory access.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nconstexpr int ELEMENTS_PER_THREAD = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    namespace cg = cooperative_groups;\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cg::this_thread_block());\n    \n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int idx = tid * ELEMENTS_PER_THREAD;\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int pos = idx + i;\n        if (pos < N) {\n            float x = input[pos];\n            output[pos] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * ELEMENTS_PER_THREAD - 1) / (THREADS_PER_BLOCK * ELEMENTS_PER_THREAD);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the ELU kernel by utilizing warp-level parallelism with cooperative groups, register caching, and predicated execution to maximize instruction-level parallelism while minimizing memory transactions.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\n\nconstexpr int WARP_SIZE = 32;\nconstexpr int ELEMENTS_PER_THREAD = 4;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    cg::thread_block tb = cg::this_thread_block();\n    cg::thread_block_tile<WARP_SIZE> warp = cg::tiled_partition<WARP_SIZE>(tb);\n    \n    int lane = warp.thread_rank();\n    int warp_offset = (blockIdx.x * blockDim.x + warp.meta_group_rank() * WARP_SIZE) * ELEMENTS_PER_THREAD;\n    \n    float reg_cache[ELEMENTS_PER_THREAD];\n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int idx = warp_offset + lane + i * WARP_SIZE;\n        reg_cache[i] = (idx < N) ? input[idx] : 0.0f;\n    }\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        float x = reg_cache[i];\n        reg_cache[i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n    }\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int idx = warp_offset + lane + i * WARP_SIZE;\n        if (idx < N) output[idx] = reg_cache[i];\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int warps_per_block = 4;\n    int threads = warps_per_block * WARP_SIZE;\n    int blocks = min((N + threads * ELEMENTS_PER_THREAD - 1) / (threads * ELEMENTS_PER_THREAD), 256);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the ELU kernel by using warp-level cooperative groups for efficient SIMD execution and register-based computation to minimize shared memory overhead while maintaining coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\n\nconstexpr int WARP_SIZE = 32;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    auto warp = cg::tiled_partition<WARP_SIZE>(cg::this_thread_block());\n    int lane = warp.thread_rank();\n    int warp_id = (blockIdx.x * blockDim.x + threadIdx.x) / WARP_SIZE;\n    int elements_per_warp = (N + gridDim.x * blockDim.x / WARP_SIZE - 1) / (gridDim.x * blockDim.x / WARP_SIZE);\n    int start = warp_id * elements_per_warp;\n    int end = min(start + elements_per_warp, N);\n\n    for (int i = start + lane; i < end; i += WARP_SIZE) {\n        float x = input[i];\n        output[i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = min((N + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK, 1024);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the ELU kernel by using warp-level operations and register-level caching to process multiple elements per thread without shared memory overhead.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x * VEC_SIZE;\n    \n    for (int i = tid * VEC_SIZE; i < N; i += stride) {\n        float4 in;\n        if (i + VEC_SIZE <= N) {\n            in = *reinterpret_cast<float4*>(&input[i]);\n        } else {\n            // Handle remainder elements\n            for (int j = 0; j < VEC_SIZE; ++j) {\n                ((float*)&in)[j] = (i + j < N) ? input[i + j] : 0.0f;\n            }\n        }\n        \n        float4 out;\n        #pragma unroll\n        for (int j = 0; j < VEC_SIZE; ++j) {\n            float x = ((float*)&in)[j];\n            ((float*)&out)[j] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n        \n        if (i + VEC_SIZE <= N) {\n            *reinterpret_cast<float4*>(&output[i]) = out;\n        } else {\n            for (int j = 0; j < VEC_SIZE; ++j) {\n                if (i + j < N) output[i + j] = ((float*)&out)[j];\n            }\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n    \n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04451839961111546
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the ELU kernel by using vectorized loads/stores and warp-level operations to maximize memory throughput while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    \n    if (idx < N) {\n        float4 in = *reinterpret_cast<float4*>(&input[idx]);\n        float4 out;\n        out.x = (in.x >= 0) ? in.x : alpha * (expf(in.x) - 1);\n        out.y = (in.y >= 0) ? in.y : alpha * (expf(in.y) - 1);\n        out.z = (in.z >= 0) ? in.z : alpha * (expf(in.z) - 1);\n        out.w = (in.w >= 0) ? in.w : alpha * (expf(in.w) - 1);\n        *reinterpret_cast<float4*>(&output[idx]) = out;\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.044111999496817586
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the ELU kernel by combining warp-level reduction with cooperative groups and processing multiple elements per thread using register-level caching to minimize global memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n#include <cooperative_groups/reduce.h>\n\nconstexpr int ELEMENTS_PER_THREAD = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    namespace cg = cooperative_groups;\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cg::this_thread_block());\n    \n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int global_stride = blockDim.x * gridDim.x;\n    \n    float reg_cache[ELEMENTS_PER_THREAD];\n    int indices[ELEMENTS_PER_THREAD];\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; i++) {\n        int idx = tid + i * global_stride;\n        indices[i] = idx;\n        if (idx < N) {\n            float x = input[idx];\n            reg_cache[i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; i++) {\n        if (indices[i] < N) {\n            output[indices[i]] = reg_cache[i];\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * ELEMENTS_PER_THREAD - 1) / (THREADS_PER_BLOCK * ELEMENTS_PER_THREAD);\n    \n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04491840004920959
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the ELU kernel by using warp-level parallelism with cooperative groups and processing multiple elements per thread through register caching while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nconstexpr int ELEMENTS_PER_THREAD = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    namespace cg = cooperative_groups;\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cg::this_thread_block());\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int global_idx = tid * ELEMENTS_PER_THREAD;\n    \n    float reg_cache[ELEMENTS_PER_THREAD];\n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int idx = global_idx + i;\n        if (idx < N) {\n            reg_cache[i] = input[idx];\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int idx = global_idx + i;\n        if (idx < N) {\n            float x = reg_cache[i];\n            output[idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * ELEMENTS_PER_THREAD - 1) / (THREADS_PER_BLOCK * ELEMENTS_PER_THREAD);\n    \n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.043398399651050565
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the ELU kernel by using warp-level vectorized loads/stores and processing multiple elements per thread without shared memory to reduce overhead and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    \n    if (idx + VEC_SIZE - 1 < N) {\n        float4 in = *reinterpret_cast<float4*>(&input[idx]);\n        float4 out;\n        out.x = (in.x >= 0) ? in.x : alpha * (expf(in.x) - 1);\n        out.y = (in.y >= 0) ? in.y : alpha * (expf(in.y) - 1);\n        out.z = (in.z >= 0) ? in.z : alpha * (expf(in.z) - 1);\n        out.w = (in.w >= 0) ? in.w : alpha * (expf(in.w) - 1);\n        *reinterpret_cast<float4*>(&output[idx]) = out;\n    } else {\n        for (int i = 0; i < VEC_SIZE && idx + i < N; ++i) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.044873600080609324
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the ELU kernel by processing multiple elements per thread using vectorized loads/stores and warp-level operations while eliminating shared memory usage for reduced latency.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    \n    if (idx + VEC_SIZE - 1 < N) {\n        float4 in = *reinterpret_cast<float4*>(&input[idx]);\n        float4 out;\n        out.x = (in.x >= 0) ? in.x : alpha * (expf(in.x) - 1);\n        out.y = (in.y >= 0) ? in.y : alpha * (expf(in.y) - 1);\n        out.z = (in.z >= 0) ? in.z : alpha * (expf(in.z) - 1);\n        out.w = (in.w >= 0) ? in.w : alpha * (expf(in.w) - 1);\n        *reinterpret_cast<float4*>(&output[idx]) = out;\n    } else {\n        for (int i = 0; i < VEC_SIZE && idx + i < N; ++i) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the ELU kernel by using vectorized memory accesses and warp-level operations while eliminating shared memory usage to reduce overhead.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 4;\nconstexpr int THREADS_PER_BLOCK = 256;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    \n    if (idx + VEC_SIZE - 1 < N) {\n        float4 in = *reinterpret_cast<float4*>(&input[idx]);\n        float4 out;\n        out.x = (in.x >= 0) ? in.x : alpha * (expf(in.x) - 1);\n        out.y = (in.y >= 0) ? in.y : alpha * (expf(in.y) - 1);\n        out.z = (in.z >= 0) ? in.z : alpha * (expf(in.z) - 1);\n        out.w = (in.w >= 0) ? in.w : alpha * (expf(in.w) - 1);\n        *reinterpret_cast<float4*>(&output[idx]) = out;\n    } else {\n        for (int i = 0; i < VEC_SIZE && idx + i < N; ++i) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.05067200027406216
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the ELU kernel by processing elements in a warp-coherent manner using warp-level primitives and avoiding shared memory for reduced synchronization overhead.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nconstexpr int WARP_SIZE = 32;\nconstexpr int THREADS_PER_BLOCK = 256;\nconstexpr int ELEMENTS_PER_THREAD = 8;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    namespace cg = cooperative_groups;\n    cg::thread_block_tile<WARP_SIZE> tile = cg::tiled_partition<WARP_SIZE>(cg::this_thread_block());\n    \n    int warp_id = (blockIdx.x * blockDim.x + threadIdx.x) / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int start_idx = warp_id * WARP_SIZE * ELEMENTS_PER_THREAD + lane_id;\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int idx = start_idx + i * WARP_SIZE;\n        if (idx < N) {\n            float x = input[idx];\n            output[idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int warps_per_block = THREADS_PER_BLOCK / WARP_SIZE;\n    int total_warps = (N + WARP_SIZE * ELEMENTS_PER_THREAD - 1) / (WARP_SIZE * ELEMENTS_PER_THREAD);\n    int blocks = (total_warps + warps_per_block - 1) / warps_per_block;\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the ELU kernel by processing 8 elements per thread using vectorized loads/stores and warp-level operations, with increased block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 8;\nconstexpr int THREADS_PER_BLOCK = 512;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int tid = threadIdx.x;\n    int idx = (blockIdx.x * THREADS_PER_BLOCK + tid) * VEC_SIZE;\n    \n    if (idx + VEC_SIZE - 1 < N) {\n        float4 in1 = *reinterpret_cast<float4*>(&input[idx]);\n        float4 in2 = *reinterpret_cast<float4*>(&input[idx + 4]);\n        \n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            float x = in1.x;\n            in1.x = (x >= 0) ? x : alpha * (expf(x) - 1);\n            x = in1.y;\n            in1.y = (x >= 0) ? x : alpha * (expf(x) - 1);\n            x = in1.z;\n            in1.z = (x >= 0) ? x : alpha * (expf(x) - 1);\n            x = in1.w;\n            in1.w = (x >= 0) ? x : alpha * (expf(x) - 1);\n            \n            x = in2.x;\n            in2.x = (x >= 0) ? x : alpha * (expf(x) - 1);\n            x = in2.y;\n            in2.y = (x >= 0) ? x : alpha * (expf(x) - 1);\n            x = in2.z;\n            in2.z = (x >= 0) ? x : alpha * (expf(x) - 1);\n            x = in2.w;\n            in2.w = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = in1;\n        *reinterpret_cast<float4*>(&output[idx + 4]) = in2;\n    } else {\n        for (int i = 0; i < VEC_SIZE && idx + i < N; ++i) {\n            float x = input[idx + i];\n            output[idx + i] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the ELU kernel by increasing thread block size for better occupancy, using vectorized loads/stores for memory efficiency, and maintaining numerical accuracy with precise expf calculations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    const int stride = blockDim.x * gridDim.x * 4;\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    \n    #pragma unroll\n    for (; idx < N; idx += stride) {\n        float4 x = *reinterpret_cast<float4*>(&input[idx]);\n        float4 result;\n        result.x = (x.x >= 0) ? x.x : alpha * (expf(x.x) - 1);\n        result.y = (x.y >= 0) ? x.y : alpha * (expf(x.y) - 1);\n        result.z = (x.z >= 0) ? x.z : alpha * (expf(x.z) - 1);\n        result.w = (x.w >= 0) ? x.w : alpha * (expf(x.w) - 1);\n        *reinterpret_cast<float4*>(&output[idx]) = result;\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 512;\n    int blocks = min((N + threads * 4 - 1) / (threads * 4), 1024);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.043580799177289006
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the ELU kernel by processing multiple elements per thread using vectorized memory access and warp-level operations without shared memory for reduced overhead.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VEC_SIZE = 2;\nconstexpr int THREADS_PER_BLOCK = 512;\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    int idx = (blockIdx.x * THREADS_PER_BLOCK + threadIdx.x) * VEC_SIZE;\n    \n    if (idx < N) {\n        float2 in = *reinterpret_cast<float2*>(&input[idx]);\n        float2 out;\n        out.x = (in.x >= 0) ? in.x : alpha * (expf(in.x) - 1);\n        out.y = (in.y >= 0) ? in.y : alpha * (expf(in.y) - 1);\n        *reinterpret_cast<float2*>(&output[idx]) = out;\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int blocks = (N + THREADS_PER_BLOCK * VEC_SIZE - 1) / (THREADS_PER_BLOCK * VEC_SIZE);\n\n    elu_kernel<<<blocks, THREADS_PER_BLOCK>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04444159977138042
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the ELU kernel by using a larger block size and increased grid size for better occupancy, while maintaining vectorized memory accesses and numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    const int stride = blockDim.x * gridDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; idx < N; idx += stride) {\n        float x = input[idx];\n        output[idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 512;\n    int blocks = min((N + threads - 1) / threads, 2048);\n\n    elu_kernel<<<blocks, threads>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04405119977891445
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the ELU kernel by using warp-level primitives for cooperative execution, shared memory for data reuse, and specialized handling for boundary conditions.}",
        "function": "#include <torch/extension.h>\n\n__global__ void elu_kernel(float* input, float* output, int N, float alpha) {\n    extern __shared__ float smem[];\n    int tid = threadIdx.x;\n    int idx = blockIdx.x * blockDim.x + tid;\n    \n    // Load data into shared memory\n    if (idx < N) {\n        smem[tid] = input[idx];\n    }\n    __syncthreads();\n    \n    // Process with warp-level cooperation\n    int warp_id = tid / 32;\n    int lane_id = tid % 32;\n    int warp_start = warp_id * 32;\n    int warp_end = min(warp_start + 32, blockDim.x);\n    \n    for (int i = warp_start + lane_id; i < warp_end; i += 32) {\n        int global_idx = blockIdx.x * blockDim.x + i;\n        if (global_idx < N) {\n            float x = smem[i];\n            output[global_idx] = (x >= 0) ? x : alpha * (expf(x) - 1);\n        }\n    }\n}\n\ntorch::Tensor elu_forward(torch::Tensor input, float alpha) {\n    auto output = torch::empty_like(input);\n    int N = input.numel();\n    int threads = 256;\n    int blocks = (N + threads - 1) / threads;\n    int smem_size = threads * sizeof(float);\n    \n    elu_kernel<<<blocks, threads, smem_size>>>(input.data_ptr<float>(), output.data_ptr<float>(), N, alpha);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &elu_forward, \"ELU activation forward\");\n}",
        "score": -0.04423359967768192
    }
]