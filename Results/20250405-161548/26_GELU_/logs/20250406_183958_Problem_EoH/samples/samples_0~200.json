[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the GELU kernel by using faster math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return x * static_cast<scalar_t>(0.5) * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing register pressure through loop unrolling and warp-level operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    const scalar_t x_cubed = x * x * x;\n    return x * static_cast<scalar_t>(0.5) * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll\n    for (; index < numel; index += stride) {\n        output[index] = gelu(input[index]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min((numel + threads - 1) / threads, 2048);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing register pressure.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t kAlpha = M_2_SQRTPI * M_SQRT1_2;\n    constexpr scalar_t kBeta = 0.044715f;\n    const scalar_t x_cube = x * x * x;\n    return static_cast<scalar_t>(0.5) * x * (static_cast<scalar_t>(1.0) + \n           tanhf(kAlpha * (x + kBeta * x_cube)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_fast(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int tid = threadIdx.x;\n    const int idx = blockIdx.x * blockDim.x * 4 + tid;\n    \n    if (idx + 3 * blockDim.x < numel) {\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            const int element_idx = idx + i * blockDim.x;\n            const scalar_t x = input[element_idx];\n            output[element_idx] = gelu(x);\n        }\n    } else {\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            const int element_idx = idx + i * blockDim.x;\n            if (element_idx < numel) {\n                const scalar_t x = input[element_idx];\n                output[element_idx] = gelu(x);\n            }\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_fast<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and increasing thread utilization with better block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    constexpr scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int idx = index + i * blockDim.x;\n        if (idx < numel) {\n            output[idx] = gelu(input[idx]);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing redundant computations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    const scalar_t x_cubed = x * x * x;\n    return static_cast<scalar_t>(0.5) * x * \n           (static_cast<scalar_t>(1.0) + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min((numel + threads - 1) / threads, 1024);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and increasing thread utilization with warp-level operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 4\n    for (int i = tid; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    blocks = std::min(blocks, 128);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, c10::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing memory latency through proper indexing and coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    constexpr scalar_t k = 0.044715;\n    const scalar_t x_cubed = x * x * x;\n    const scalar_t inner = sqrt_2_over_pi * (x + k * x_cubed);\n    return x * static_cast<scalar_t>(0.5) * (static_cast<scalar_t>(1.0) + tanh(inner));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int stride = gridDim.x * blockDim.x;\n    for (int index = blockIdx.x * blockDim.x + threadIdx.x; \n         index < numel; \n         index += stride) {\n        const scalar_t x = input[index];\n        output[index] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations, loop unrolling, and increased thread utilization while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int tid = threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (size_t i = blockIdx.x * blockDim.x + tid; i < numel; i += stride) {\n        scalar_t x = input[i];\n        output[i] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min(65535, (int)((numel + threads - 1) / threads));\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing memory latency through proper indexing and warp-level operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return x * 0.5f * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int stride = gridDim.x * blockDim.x;\n    for (int index = blockIdx.x * blockDim.x + threadIdx.x; \n         index < numel; \n         index += stride) {\n        output[index] = gelu(input[index]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimized GELU implementation using fast math approximations and improved memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return x * static_cast<scalar_t>(0.5) * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t x = input[i];\n        output[i] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    blocks = min(blocks, 128);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing register pressure through loop unrolling and vectorized memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t, int UNROLL_FACTOR=4>\n__global__ void gelu_kernel_optimized(const scalar_t* __restrict__ input,\n                                    scalar_t* __restrict__ output,\n                                    const size_t numel) {\n    const int tid = threadIdx.x;\n    const int idx = blockIdx.x * blockDim.x * UNROLL_FACTOR + tid;\n    \n    #pragma unroll\n    for (int i = 0; i < UNROLL_FACTOR; ++i) {\n        int element_idx = idx + i * blockDim.x;\n        if (element_idx < numel) {\n            output[element_idx] = gelu(input[element_idx]);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int unroll_factor = 4;\n    const int blocks = (numel + threads * unroll_factor - 1) / (threads * unroll_factor);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t, unroll_factor>\n            <<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                 output.data_ptr<scalar_t>(),\n                                 numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize GELU by using fast math approximations, loop unrolling, and increased thread utilization while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                           scalar_t* __restrict__ output,\n                           const size_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        const scalar_t x = input[index];\n        output[index] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    blocks = min(blocks, 1024);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and increasing thread utilization with better block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return static_cast<scalar_t>(0.5) * x * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll\n    for (; index < numel; index += stride) {\n        const scalar_t x = input[index];\n        output[index] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int multiprocessor_count;\n    cudaDeviceGetAttribute(&multiprocessor_count, cudaDevAttrMultiProcessorCount, device);\n    \n    const int threads = 256;\n    const int blocks = 4 * multiprocessor_count;\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, vectorizing memory accesses, and increasing occupancy through optimal block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <c10/cuda/CUDAMathCompat.h>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + \n           c10::cuda::compat::tanh(sqrt_2_over_pi * (x + k * x_cubed));\n}\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void gelu_kernel_fast(const scalar_t* __restrict__ input,\n                                scalar_t* __restrict__ output,\n                                const size_t numel) {\n    const int index = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    if (index < numel) {\n        scalar_t x[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (index + i < numel) {\n                x[i] = input[index + i];\n                output[index + i] = gelu(x[i]);\n            }\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    constexpr int VEC_SIZE = 4;\n    const int threads = 256;\n    const int blocks = (numel + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_fast<scalar_t, VEC_SIZE><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing memory latency through better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    scalar_t inner = sqrt_2_over_pi * (x + k * x_cubed);\n    return x * 0.5f * (1.0f + tanhf(inner));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    blocks = min(blocks, 128);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, improving memory access patterns, and increasing occupancy through optimal block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return x * static_cast<scalar_t>(0.5) * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_fast(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int device;\n    cudaGetDevice(&device);\n    int blocks;\n    cudaDeviceGetAttribute(&blocks, cudaDevAttrMultiProcessorCount, device);\n    blocks *= 4;  // Adjust for higher occupancy\n    const int threads = 256;\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_fast<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and increasing thread utilization with better block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return x * 0.5f * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        output[index] = gelu(input[index]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    blocks = min(blocks, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, reducing redundant computations, and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    constexpr scalar_t k = 0.044715;\n    const scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < numel) {\n        output[index] = gelu(input[index]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining accuracy, reducing register pressure, and increasing occupancy through optimal block/thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    constexpr scalar_t k = 0.044715;\n    const scalar_t x_cubed = x * x * x;\n    const scalar_t inner = sqrt_2_over_pi * (x + k * x_cubed);\n    return x * 0.5f * (1.0f + tanh(inner));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t x = input[i];\n        output[i] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    blocks = std::min(blocks, 1024);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, unrolling the tanh computation, and increasing thread utilization with proper block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    scalar_t inner = sqrt_2_over_pi * (x + k * x_cubed);\n    \n    // Fast tanh approximation with sufficient accuracy\n    scalar_t tanh_val = inner * (1.0f + inner * inner * (-0.3333333f + inner * inner * 0.13333333f));\n    return x * 0.5f * (1.0f + tanh_val);\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    blocks = min(blocks, 1024);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, vectorizing memory accesses, and increasing occupancy with optimal block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <c10/util/Half.h>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void gelu_kernel_fast(const scalar_t* __restrict__ input,\n                                scalar_t* __restrict__ output,\n                                const size_t numel) {\n    const int index = blockIdx.x * blockDim.x * VEC_SIZE + threadIdx.x * VEC_SIZE;\n    if (index < numel) {\n        scalar_t x[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; i++) {\n            if (index + i < numel) {\n                x[i] = input[index + i];\n                output[index + i] = gelu(x[i]);\n            }\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    constexpr int vec_size = 4;\n    const int threads = 256;\n    const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_fast<scalar_t, vec_size><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and increasing thread utilization with better block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            const scalar_t x = input[elem];\n            output[elem] = gelu(x);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing warp divergence.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return x * static_cast<scalar_t>(0.5) * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = tid; i < numel; i += stride) {\n        const scalar_t x = input[i];\n        output[i] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    blocks = min(blocks, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, vectorizing memory accesses, and increasing occupancy through optimal block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void gelu_kernel_fast(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 const size_t numel) {\n    const int tid = blockIdx.x * blockDim.x * VEC_SIZE + threadIdx.x * VEC_SIZE;\n    if (tid < numel) {\n        scalar_t x[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (tid + i < numel) {\n                x[i] = input[tid + i];\n                output[tid + i] = gelu(x[i]);\n            }\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    constexpr int VEC_SIZE = 4;\n    const int threads = 256;\n    const int blocks = (numel + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_fast<scalar_t, VEC_SIZE><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations, loop unrolling, and increased parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\n// Optimized GELU approximation with fast math\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    const scalar_t x_cubed = x * x * x;\n    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        const scalar_t x = input[index];\n        output[index] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min(65535, (int)((numel + threads - 1) / threads));\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and increasing thread utilization with better block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    constexpr scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t, int UNROLL_FACTOR=4>\n__global__ void gelu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    int index = blockIdx.x * blockDim.x * UNROLL_FACTOR + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < UNROLL_FACTOR; ++i) {\n        if (index < numel) {\n            output[index] = gelu(input[index]);\n            index += blockDim.x;\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    constexpr int threads = 256;\n    constexpr int unroll_factor = 4;\n    const int blocks = (numel + threads * unroll_factor - 1) / (threads * unroll_factor);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t, unroll_factor>\n            <<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                 output.data_ptr<scalar_t>(),\n                                 numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread block size, and minimizing memory operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return x * 0.5f * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 1024;\n    const int blocks = (numel + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize GELU kernel by using fast math approximations, loop unrolling, and increased parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n// Optimized device function to compute GELU activation\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    constexpr scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed));\n}\n\n// Optimized CUDA kernel with unrolled loops\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    constexpr int unroll_factor = 4;\n    const int tid = blockIdx.x * blockDim.x * unroll_factor + threadIdx.x;\n    \n    #pragma unroll\n    for (int i = 0; i < unroll_factor; ++i) {\n        int index = tid + i * blockDim.x;\n        if (index < numel) {\n            const scalar_t x = input[index];\n            output[index] = gelu(x);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                output.data_ptr<scalar_t>(),\n                                                numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the GELU kernel by using faster math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return x * 0.5f * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        const scalar_t x = input[index];\n        output[index] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min(65535, (int)((numel + threads - 1) / threads));\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations, loop unrolling, and increased parallelism while maintaining numerical accuracy through careful constant selection.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t kAlpha = M_2_SQRTPI * M_SQRT1_2;\n    constexpr scalar_t kBeta = 0.044715f;\n    const scalar_t x_cube = x * x * x;\n    const scalar_t inner = kAlpha * (x + kBeta * x_cube);\n    return static_cast<scalar_t>(0.5) * x * (static_cast<scalar_t>(1.0) + tanh(inner));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    constexpr int unroll_factor = 4;\n    const int index = unroll_factor * (blockIdx.x * blockDim.x + threadIdx.x);\n    \n    #pragma unroll\n    for (int i = 0; i < unroll_factor; ++i) {\n        if (index + i < numel) {\n            const scalar_t x = input[index + i];\n            output[index + i] = gelu(x);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    constexpr int threads = 256;\n    constexpr int unroll_factor = 4;\n    const int blocks = (numel + threads * unroll_factor - 1) / (threads * unroll_factor);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing memory latency through proper memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return x * static_cast<scalar_t>(0.5) * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int stride = blockDim.x * gridDim.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    for (; i < numel; i += stride) {\n        scalar_t x = input[i];\n        output[i] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    int threads = 256;\n    int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and increasing thread utilization with better block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    constexpr scalar_t k = 0.044715;\n    const scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed));\n}\n\ntemplate <typename scalar_t, int UNROLL_FACTOR=4>\n__global__ void gelu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int tid = blockIdx.x * blockDim.x * UNROLL_FACTOR + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < UNROLL_FACTOR; ++i) {\n        const int idx = tid + i * blockDim.x;\n        if (idx < numel) {\n            const scalar_t x = input[idx];\n            output[idx] = gelu(x);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    constexpr int threads = 256;\n    constexpr int unroll_factor = 4;\n    const int blocks = (numel + threads * unroll_factor - 1) / (threads * unroll_factor);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t, unroll_factor>\n            <<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                 output.data_ptr<scalar_t>(),\n                                 numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, vectorizing memory accesses, and increasing occupancy through optimal block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <c10/util/Half.h>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return x * static_cast<scalar_t>(0.5) * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void gelu_kernel_fast(const scalar_t* __restrict__ input,\n                                scalar_t* __restrict__ output,\n                                const size_t numel) {\n    const int index = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    if (index < numel) {\n        scalar_t x[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (index + i < numel) {\n                x[i] = input[index + i];\n                output[index + i] = gelu(x[i]);\n            }\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    constexpr int VEC_SIZE = 4;\n    const int threads = 256;\n    const int blocks = (numel + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_fast<scalar_t, VEC_SIZE><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize GELU kernel by using fast math approximations, loop unrolling, and increased thread utilization while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int tid = threadIdx.x;\n    const int idx = blockIdx.x * blockDim.x * 4 + tid;\n    \n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int element = idx + i * blockDim.x;\n        if (element < numel) {\n            output[element] = gelu(input[element]);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations, loop unrolling, and increased thread utilization while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    constexpr scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                           scalar_t* __restrict__ output,\n                           const size_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        const scalar_t x = input[index];\n        output[index] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min(65535, (int)((numel + threads - 1) / threads));\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the GELU kernel by using faster math approximations, loop unrolling, and increased parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n// Fast GELU approximation with minimal precision loss\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t fast_gelu(scalar_t x) {\n    const scalar_t a = 0.7978845608028654f; // sqrt(2/pi)\n    const scalar_t b = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return 0.5f * x * (1.0f + tanh(a * (x + b * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_optimized(const scalar_t* __restrict__ input,\n                                     scalar_t* __restrict__ output,\n                                     const size_t numel) {\n    const int tid = threadIdx.x;\n    const int idx = blockIdx.x * blockDim.x * 4 + tid;\n    \n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int element = idx + i * blockDim.x;\n        if (element < numel) {\n            output[element] = fast_gelu(input[element]);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize GELU computation by using fast math approximations, loop unrolling, and increased thread utilization while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return x * 0.5f * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 4\n    for (int i = index; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min(65535, (int)((numel + threads - 1) / threads);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing redundant computations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return x * 0.5f * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min((numel + threads - 1) / threads, 256);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, vectorizing memory accesses, and increasing occupancy through optimal block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                           scalar_t* __restrict__ output,\n                           const size_t numel) {\n    constexpr int vec_size = 4;\n    const int index = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (index + vec_size - 1 < numel) {\n        scalar_t in[vec_size], out[vec_size];\n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            in[i] = input[index + i];\n            out[i] = gelu(in[i]);\n            output[index + i] = out[i];\n        }\n    } else {\n        for (int i = 0; i < vec_size && (index + i) < numel; ++i) {\n            output[index + i] = gelu(input[index + i]);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    constexpr int threads = 256;\n    constexpr int vec_size = 4;\n    const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and increasing thread utilization with better block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return x * static_cast<scalar_t>(0.5) * (static_cast<scalar_t>(1.0) + \n           tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        const scalar_t x = input[index];\n        output[index] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min((numel + threads - 1) / threads, 1024);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                   output.data_ptr<scalar_t>(),\n                                                   numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining accuracy, unrolling loops, and increasing thread utilization with better block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return scalar_t(0.5) * x * (scalar_t(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        const scalar_t x = input[index];\n        output[index] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min((numel + threads - 1) / threads, 1024);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations, loop unrolling, and increased occupancy while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return x * 0.5f * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 4\n    for (int i = index; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min(65535, (int)((numel + threads - 1) / threads);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, vectorizing memory accesses, and increasing occupancy through optimal block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <c10/util/Half.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return static_cast<scalar_t>(0.5) * x * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void gelu_kernel_fast(const scalar_t* __restrict__ input,\n                                scalar_t* __restrict__ output,\n                                const size_t numel) {\n    const int index = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    if (index < numel) {\n        scalar_t x[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (index + i < numel) {\n                x[i] = input[index + i];\n                output[index + i] = gelu(x[i]);\n            }\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    constexpr int VEC_SIZE = 4;\n    const int threads = 256;\n    const int blocks = (numel + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_fast<scalar_t, VEC_SIZE><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing memory latency through proper indexing and warp-level operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return x * 0.5f * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = tid; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device);\n    \n    const int threads = 256;\n    const int blocks = std::min((numel + threads - 1) / threads, prop.maxGridSize[0]);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, c10::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and increasing thread utilization with better block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return static_cast<scalar_t>(0.5) * x * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        const scalar_t x = input[index];\n        output[index] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min(65535, (int)((numel + threads - 1) / threads));\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the GELU kernel by reducing redundant computations, using fast math approximations where safe, and improving memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    const scalar_t x_cubed = x * x * x;\n    const scalar_t inner = sqrt_2_over_pi * (x + k * x_cubed);\n    return x * static_cast<scalar_t>(0.5) * (static_cast<scalar_t>(1.0) + tanhf(inner));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < numel) {\n        output[index] = gelu(input[index]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimize the GELU kernel by using faster math approximations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    const scalar_t x_cubed = x * x * x;\n    return static_cast<scalar_t>(0.5) * x * \n           (static_cast<scalar_t>(1.0) + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        const scalar_t x = input[index];\n        output[index] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    blocks = min(blocks, 1024);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                  output.data_ptr<scalar_t>(),\n                                                  numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations, loop unrolling, and increased parallelism while maintaining numerical accuracy through careful constant selection.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    constexpr scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    scalar_t inner = sqrt_2_over_pi * (x + k * x_cubed);\n    return 0.5f * x * (1.0f + tanh(inner));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        if (index + i < numel) {\n            const scalar_t x = input[index + i];\n            output[index + i] = gelu(x);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + 4 * threads - 1) / (4 * threads);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                   output.data_ptr<scalar_t>(),\n                                                   numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations, loop unrolling, and increased parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        output[index] = gelu(input[index]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = min(65535, (int)((numel + threads - 1) / threads));\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing register pressure through loop unrolling and vectorized memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return x * 0.5f * (1.0f + tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                           scalar_t* __restrict__ output,\n                           const size_t numel) {\n    constexpr int unroll_factor = 4;\n    const int tid = threadIdx.x;\n    const int idx = blockIdx.x * blockDim.x * unroll_factor + tid;\n\n    #pragma unroll\n    for (int i = 0; i < unroll_factor; ++i) {\n        int element_idx = idx + i * blockDim.x;\n        if (element_idx < numel) {\n            output[element_idx] = gelu(input[element_idx]);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    constexpr int threads = 256;\n    constexpr int unroll_factor = 4;\n    const int blocks = (numel + threads * unroll_factor - 1) / (threads * unroll_factor);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                output.data_ptr<scalar_t>(),\n                                                numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize the GELU kernel by using faster math operations, minimizing register usage, and increasing occupancy through optimal block/thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\nconstexpr float kSqrt2OverPi = 0.7978845608028654f;\nconstexpr float kAlpha = 0.044715f;\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t x_cubed = x * x * x;\n    const scalar_t inner = kSqrt2OverPi * (x + kAlpha * x_cubed);\n    return x * static_cast<scalar_t>(0.5f) * (static_cast<scalar_t>(1.0f) + tanhf(inner));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < numel; i += stride) {\n        const scalar_t x = input[i];\n        output[i] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int blocks;\n    cudaDeviceGetAttribute(&blocks, cudaDevAttrMultiProcessorCount, device);\n    blocks *= 4;  // Increase occupancy\n    const int threads = 256;  // Better balance for modern GPUs\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, increasing thread utilization, and minimizing memory latency through better memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654f;\n    const scalar_t k = 0.044715f;\n    scalar_t x_cubed = x * x * x;\n    return 0.5f * x * (1.0f + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    const size_t numel) {\n    \n    const int stride = blockDim.x * gridDim.x;\n    for (int i = blockIdx.x * blockDim.x + threadIdx.x; \n         i < numel; \n         i += stride) {\n        const scalar_t x = input[i];\n        output[i] = gelu(x);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    \n    int threads = 256;\n    int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimize the GELU kernel by using fast math approximations while maintaining numerical accuracy, improving memory access patterns, and increasing thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cmath>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    const scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    const scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return static_cast<scalar_t>(0.5) * x * \n           (static_cast<scalar_t>(1.0) + tanhf(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = gelu(input[i]);\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    int threads = 256;\n    int blocks = (numel + threads - 1) / threads;\n    \n    blocks = std::min(blocks, 128);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize GELU kernel by using fast math approximations, loop unrolling, and increased parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n// Optimized device function to compute GELU activation\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t gelu(scalar_t x) {\n    constexpr scalar_t sqrt_2_over_pi = 0.7978845608028654;\n    constexpr scalar_t k = 0.044715;\n    scalar_t x_cubed = x * x * x;\n    return x * static_cast<scalar_t>(0.5) * (static_cast<scalar_t>(1.0) + \n           tanh(sqrt_2_over_pi * (x + k * x_cubed)));\n}\n\ntemplate <typename scalar_t>\n__global__ void gelu_kernel(const scalar_t* __restrict__ input,\n                            scalar_t* __restrict__ output,\n                            const size_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            const scalar_t x = input[elem];\n            output[elem] = gelu(x);\n        }\n    }\n}\n\ntorch::Tensor gelu_forward(torch::Tensor input) {\n    auto output = torch::empty_like(input);\n    const size_t numel = input.numel();\n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_forward_cuda\", ([&] {\n        gelu_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(),\n                                                   output.data_ptr<scalar_t>(),\n                                                   numel);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &gelu_forward, \"GELU activation forward (CUDA)\");\n}",
        "score": null
    }
]