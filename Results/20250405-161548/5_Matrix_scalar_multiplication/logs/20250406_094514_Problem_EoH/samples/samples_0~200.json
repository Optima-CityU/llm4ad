[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the CUDA kernel by using a 1D grid-stride loop for better memory access patterns and increased occupancy, while maintaining correct results.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < size; i += stride) {\n        C[i] = A[i] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    int size = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    \n    matrix_scalar_multiply_kernel<<<blocks, threads>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8560927927494049
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with coalesced memory access and increased occupancy through larger block sizes.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < num_elements) {\n        C[idx] = A[idx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n\n    int block_size = 256;\n    int grid_size = (num_elements + block_size - 1) / block_size;\n\n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8565824031829834
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimized the kernel by using 1D thread blocks and coalesced memory access for better memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        C[idx] = A[idx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto size = M * N;\n\n    auto C = torch::zeros_like(A);\n\n    int block_size = 256;\n    int grid_size = (size + block_size - 1) / block_size;\n\n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8559711992740631
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize by using 1D thread blocks for better memory access patterns and increased occupancy, while maintaining correctness through proper indexing.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        C[idx] = A[idx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto size = M * N;\n\n    auto C = torch::zeros_like(A);\n\n    int block_size = 256;\n    int grid_size = (size + block_size - 1) / block_size;\n\n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8557823956012726
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the CUDA kernel by increasing thread block size to 32x32 for better GPU occupancy and using coalesced memory access by transposing the matrix layout.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int idy = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    if (idx < N && idy < M) {\n        int index = idy * N + idx;\n        C[index] = A[index] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n\n    auto C = torch::zeros_like(A);\n\n    dim3 block(32, 32);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8728895962238312
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the CUDA kernel by using a more efficient memory access pattern through coalesced reads and writes, and increasing thread block utilization by processing multiple elements per thread.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y * 4 + threadIdx.y;\n    \n    if (row < M) {\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            if (col + i * blockDim.y < N) {\n                int index = row * N + col + i * blockDim.y;\n                C[index] = A[index] * s;\n            }\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n\n    auto C = torch::zeros_like(A);\n\n    dim3 block(32, 8);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y * 4 - 1) / (block.y * 4));\n\n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -2.188367986679077
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with coalesced memory access and increased block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < num_elements) {\n        C[idx] = A[idx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int block_size = 256;\n    int grid_size = (num_elements + block_size - 1) / block_size;\n    \n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8563424050807953
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the CUDA kernel by using a 1D grid layout with coalesced memory access and increased block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < num_elements) {\n        C[idx] = A[idx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n\n    int block_size = 256;\n    int grid_size = (num_elements + block_size - 1) / block_size;\n\n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8579200029373169
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the kernel by using a 2D tiled approach with shared memory to improve memory access locality while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    __shared__ float tile[TILE_SIZE][TILE_SIZE];\n    \n    int bx = blockIdx.x * TILE_SIZE;\n    int by = blockIdx.y * TILE_SIZE;\n    \n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    \n    int row = by + ty;\n    int col = bx + tx;\n    \n    if (row < M && col < N) {\n        tile[ty][tx] = A[row * N + col];\n    }\n    \n    __syncthreads();\n    \n    if (row < M && col < N) {\n        C[row * N + col] = tile[ty][tx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto C = torch::zeros_like(A);\n    \n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n    \n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the CUDA kernel by using warp-level programming with cooperative groups for efficient SIMD execution and reduced thread divergence, while maintaining coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cooperative_groups.h>\n\nusing namespace cooperative_groups;\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    auto g = coalesced_threads();\n    int idx = g.thread_rank() + g.meta_group_rank() * g.size();\n    int stride = g.meta_group_size() * g.size();\n    \n    for (int i = idx; i < size; i += stride) {\n        C[i] = A[i] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    int size = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    \n    matrix_scalar_multiply_kernel<<<blocks, threads>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the kernel by using a 2D block layout with shared memory for better memory locality and reduced global memory accesses, while maintaining correct results.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    __shared__ float tile[TILE_SIZE][TILE_SIZE];\n    \n    int bx = blockIdx.x * TILE_SIZE;\n    int by = blockIdx.y * TILE_SIZE;\n    \n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    \n    int row = by + ty;\n    int col = bx + tx;\n    \n    if (row < M && col < N) {\n        tile[ty][tx] = A[row * N + col];\n    }\n    __syncthreads();\n    \n    if (row < M && col < N) {\n        C[row * N + col] = tile[ty][tx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto C = torch::zeros_like(A);\n    \n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n    \n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the kernel by using warp-level shuffling to reduce memory bandwidth usage while maintaining coalesced access patterns.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    if (idx < size) {\n        float4 a = reinterpret_cast<float4*>(A)[idx/4];\n        float4 c;\n        c.x = a.x * s;\n        c.y = a.y * s;\n        c.z = a.z * s;\n        c.w = a.w * s;\n        reinterpret_cast<float4*>(C)[idx/4] = c;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    int size = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int threads = 256;\n    int blocks = (size + threads * 4 - 1) / (threads * 4);\n    \n    matrix_scalar_multiply_kernel<<<blocks, threads>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the kernel by using a 1D grid with warp-level parallelism and vectorized memory access for improved throughput.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float4 *A, float s, float4 *C, int num_elements) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < num_elements) {\n        float4 a = A[idx];\n        C[idx] = make_float4(a.x*s, a.y*s, a.z*s, a.w*s);\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel() / 4;\n    auto C = torch::zeros_like(A);\n    \n    int block_size = 256;\n    int grid_size = (num_elements + block_size - 1) / block_size;\n    \n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(\n        reinterpret_cast<float4*>(A.data_ptr<float>()), \n        s, \n        reinterpret_cast<float4*>(C.data_ptr<float>()), \n        num_elements);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8573760032653809
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the CUDA kernel by using a 2D grid-stride loop with warp-level operations to maximize memory throughput while maintaining coalesced access.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride_row = blockDim.y * gridDim.y;\n    int stride_col = blockDim.x * gridDim.x;\n    \n    for (int i = row; i < M; i += stride_row) {\n        for (int j = col; j < N; j += stride_col) {\n            int index = i * N + j;\n            C[index] = A[index] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto C = torch::zeros_like(A);\n\n    dim3 block(16, 16);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8568639934062958
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with warp-level parallelism and vectorized memory access for improved memory throughput.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    if (idx + 3 < num_elements) {\n        float4 a = reinterpret_cast<float4*>(A)[idx/4];\n        float4 c;\n        c.x = a.x * s;\n        c.y = a.y * s;\n        c.z = a.z * s;\n        c.w = a.w * s;\n        reinterpret_cast<float4*>(C)[idx/4] = c;\n    } else {\n        for (int i = 0; i < 4 && idx + i < num_elements; i++) {\n            C[idx + i] = A[idx + i] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n\n    int block_size = 256;\n    int grid_size = (num_elements + block_size * 4 - 1) / (block_size * 4);\n\n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8575551986694336
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with warp-level parallelism and vectorized memory access for improved memory throughput.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    if (idx + 3 < num_elements) {\n        float4 a = reinterpret_cast<float4*>(A)[idx/4];\n        float4 c;\n        c.x = a.x * s;\n        c.y = a.y * s;\n        c.z = a.z * s;\n        c.w = a.w * s;\n        reinterpret_cast<float4*>(C)[idx/4] = c;\n    } else {\n        for (int i = 0; i < 4 && idx + i < num_elements; i++) {\n            C[idx + i] = A[idx + i] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int block_size = 256;\n    int grid_size = (num_elements + block_size * 4 - 1) / (block_size * 4);\n    \n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8576992094516754
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimized the kernel by using warp-level parallelism and vectorized memory access to further improve memory throughput.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    if (idx + 3 < size) {\n        float4 a = reinterpret_cast<float4*>(A)[idx/4];\n        float4 c;\n        c.x = a.x * s;\n        c.y = a.y * s;\n        c.z = a.z * s;\n        c.w = a.w * s;\n        reinterpret_cast<float4*>(C)[idx/4] = c;\n    }\n    else {\n        for (int i = 0; i < 4 && (idx + i) < size; i++) {\n            C[idx + i] = A[idx + i] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto size = M * N;\n\n    auto C = torch::zeros_like(A);\n\n    int block_size = 256;\n    int grid_size = (size + block_size * 4 - 1) / (block_size * 4);\n\n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8577568054199218
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with vectorized memory access (float4) to reduce memory transactions and increase bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    if (idx + 3 < num_elements) {\n        float4 a_val = *reinterpret_cast<float4*>(&A[idx]);\n        float4 c_val;\n        c_val.x = a_val.x * s;\n        c_val.y = a_val.y * s;\n        c_val.z = a_val.z * s;\n        c_val.w = a_val.w * s;\n        *reinterpret_cast<float4*>(&C[idx]) = c_val;\n    } else {\n        // Handle remaining elements\n        for (int i = 0; i < 4 && (idx + i) < num_elements; i++) {\n            C[idx + i] = A[idx + i] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int block_size = 256;\n    int grid_size = (num_elements / 4 + block_size - 1) / block_size;\n    \n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with warp-level parallelism and vectorized memory access for improved memory throughput.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    if (idx + 3 < num_elements) {\n        float4 a_val = reinterpret_cast<float4*>(A)[idx/4];\n        float4 c_val;\n        c_val.x = a_val.x * s;\n        c_val.y = a_val.y * s;\n        c_val.z = a_val.z * s;\n        c_val.w = a_val.w * s;\n        reinterpret_cast<float4*>(C)[idx/4] = c_val;\n    } else {\n        for (int i = 0; i < 4 && idx + i < num_elements; i++) {\n            C[idx + i] = A[idx + i] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int block_size = 256;\n    int grid_size = (num_elements + block_size * 4 - 1) / (block_size * 4);\n    \n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.859718406200409
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with vectorized memory access (float4) and increased block size for better memory throughput.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    if (idx + 3 < num_elements) {\n        float4 a = *reinterpret_cast<float4*>(&A[idx]);\n        float4 c;\n        c.x = a.x * s;\n        c.y = a.y * s;\n        c.z = a.z * s;\n        c.w = a.w * s;\n        *reinterpret_cast<float4*>(&C[idx]) = c;\n    } else {\n        // Handle remaining elements\n        for (int i = 0; i < 4 && (idx + i) < num_elements; i++) {\n            C[idx + i] = A[idx + i] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int block_size = 256;\n    int grid_size = (num_elements / 4 + block_size - 1) / block_size;\n    \n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8574752092361451
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with coalesced memory access, increased block size for better occupancy, and unrolling loops for improved instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    if (idx + 3 < num_elements) {\n        C[idx] = A[idx] * s;\n        C[idx+1] = A[idx+1] * s;\n        C[idx+2] = A[idx+2] * s;\n        C[idx+3] = A[idx+3] * s;\n    } else {\n        for (int i = 0; i < 4 && (idx + i) < num_elements; i++) {\n            C[idx+i] = A[idx+i] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int block_size = 512;\n    int grid_size = (num_elements + block_size * 4 - 1) / (block_size * 4);\n    \n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8583007991313935
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with improved memory coalescing and increased occupancy through warp-aligned block sizes and loop unrolling.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = blockIdx.x * (blockDim.x * 4) + threadIdx.x;\n    #pragma unroll 4\n    for (int i = 0; i < 4; ++i) {\n        if (idx < num_elements) {\n            C[idx] = A[idx] * s;\n            idx += blockDim.x;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n\n    int block_size = 128;  // Warp-aligned size\n    int grid_size = (num_elements + block_size * 4 - 1) / (block_size * 4);\n\n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8586240172386169
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with larger block size (512 threads) and unrolled loops for better instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    if (idx < num_elements) {\n        float4 a = reinterpret_cast<float4*>(A)[idx/4];\n        float4 c;\n        c.x = a.x * s;\n        c.y = a.y * s;\n        c.z = a.z * s;\n        c.w = a.w * s;\n        reinterpret_cast<float4*>(C)[idx/4] = c;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int block_size = 512;\n    int grid_size = (num_elements / 4 + block_size - 1) / block_size;\n    \n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8572800099849701
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the kernel by using warp-level parallelism with vectorized memory access and shared memory for reduced global memory transactions.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define VEC_SIZE 4\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    \n    if (idx < num_elements) {\n        float4 a_vec = *reinterpret_cast<float4*>(&A[idx]);\n        float4 c_vec;\n        c_vec.x = a_vec.x * s;\n        c_vec.y = a_vec.y * s;\n        c_vec.z = a_vec.z * s;\n        c_vec.w = a_vec.w * s;\n        *reinterpret_cast<float4*>(&C[idx]) = c_vec;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n\n    int block_size = 256;\n    int grid_size = (num_elements / VEC_SIZE + block_size - 1) / block_size;\n\n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the kernel by using a 2D grid layout with larger block size (32x8) and memory coalescing while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    if (row < M && col < N) {\n        int index = col * M + row;\n        C[index] = A[index] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n\n    auto C = torch::zeros_like(A);\n\n    dim3 block(32, 8);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8551359951496125
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with warp-level parallelism and vectorized memory access for improved throughput.}",
        "function": "#include <torch/extension.h>\n\nconstexpr int VECTOR_SIZE = 4;\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VECTOR_SIZE;\n    if (idx < num_elements) {\n        float4 a = *reinterpret_cast<float4*>(&A[idx]);\n        float4 c;\n        c.x = a.x * s;\n        c.y = a.y * s;\n        c.z = a.z * s;\n        c.w = a.w * s;\n        *reinterpret_cast<float4*>(&C[idx]) = c;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n\n    int block_size = 256;\n    int grid_size = (num_elements / VECTOR_SIZE + block_size - 1) / block_size;\n\n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the kernel by using warp-level parallelism with shuffle instructions to reduce memory bandwidth requirements while maintaining coalesced access.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * warpSize;\n    if (idx < num_elements) {\n        #pragma unroll\n        for (int i = 0; i < warpSize && (idx + i) < num_elements; ++i) {\n            C[idx + i] = A[idx + i] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n\n    int block_size = 256;\n    int grid_size = (num_elements + block_size * warpSize - 1) / (block_size * warpSize);\n\n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with warp-level parallelism and increased block size while maintaining coalesced memory access.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; i++) {\n        int element_idx = idx + i * blockDim.x;\n        if (element_idx < num_elements) {\n            C[element_idx] = A[element_idx] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int block_size = 256;\n    int grid_size = (num_elements + block_size * 4 - 1) / (block_size * 4);\n    \n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8638495981693268
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the kernel by using a 2D grid layout with shared memory for better memory locality and reduced global memory accesses, while maintaining correct results.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    extern __shared__ float tile[];\n    \n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    int bx = blockIdx.x * blockDim.x;\n    int by = blockIdx.y * blockDim.y;\n    \n    int row = by + ty;\n    int col = bx + tx;\n    \n    if (row < M && col < N) {\n        int index = row * N + col;\n        tile[ty * blockDim.x + tx] = A[index];\n        __syncthreads();\n        \n        C[index] = tile[ty * blockDim.x + tx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto C = torch::zeros_like(A);\n    \n    dim3 block(16, 16);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n    \n    size_t shared_mem_size = block.x * block.y * sizeof(float);\n    matrix_scalar_multiply_kernel<<<grid, block, shared_mem_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the CUDA kernel by using a 1D grid-stride loop with warp-level parallelism and shared memory for coalesced memory access, while maintaining correct results.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    extern __shared__ float shared_A[];\n    \n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int tid = threadIdx.x;\n    int warp_size = 32;\n    int warp_id = tid / warp_size;\n    int lane_id = tid % warp_size;\n    int warp_stride = blockDim.x / warp_size;\n    \n    for (int i = blockIdx.x * blockDim.x + warp_id * warp_size; \n         i < size; \n         i += gridDim.x * blockDim.x) {\n        int load_idx = i + lane_id;\n        if (load_idx < size) {\n            shared_A[tid] = A[load_idx];\n        }\n        __syncthreads();\n        \n        if (load_idx < size) {\n            C[load_idx] = shared_A[tid] * s;\n        }\n        __syncthreads();\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    int size = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int threads = 256;\n    int blocks = min((size + threads - 1) / threads, 2048);\n    int shared_mem = threads * sizeof(float);\n    \n    matrix_scalar_multiply_kernel<<<blocks, threads, shared_mem>>>(\n        A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8763296008110046
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the kernel by using warp-level programming and vectorized memory access to maximize memory throughput while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    if (idx < size) {\n        float4 a = reinterpret_cast<float4*>(A)[idx/4];\n        float4 c;\n        c.x = a.x * s;\n        c.y = a.y * s;\n        c.z = a.z * s;\n        c.w = a.w * s;\n        reinterpret_cast<float4*>(C)[idx/4] = c;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    int size = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int threads = 256;\n    int blocks = (size + threads * 4 - 1) / (threads * 4);\n    \n    matrix_scalar_multiply_kernel<<<blocks, threads>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8587456047534943
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the CUDA kernel by using 2D grid-stride loops with coalesced memory access and increased occupancy, while maintaining correct results.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    int stride_row = blockDim.x * gridDim.x;\n    int stride_col = blockDim.y * gridDim.y;\n    \n    for (int i = row; i < M; i += stride_row) {\n        for (int j = col; j < N; j += stride_col) {\n            int index = i * N + j;\n            C[index] = A[index] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto C = torch::zeros_like(A);\n    \n    dim3 block(16, 16);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n    \n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the kernel by using warp-level parallelism with shuffle instructions to reduce memory bandwidth requirements while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int warp_id = idx / 32;\n    int lane_id = idx % 32;\n    \n    if (warp_id * 32 < size) {\n        int end = min((warp_id + 1) * 32, size);\n        for (int i = warp_id * 32 + lane_id; i < end; i += 32) {\n            C[i] = A[i] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    int size = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    \n    matrix_scalar_multiply_kernel<<<blocks, threads>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8560351967811585
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the CUDA kernel by using a 2D grid-stride loop with coalesced memory access and increased occupancy, while maintaining correct results.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int idy = blockIdx.y * blockDim.y + threadIdx.y;\n    int stride_x = blockDim.x * gridDim.x;\n    int stride_y = blockDim.y * gridDim.y;\n    \n    for (int i = idx; i < M; i += stride_x) {\n        for (int j = idy; j < N; j += stride_y) {\n            int index = i + j * M;\n            C[index] = A[index] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto C = torch::zeros_like(A);\n    \n    dim3 block(16, 16);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n    \n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8575103998184204
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the CUDA kernel by using a 2D grid-stride loop with coalesced memory access and increased thread block size for better performance.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int idy = blockIdx.y * blockDim.y + threadIdx.y;\n    int stride_x = blockDim.x * gridDim.x;\n    int stride_y = blockDim.y * gridDim.y;\n    \n    for (int i = idx; i < M; i += stride_x) {\n        for (int j = idy; j < N; j += stride_y) {\n            int index = j * M + i;\n            C[index] = A[index] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto C = torch::zeros_like(A);\n    \n    dim3 block(32, 32);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n    \n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.897404795885086
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the kernel by using a 1D grid layout with coalesced memory access, increased block size to 512 threads for higher occupancy, and adding loop unrolling for better instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int num_elements) {\n    int idx = blockIdx.x * (blockDim.x * 4) + threadIdx.x;\n    #pragma unroll 4\n    for (int i = 0; i < 4; i++) {\n        int current_idx = idx + i * blockDim.x;\n        if (current_idx < num_elements) {\n            C[current_idx] = A[current_idx] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto num_elements = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int block_size = 512;\n    int grid_size = (num_elements + block_size * 4 - 1) / (block_size * 4);\n    \n    matrix_scalar_multiply_kernel<<<grid_size, block_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), num_elements);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the CUDA kernel by using a 2D grid-stride loop with vectorized memory access for improved memory throughput while maintaining correct results.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    int stride_row = blockDim.x * gridDim.x;\n    int stride_col = blockDim.y * gridDim.y;\n    \n    for (int i = row; i < M; i += stride_row) {\n        for (int j = col; j < N; j += stride_col) {\n            int index = i * N + j;\n            C[index] = A[index] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto C = torch::zeros_like(A);\n    \n    dim3 block(16, 16);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n    \n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -1.270854389667511
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the CUDA kernel by using a 2D grid-stride loop with increased block size for better memory access patterns and higher occupancy, while maintaining correct results.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int idy = blockIdx.y * blockDim.y + threadIdx.y;\n    int stride_x = blockDim.x * gridDim.x;\n    int stride_y = blockDim.y * gridDim.y;\n    \n    for (int i = idx; i < M; i += stride_x) {\n        for (int j = idy; j < N; j += stride_y) {\n            int index = j * M + i;\n            C[index] = A[index] * s;\n        }\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n    auto C = torch::zeros_like(A);\n    \n    dim3 block(32, 32);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n    \n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8990048050880433
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the kernel by using 2D thread blocks with increased block size (32x32) to better utilize shared memory and reduce global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int idy = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    if (idx < M && idy < N) {\n        int index = idy * M + idx;\n        C[index] = A[index] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n\n    auto C = torch::zeros_like(A);\n\n    dim3 block(32, 32);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8702975869178772
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the kernel by using shared memory to batch load elements and perform coalesced global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    extern __shared__ float shared_A[];\n    \n    int tid = threadIdx.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < size; i += stride) {\n        shared_A[tid] = A[i];\n        __syncthreads();\n        \n        float val = shared_A[tid] * s;\n        __syncthreads();\n        \n        C[i] = val;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    int size = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    int shared_mem_size = threads * sizeof(float);\n    \n    matrix_scalar_multiply_kernel<<<blocks, threads, shared_mem_size>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the kernel by using a 1D block layout with increased block size (256 threads) and coalesced memory access while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < size) {\n        C[idx] = A[idx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    int size = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    \n    matrix_scalar_multiply_kernel<<<blocks, threads>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8556383907794952
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the kernel by using shared memory to batch load elements and process them cooperatively within thread blocks, reducing global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    extern __shared__ float shared_A[];\n    \n    int tid = threadIdx.x;\n    int idx = blockIdx.x * blockDim.x + tid;\n    int chunk_size = blockDim.x;\n    \n    if (idx < size) {\n        shared_A[tid] = A[idx];\n    }\n    __syncthreads();\n    \n    if (idx < size) {\n        C[idx] = shared_A[tid] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    int size = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    int shared_mem = threads * sizeof(float);\n    \n    matrix_scalar_multiply_kernel<<<blocks, threads, shared_mem>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the kernel by using 2D tiling with shared memory to improve memory access locality and reduce global memory transactions while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    __shared__ float tile[TILE_SIZE][TILE_SIZE];\n    \n    int bx = blockIdx.x * TILE_SIZE;\n    int by = blockIdx.y * TILE_SIZE;\n    \n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    \n    int row = by + ty;\n    int col = bx + tx;\n    \n    if (row < M && col < N) {\n        tile[ty][tx] = A[row * N + col];\n    }\n    \n    __syncthreads();\n    \n    if (row < M && col < N) {\n        C[row * N + col] = tile[ty][tx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n\n    auto C = torch::zeros_like(A);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the kernel by processing multiple elements per thread in a coalesced memory access pattern while maintaining warp-level efficiency.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int size) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    \n    if (idx < size) {\n        float4 a = reinterpret_cast<float4*>(A)[idx/4];\n        float4 c;\n        c.x = a.x * s;\n        c.y = a.y * s;\n        c.z = a.z * s;\n        c.w = a.w * s;\n        reinterpret_cast<float4*>(C)[idx/4] = c;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    int size = A.numel();\n    auto C = torch::zeros_like(A);\n    \n    int threads = 256;\n    int blocks = (size / 4 + threads - 1) / threads;\n    \n    matrix_scalar_multiply_kernel<<<blocks, threads>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), size);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");\n}",
        "score": -0.8582272052764892
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the kernel by using 2D tiling with shared memory to improve data reuse and reduce global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matrix_scalar_multiply_kernel(float *A, float s, float *C, int M, int N) {\n    __shared__ float tile[TILE_SIZE][TILE_SIZE];\n    \n    int bx = blockIdx.x * TILE_SIZE;\n    int by = blockIdx.y * TILE_SIZE;\n    \n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    \n    int row = by + ty;\n    int col = bx + tx;\n    \n    if (row < M && col < N) {\n        tile[ty][tx] = A[row * N + col];\n    }\n    \n    __syncthreads();\n    \n    if (row < M && col < N) {\n        C[row * N + col] = tile[ty][tx] * s;\n    }\n}\n\ntorch::Tensor module_fn_cuda(torch::Tensor A, float s) {\n    auto M = A.size(0);\n    auto N = A.size(1);\n\n    auto C = torch::zeros_like(A);\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matrix_scalar_multiply_kernel<<<grid, block>>>(A.data_ptr<float>(), s, C.data_ptr<float>(), M, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &module_fn_cuda, \"Matrix-scalar multiplication kernel\");",
        "score": null
    }
]