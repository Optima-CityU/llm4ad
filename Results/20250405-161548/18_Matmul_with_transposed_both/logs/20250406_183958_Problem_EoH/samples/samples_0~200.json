[
    {
        "sample_order": 1,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and thread block configuration for transposed inputs.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = row;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        int b_col = col;\n\n        if (a_col < K && a_row < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_col * M + a_row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(\n            A.data_ptr<scalar_t>(), \n            B.data_ptr<scalar_t>(), \n            C.data_ptr<scalar_t>(), \n            M, K, N\n        );\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, improved memory access patterns, and increased block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        \n        // Load A tile\n        int A_row = row;\n        int A_col = tile_offset + threadIdx.y;\n        if (A_row < M && A_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[A_col * M + A_row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        // Load B tile\n        int B_row = tile_offset + threadIdx.x;\n        int B_col = col;\n        if (B_row < K && B_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[B_row * N + B_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial sum\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimized kernel using shared memory for tiled matrix multiplication with transposed inputs to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, improved memory access patterns, and increased occupancy through better block/grid dimensions.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tiles into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[(tiled_k + threadIdx.x) * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial sum\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + threadIdx.y;\n        int B_row = tiled_k + threadIdx.x;\n\n        if (row < M && A_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[A_col * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && B_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[B_row * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimized the kernel using shared memory for tiled matrix multiplication with transposed inputs, improving memory access patterns and reducing global memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimized the kernel by using shared memory to cache tiles of input matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int local_k = tiled_k + threadIdx.y;\n        \n        if (row < M && local_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[local_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        if (col < N && local_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[local_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, better memory access patterns, and increased thread block size for improved occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\nconstexpr int TILE_SIZE = 32;\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, improving memory access patterns and reducing global memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.x;\n        int b_col = tiled_k + threadIdx.x;\n        \n        if (row < M && a_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && b_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.x;\n        int b_col = tiled_k + threadIdx.x;\n\n        if (row < M && a_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of input matrices A and B for better memory access patterns and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled = t * TILE_SIZE;\n        \n        // Load tiles into shared memory\n        int A_row = tiled + threadIdx.y;\n        int A_col = row;\n        if (A_row < K && A_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[A_row * M + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        int B_row = tiled + threadIdx.y;\n        int B_col = col;\n        if (B_row < K && B_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_row * N + B_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial sum\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of input matrices A and B, with improved memory access patterns and thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_row = tile_offset + threadIdx.y;\n        int b_col = tile_offset + threadIdx.x;\n        \n        if (row < M && a_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && b_col < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, coalesced memory access, and increased thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.y][threadIdx.x] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n    \n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n    \n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of input matrices A and B, with improved memory access patterns and thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        \n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_col * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && b_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the matrix multiplication with transposed inputs by using shared memory to cache tiles of A and B, and increasing thread block size to improve memory access patterns and occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tRow = t * TILE_SIZE + threadIdx.x;\n        int tCol = t * TILE_SIZE + threadIdx.y;\n        \n        if (tRow < K && row < M) {\n            As[threadIdx.y][threadIdx.x] = A[tRow * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (tCol < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tCol * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n    \n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n    \n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and increased occupancy through block size tuning.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int b_col = tiled_k + threadIdx.x;\n\n        if (row < M && a_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(\n            A.data_ptr<scalar_t>(),\n            B.data_ptr<scalar_t>(),\n            C.data_ptr<scalar_t>(),\n            M, K, N\n        );\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of input matrices A and B, with improved memory access patterns and thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        \n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_col * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && b_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (tiled_i < K && row < M) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (tiled_i < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimized implementation using shared memory for coalesced memory access and loop unrolling to reduce memory latency and improve throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        \n        // Load tiles into shared memory\n        if (row < M && (tiledK + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[(tiledK + threadIdx.y) * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        if (col < N && (tiledK + threadIdx.x) < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[(tiledK + threadIdx.x) * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial sum with unrolling\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n            sum += As[threadIdx.x][i+1] * Bs[i+1][threadIdx.y];\n            sum += As[threadIdx.x][i+2] * Bs[i+2][threadIdx.y];\n            sum += As[threadIdx.x][i+3] * Bs[i+3][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimized implementation using shared memory for tiling to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimized implementation using shared memory for tiling to reduce global memory accesses and increase memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.x;\n        int b_col = tiled_k + threadIdx.x;\n        \n        if (row < M && a_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && b_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n    \n    auto C = torch::zeros({M, N}, A.options());\n    \n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n    \n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(\n            A.data_ptr<scalar_t>(), \n            B.data_ptr<scalar_t>(), \n            C.data_ptr<scalar_t>(), \n            M, K, N\n        );\n    }));\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the CUDA kernel by utilizing shared memory for tiling to reduce global memory accesses and improve memory coalescing, while maintaining the same computation pattern as the original kernel.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and bank conflict avoidance to compute A.T * B.T efficiently.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        \n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_col * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && b_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n        \n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[a_col * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B matrices, reducing global memory accesses, and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.x;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.y;\n        int b_col = col;\n        \n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the matrix multiplication with transposed inputs by using shared memory to cache tiles of input matrices and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int A_row = tiledK + threadIdx.y;\n        int A_col = row;\n        int B_row = tiledK + threadIdx.x;\n        int B_col = col;\n        \n        if (A_row < K && A_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[A_row * M + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (B_row < K && B_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[B_row * N + B_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of A and B matrices, with improved memory access patterns and thread block configuration for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_row = tiled_k + threadIdx.x;\n        int B_col = tiled_k + threadIdx.x;\n        \n        if (row < M && A_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[A_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && B_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of A and B matrices, with improved memory access patterns and thread block configuration for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_row = tiled_k + threadIdx.x;\n        int B_col = tiled_k + threadIdx.x;\n        \n        if (row < M && A_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[A_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && B_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the matrix multiplication with transposed inputs by using shared memory to cache tiles of input matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tiles into shared memory\n        if (row < M && (tiled_k + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[(tiled_k + threadIdx.y) * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.x) < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[(tiled_k + threadIdx.x) * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial sum\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimized implementation using shared memory for coalesced memory access and loop unrolling to reduce global memory accesses and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < K; t += TILE_SIZE) {\n        if (row < M && (t + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[(t + threadIdx.y) * M + row];\n        }\n        if (col < N && (t + threadIdx.x) < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[(t + threadIdx.x) * N + col];\n        }\n        __syncthreads();\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n            sum += As[threadIdx.x][i+1] * Bs[i+1][threadIdx.y];\n            sum += As[threadIdx.x][i+2] * Bs[i+2][threadIdx.y];\n            sum += As[threadIdx.x][i+3] * Bs[i+3][threadIdx.y];\n        }\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n    \n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n    \n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimized implementation using shared memory to cache tiles of A and B, and increasing thread block size to 32x32 for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled = t * TILE_SIZE;\n        int A_row = tiled + threadIdx.y;\n        int B_col = tiled + threadIdx.x;\n        \n        if (row < M && A_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[A_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && B_col < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[B_col * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (tiled_i < K && row < M) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (tiled_i < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimized implementation using shared memory for coalesced memory access and increased arithmetic intensity by tiling the computation.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and increased block size for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.y][threadIdx.x] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and bank conflict avoidance.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.x;\n        int b_col = tiled_k + threadIdx.y;\n        \n        if (a_row < K && row < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (b_col < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimized implementation using shared memory for tiling to reduce global memory accesses and improve memory coalescing, while maintaining correct matrix multiplication with transposed inputs.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int b_col = tiled_k + threadIdx.x;\n        \n        if (row < M && a_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && b_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the matrix multiplication with transposed inputs by using shared memory for tiling and increasing thread block size to improve memory access patterns and occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_row = row;\n        int A_col = tiled_k + threadIdx.y;\n        int B_row = tiled_k + threadIdx.x;\n        int B_col = col;\n        \n        if (A_row < M && A_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[A_col * M + A_row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        if (B_row < K && B_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[B_col * K + B_row];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and bank conflict avoidance.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = row;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        int b_col = col;\n        \n        if (a_row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_col * M + a_row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_col * K + b_row];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and bank conflict avoidance.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        \n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_col * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && b_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n    \n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n    \n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and bank conflict avoidance.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        \n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_col * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int local_k = tiled_k + threadIdx.y;\n        \n        if (row < M && local_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[local_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        local_k = tiled_k + threadIdx.x;\n        if (col < N && local_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[local_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[threadIdx.y][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimized the matmul_transpose kernel by using shared memory for tiled matrix multiplication and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n        \n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_col * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimized implementation using shared memory for tiling to reduce global memory accesses and increase memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = row;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n        \n        if (a_col < K && a_row < M) {\n            As[threadIdx.x][threadIdx.y] = A[a_col * M + a_row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and bank conflict avoidance.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and bank conflict avoidance.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n        \n        if (row < M && A_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[A_col * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && B_row < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory access patterns and increased block size for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled = t * TILE_SIZE;\n        int A_row = tiled + threadIdx.x;\n        int B_col = tiled + threadIdx.y;\n        \n        if (row < M && A_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[A_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && B_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and bank conflict avoidance.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with transposed inputs, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int b_col = tiled_k + threadIdx.x;\n        \n        if (row < M && a_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && b_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    AT_DISPATCH_FLOATING_TYPES(A.type(), \"matmul_transpose\", ([&] {\n        matmul_transpose_kernel<scalar_t><<<grid, block>>>(A.data_ptr<scalar_t>(), B.data_ptr<scalar_t>(), C.data_ptr<scalar_t>(), M, K, N);\n    }));\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transposed inputs\");\n}",
        "score": null
    }
]