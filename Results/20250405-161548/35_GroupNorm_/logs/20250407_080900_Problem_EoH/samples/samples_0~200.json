[
    {
        "sample_order": 1,
        "algorithm": "{Optimize by parallelizing over groups and using shared memory for mean/variance calculations within each group, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias,\n                                 float* output, int batch_size, int num_features,\n                                 int num_groups, int group_size, int num_elements) {\n    extern __shared__ float shared_data[];\n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int tid = threadIdx.x;\n\n    int feature_offset = group_idx * group_size;\n    int batch_offset = batch_idx * num_features;\n    int group_start = batch_offset + feature_offset;\n\n    // Compute mean\n    float sum = 0.0f;\n    if (tid < group_size) {\n        sum = x[group_start + tid];\n    }\n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        __syncthreads();\n        if (tid < stride && tid + stride < group_size) {\n            sum += x[group_start + tid + stride];\n        }\n    }\n    __syncthreads();\n    if (tid == 0) {\n        shared_data[0] = sum / group_size;\n    }\n    __syncthreads();\n    float mean = shared_data[0];\n\n    // Compute variance\n    float var_sum = 0.0f;\n    if (tid < group_size) {\n        float diff = x[group_start + tid] - mean;\n        var_sum = diff * diff;\n    }\n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        __syncthreads();\n        if (tid < stride && tid + stride < group_size) {\n            var_sum += var_sum;\n        }\n    }\n    __syncthreads();\n    if (tid == 0) {\n        shared_data[1] = var_sum / group_size;\n    }\n    __syncthreads();\n    float stddev = sqrtf(shared_data[1] + 1e-5f);\n\n    // Apply normalization\n    if (tid < group_size) {\n        int feature_idx = feature_offset + tid;\n        output[batch_offset + feature_idx] = \n            gn_weight[feature_idx] * ((x[group_start + tid] - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(batch_size, num_groups);\n    int threads = min(1024, (group_size + 31) / 32 * 32);\n    size_t shared_mem = 2 * sizeof(float);\n\n    group_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, batch_size * num_features\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize by parallelizing mean and variance calculations across threads within a group, using shared memory for intermediate results, and reducing atomic operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    extern __shared__ float shared_data[];\n    \n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    int group_offset = group_idx * group_size;\n    int batch_offset = batch_idx * num_features;\n    \n    // Load data into shared memory\n    if (tid < group_size) {\n        shared_data[tid] = x[batch_offset + group_offset + tid];\n    }\n    __syncthreads();\n    \n    // Compute mean\n    float sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        sum += shared_data[i];\n    }\n    \n    __shared__ float mean_shared;\n    float block_sum = blockReduceSum(sum);\n    if (tid == 0) {\n        mean_shared = block_sum / group_size;\n    }\n    __syncthreads();\n    \n    // Compute variance\n    float var_sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        float diff = shared_data[i] - mean_shared;\n        var_sum += diff * diff;\n    }\n    \n    __shared__ float var_shared;\n    float block_var_sum = blockReduceSum(var_sum);\n    if (tid == 0) {\n        var_shared = sqrtf(block_var_sum / group_size + 1e-5f);\n    }\n    __syncthreads();\n    \n    // Normalize and store\n    if (tid < group_size) {\n        int feature_idx = group_offset + tid;\n        float val = (shared_data[tid] - mean_shared) / var_shared;\n        output[batch_offset + feature_idx] = gn_weight[feature_idx] * val + gn_bias[feature_idx];\n    }\n}\n\n__device__ float blockReduceSum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n    \n    val = warpReduceSum(val);\n    \n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n    \n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;\n    if (wid == 0) val = warpReduceSum(val);\n    return val;\n}\n\n__device__ float warpReduceSum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    dim3 blocks(batch_size, num_groups);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t shared_size = group_size * sizeof(float);\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    group_norm_kernel<<<blocks, threads, shared_size>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n                                                       gn_bias.data_ptr<float>(), output.data_ptr<float>(), \n                                                       batch_size, num_features, num_groups, group_size, \n                                                       batch_size * num_features);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize by parallelizing mean and variance calculations per group using shared memory and warp-level reductions, then applying normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0;\n    if (wid == 0) val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size, int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    \n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_groups;\n    \n    int tid = threadIdx.x;\n    int feature_offset = group_idx * group_size;\n    int batch_offset = batch_idx * num_features;\n    \n    // Compute mean\n    float sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int idx = batch_offset + feature_offset + i;\n        sum += x[idx];\n    }\n    \n    sum = block_reduce_sum(sum);\n    if (tid == 0) {\n        mean_shared[group_idx] = sum / group_size;\n    }\n    block.sync();\n    \n    float mean = mean_shared[group_idx];\n    \n    // Compute variance\n    float var_sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int idx = batch_offset + feature_offset + i;\n        float diff = x[idx] - mean;\n        var_sum += diff * diff;\n    }\n    \n    var_sum = block_reduce_sum(var_sum);\n    if (tid == 0) {\n        var_shared[group_idx] = var_sum / group_size;\n    }\n    block.sync();\n    \n    float variance = var_shared[group_idx];\n    float stddev = sqrtf(variance + 1e-5f);\n    \n    // Apply normalization\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_offset + i;\n        int idx = batch_offset + feature_idx;\n        output[idx] = gn_weight[feature_idx] * ((x[idx] - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = 2 * num_groups * sizeof(float);\n    \n    group_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, batch_size * num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{The optimized algorithm uses parallel reduction within each group to compute mean and variance, followed by cooperative group synchronization for correctness, and then applies normalization with proper memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cooperative_groups/reduce.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    cg::thread_block cta = cg::this_thread_block();\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cta);\n    \n    int batch_idx = blockIdx.y;\n    int group_idx = blockIdx.x;\n    int feature_idx = group_idx * group_size + threadIdx.x;\n    \n    if (batch_idx >= batch_size || group_idx >= num_groups) return;\n    \n    // Load data into shared memory\n    extern __shared__ float shmem[];\n    float* group_data = shmem;\n    \n    if (threadIdx.x < group_size) {\n        group_data[threadIdx.x] = x[batch_idx * num_features + feature_idx];\n    }\n    cta.sync();\n    \n    // Parallel reduction for mean\n    float sum = 0.0f;\n    for (int i = threadIdx.x; i < group_size; i += blockDim.x) {\n        sum += group_data[i];\n    }\n    sum = cg::reduce(tile, sum, cg::plus<float>());\n    float mean = sum / group_size;\n    cta.sync();\n    \n    // Parallel reduction for variance\n    float var_sum = 0.0f;\n    for (int i = threadIdx.x; i < group_size; i += blockDim.x) {\n        float diff = group_data[i] - mean;\n        var_sum += diff * diff;\n    }\n    var_sum = cg::reduce(tile, var_sum, cg::plus<float>());\n    float variance = var_sum / group_size;\n    float stddev = sqrtf(variance + 1e-5f);\n    cta.sync();\n    \n    // Apply normalization\n    if (threadIdx.x < group_size) {\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((group_data[threadIdx.x] - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    // Allocate memory for output tensor\n    torch::Tensor output = torch::empty_like(x);\n    \n    // Launch kernel with 2D grid\n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t shmem_size = group_size * sizeof(float);\n    \n    group_norm_kernel<<<blocks, threads, shmem_size>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n                                                     gn_bias.data_ptr<float>(), output.data_ptr<float>(), \n                                                     batch_size, num_features, num_groups, group_size, 0);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize by parallelizing mean and variance calculations per group using shared memory and warp-level reductions, then applying normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val, float* shared) {\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;\n    if (wid == 0) val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size, \n                                 int num_elements) {\n    extern __shared__ float shared[];\n\n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int tid = threadIdx.x;\n\n    int group_start = group_idx * group_size;\n    int group_end = group_start + group_size;\n\n    float mean = 0.0f, variance = 0.0f;\n    float val;\n\n    // Calculate mean\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_start + i;\n        val = x[batch_idx * num_features + feature_idx];\n        mean += val;\n    }\n    mean = block_reduce_sum(mean, shared);\n    if (threadIdx.x == 0) {\n        mean /= group_size;\n        shared[0] = mean;\n    }\n    __syncthreads();\n    mean = shared[0];\n\n    // Calculate variance\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_start + i;\n        val = x[batch_idx * num_features + feature_idx] - mean;\n        variance += val * val;\n    }\n    variance = block_reduce_sum(variance, shared);\n    if (threadIdx.x == 0) {\n        variance = variance / group_size + 1e-5f;\n        shared[0] = rsqrtf(variance);\n    }\n    __syncthreads();\n    float inv_std = shared[0];\n\n    // Apply normalization\n    if (tid < group_size) {\n        int feature_idx = group_start + tid;\n        float normalized = (x[batch_idx * num_features + feature_idx] - mean) * inv_std;\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * normalized + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(batch_size, num_groups);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t shared_size = sizeof(float) * ((threads + 31) / 32);\n\n    group_norm_kernel<<<blocks, threads, shared_size>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n                                                       gn_bias.data_ptr<float>(), output.data_ptr<float>(), \n                                                       batch_size, num_features, num_groups, group_size, \n                                                       batch_size * num_features);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize group normalization by parallelizing mean and variance calculations across groups and using shared memory for intermediate results within each group.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    extern __shared__ float shared_data[];\n    \n    int batch_idx = blockIdx.y;\n    int group_idx = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    int feature_offset = group_idx * group_size;\n    int input_offset = batch_idx * num_features + feature_offset;\n    \n    // Compute mean\n    float sum = 0.0f;\n    if (tid < group_size) {\n        sum = x[input_offset + tid];\n    }\n    \n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        __syncthreads();\n        if (tid < stride && (tid + stride) < group_size) {\n            sum += x[input_offset + tid + stride];\n        }\n    }\n    \n    __syncthreads();\n    if (tid == 0) {\n        shared_data[0] = sum / group_size;\n    }\n    __syncthreads();\n    float mean = shared_data[0];\n    \n    // Compute variance\n    float var_sum = 0.0f;\n    if (tid < group_size) {\n        float diff = x[input_offset + tid] - mean;\n        var_sum = diff * diff;\n    }\n    \n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        __syncthreads();\n        if (tid < stride && (tid + stride) < group_size) {\n            var_sum += var_sum;\n        }\n    }\n    \n    __syncthreads();\n    if (tid == 0) {\n        shared_data[1] = var_sum / group_size;\n    }\n    __syncthreads();\n    float stddev = sqrtf(shared_data[1] + 1e-5f);\n    \n    // Apply normalization\n    if (tid < group_size) {\n        int feature_idx = feature_offset + tid;\n        int output_idx = batch_idx * num_features + feature_idx;\n        output[output_idx] = gn_weight[feature_idx] * ((x[output_idx] - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, (group_size + 31) / 32 * 32);\n    size_t shared_mem_size = 2 * sizeof(float);\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    group_norm_kernel<<<blocks, threads, shared_mem_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), gn_bias.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, num_groups, group_size, batch_size * num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the group normalization by computing mean and variance per group in parallel using shared memory and warp-level reductions, then applying normalization to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_fp16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0)\n        shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0;\n    if (wid == 0)\n        val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    \n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_groups;\n\n    int tid = threadIdx.x;\n    int feature_offset = group_idx * group_size;\n    int batch_offset = batch_idx * num_features;\n    \n    // Compute mean\n    float mean = 0.0f;\n    if (tid < group_size) {\n        mean = x[batch_offset + feature_offset + tid];\n    }\n    mean = block_reduce_sum(mean);\n    if (threadIdx.x == 0) {\n        mean_shared[group_idx] = mean / group_size;\n    }\n    __syncthreads();\n    mean = mean_shared[group_idx];\n    \n    // Compute variance\n    float variance = 0.0f;\n    if (tid < group_size) {\n        float diff = x[batch_offset + feature_offset + tid] - mean;\n        variance = diff * diff;\n    }\n    variance = block_reduce_sum(variance);\n    if (threadIdx.x == 0) {\n        var_shared[group_idx] = variance / group_size;\n    }\n    __syncthreads();\n    float stddev = sqrtf(var_shared[group_idx] + 1e-5f);\n    \n    // Apply normalization\n    if (tid < group_size) {\n        int feature_idx = feature_offset + tid;\n        output[batch_offset + feature_idx] = \n            gn_weight[feature_idx] * ((x[batch_offset + feature_idx] - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = num_groups * 2 * sizeof(float);\n    \n    torch::Tensor output = torch::empty_like(x);\n    group_norm_kernel<<<blocks, threads, smem_size>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n                                                     gn_bias.data_ptr<float>(), output.data_ptr<float>(), \n                                                     batch_size, num_features, num_groups, group_size, \n                                                     batch_size * num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize by parallelizing mean and variance calculations per group using shared memory and warp-level reductions, then applying normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0;\n    if (wid == 0) val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, \n                                 int group_size, int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    int tid = threadIdx.x;\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_groups;\n\n    int feature_start = group_idx * group_size;\n    int feature_end = feature_start + group_size;\n    int num_active_threads = min(group_size, blockDim.x);\n\n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_start + i;\n        mean += x[batch_idx * num_features + feature_idx];\n    }\n    mean = block_reduce_sum(mean) / group_size;\n    if (tid == 0) mean_shared[group_idx] = mean;\n    __syncthreads();\n    mean = mean_shared[group_idx];\n\n    // Compute variance\n    float variance = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_start + i;\n        float diff = x[batch_idx * num_features + feature_idx] - mean;\n        variance += diff * diff;\n    }\n    variance = block_reduce_sum(variance) / group_size;\n    if (tid == 0) var_shared[group_idx] = variance;\n    __syncthreads();\n    variance = var_shared[group_idx];\n\n    // Apply normalization\n    float stddev = sqrtf(variance + 1e-5f);\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_start + i;\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    int num_elements = batch_size * num_features;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = 2 * num_groups * sizeof(float);\n\n    group_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, num_elements\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize by parallelizing mean and variance calculations per group using shared memory and warp-level reductions, then applying normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;\n    if (wid == 0) val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size, int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    \n    int group_start = group_idx * group_size;\n    int group_end = group_start + group_size;\n    \n    float mean = 0.0f, variance = 0.0f;\n    \n    // First pass: compute mean\n    for (int feature_idx = threadIdx.x + group_start; feature_idx < group_end; feature_idx += blockDim.x) {\n        float val = x[batch_idx * num_features + feature_idx];\n        mean += val;\n    }\n    mean = block_reduce_sum(mean) / group_size;\n    \n    // Second pass: compute variance\n    for (int feature_idx = threadIdx.x + group_start; feature_idx < group_end; feature_idx += blockDim.x) {\n        float val = x[batch_idx * num_features + feature_idx];\n        variance += (val - mean) * (val - mean);\n    }\n    variance = block_reduce_sum(variance) / group_size;\n    float stddev = sqrtf(variance + 1e-5f);\n    \n    // Apply normalization\n    for (int feature_idx = threadIdx.x + group_start; feature_idx < group_end; feature_idx += blockDim.x) {\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    dim3 blocks(batch_size, num_groups);\n    int threads = min(1024, group_size);\n    \n    group_norm_kernel<<<blocks, threads>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n                                         gn_bias.data_ptr<float>(), output.data_ptr<float>(), \n                                         batch_size, num_features, num_groups, group_size, batch_size * num_features);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize by parallelizing mean and variance calculations across threads within a group, using shared memory for intermediate results, and warp-level reductions for efficiency.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda_bf16.h>\n#include <cuda_fp16.h>\n\ntemplate<typename T>\n__device__ __inline__ T warp_reduce_sum(T val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + blockDim.x / 32;\n\n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int tid = threadIdx.x;\n    int lane = tid % 32;\n    int warp = tid / 32;\n\n    int group_start = group_idx * group_size;\n    int group_end = group_start + group_size;\n    int feature_idx = group_start + lane;\n\n    // Initialize accumulators\n    float mean = 0.0f, variance = 0.0f;\n\n    // Calculate mean\n    if (feature_idx < group_end) {\n        mean = x[batch_idx * num_features + feature_idx];\n    }\n    mean = warp_reduce_sum(mean);\n    if (lane == 0) {\n        mean_shared[warp] = mean;\n    }\n    __syncthreads();\n\n    if (warp == 0) {\n        mean = lane < blockDim.x / 32 ? mean_shared[lane] : 0.0f;\n        mean = warp_reduce_sum(mean) / group_size;\n        if (lane == 0) {\n            mean_shared[0] = mean;\n        }\n    }\n    __syncthreads();\n    mean = mean_shared[0];\n\n    // Calculate variance\n    if (feature_idx < group_end) {\n        float diff = x[batch_idx * num_features + feature_idx] - mean;\n        variance = diff * diff;\n    }\n    variance = warp_reduce_sum(variance);\n    if (lane == 0) {\n        var_shared[warp] = variance;\n    }\n    __syncthreads();\n\n    if (warp == 0) {\n        variance = lane < blockDim.x / 32 ? var_shared[lane] : 0.0f;\n        variance = warp_reduce_sum(variance) / group_size;\n        if (lane == 0) {\n            var_shared[0] = variance;\n        }\n    }\n    __syncthreads();\n    float stddev = sqrtf(var_shared[0] + 1e-5f);\n\n    // Apply normalization\n    if (feature_idx < group_end) {\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((x[batch_idx * num_features + feature_idx] - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(batch_size, num_groups);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = (threads / 32) * 2 * sizeof(float);\n\n    group_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, batch_size * num_features\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimized algorithm: Use parallel reduction within each thread block to compute mean and variance for each group, then apply normalization with cooperative loading of input data to minimize memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __inline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __inline__ float block_reduce_sum(float val, cg::thread_block &cta) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0)\n        shared[wid] = val;\n    \n    cg::sync(cta);\n\n    val = (threadIdx.x < (blockDim.x / 32)) ? shared[lane] : 0.0f;\n    val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size, \n                                 int num_elements) {\n    cg::thread_block cta = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    \n    int feature_start = group_idx * group_size;\n    int feature_end = feature_start + group_size;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int f = threadIdx.x; f < group_size; f += blockDim.x) {\n        int feature_idx = feature_start + f;\n        mean += x[batch_idx * num_features + feature_idx];\n    }\n    mean = block_reduce_sum(mean, cta) / group_size;\n    \n    // Compute variance\n    float variance = 0.0f;\n    for (int f = threadIdx.x; f < group_size; f += blockDim.x) {\n        int feature_idx = feature_start + f;\n        float diff = x[batch_idx * num_features + feature_idx] - mean;\n        variance += diff * diff;\n    }\n    variance = block_reduce_sum(variance, cta) / group_size;\n    float stddev = sqrtf(variance + 1e-5f);\n    \n    // Apply normalization\n    for (int f = threadIdx.x; f < group_size; f += blockDim.x) {\n        int feature_idx = feature_start + f;\n        float normalized = (x[batch_idx * num_features + feature_idx] - mean) / stddev;\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * normalized + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    \n    group_norm_kernel<<<blocks, threads>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n                                          gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n                                          batch_size, num_features, num_groups, group_size,\n                                          batch_size * num_features);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize by parallelizing mean and variance calculations across threads within a group, using shared memory for intermediate results, and reducing memory accesses through cooperative loading.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(block);\n    \n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int local_idx = threadIdx.x;\n    \n    if (batch_idx >= batch_size || group_idx >= num_groups) return;\n    \n    int group_start = group_idx * group_size;\n    int global_idx = batch_idx * num_features + group_start + local_idx;\n    \n    __shared__ float shared_mean;\n    __shared__ float shared_variance;\n    \n    // Load data and compute mean\n    float val = (local_idx < group_size) ? x[global_idx] : 0.0f;\n    float sum = cg::reduce(tile, val, cg::plus<float>());\n    \n    if (tile.thread_rank() == 0) {\n        atomicAdd(&shared_mean, sum);\n    }\n    block.sync();\n    \n    float mean = shared_mean / group_size;\n    \n    // Compute variance\n    float diff = (local_idx < group_size) ? (val - mean) : 0.0f;\n    float variance = cg::reduce(tile, diff * diff, cg::plus<float>());\n    \n    if (tile.thread_rank() == 0) {\n        atomicAdd(&shared_variance, variance);\n    }\n    block.sync();\n    \n    float stddev = sqrtf(shared_variance / group_size + 1e-5f);\n    \n    // Apply normalization\n    if (local_idx < group_size) {\n        output[global_idx] = gn_weight[group_start + local_idx] * ((val - mean) / stddev) + gn_bias[group_start + local_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    dim3 blocks(batch_size, num_groups);\n    int threads = (group_size + 31) / 32 * 32;\n    \n    torch::Tensor output = torch::empty_like(x);\n    group_norm_kernel<<<blocks, threads>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n                                          gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n                                          batch_size, num_features, num_groups, group_size, batch_size * num_features);\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize by computing mean and variance per group in parallel using shared memory and warp-level reductions, then normalize all features in the group using these precomputed statistics.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ inline float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ inline float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;\n    if (wid == 0) val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, \n                                 int group_size, int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n    \n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int feature_offset = group_idx * group_size;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = threadIdx.x; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_offset + i;\n        mean += x[batch_idx * num_features + feature_idx];\n    }\n    mean = warp_reduce_sum(mean);\n    mean = block_reduce_sum(mean) / group_size;\n    \n    // Compute variance\n    float variance = 0.0f;\n    for (int i = threadIdx.x; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_offset + i;\n        float diff = x[batch_idx * num_features + feature_idx] - mean;\n        variance += diff * diff;\n    }\n    variance = warp_reduce_sum(variance);\n    variance = block_reduce_sum(variance) / group_size;\n    float stddev = sqrtf(variance + 1e-5f);\n    \n    // Normalize features\n    for (int i = threadIdx.x; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_offset + i;\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    dim3 blocks(batch_size, num_groups);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    \n    group_norm_kernel<<<blocks, threads>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n                                         gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n                                         batch_size, num_features, num_groups, group_size, 0);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize by parallelizing mean and variance calculations per group using shared memory and warp-level reductions, then applying normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;\n    if (wid == 0) val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size, \n                                 int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    \n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_groups;\n\n    int feature_start = group_idx * group_size;\n    int feature_end = min(feature_start + group_size, num_features);\n    int actual_group_size = feature_end - feature_start;\n\n    float mean = 0.0f, variance = 0.0f;\n    for (int f = threadIdx.x; f < actual_group_size; f += blockDim.x) {\n        int feature_idx = feature_start + f;\n        float val = x[batch_idx * num_features + feature_idx];\n        mean += val;\n        variance += val * val;\n    }\n\n    mean = block_reduce_sum(mean);\n    variance = block_reduce_sum(variance);\n    \n    if (threadIdx.x == 0) {\n        mean /= actual_group_size;\n        variance = variance / actual_group_size - mean * mean;\n        mean_shared[group_idx] = mean;\n        var_shared[group_idx] = sqrtf(variance + 1e-5f);\n    }\n    __syncthreads();\n\n    mean = mean_shared[group_idx];\n    float stddev = var_shared[group_idx];\n\n    for (int f = threadIdx.x; f < actual_group_size; f += blockDim.x) {\n        int feature_idx = feature_start + f;\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    int num_elements = batch_size * num_features;\n\n    torch::Tensor output = torch::empty_like(x);\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = 2 * num_groups * sizeof(float);\n    \n    group_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, num_elements\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize by computing mean and variance per group in parallel using warp-level reductions and shared memory, then applying normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                  const float* __restrict__ gn_bias, float* __restrict__ output, \n                                  int batch_size, int num_features, int num_groups, \n                                  int group_size, int num_elements) {\n    extern __shared__ float smem[];\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n\n    int batch_idx = blockIdx.y;\n    int group_idx = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_idx * group_size + i;\n        mean += x[batch_idx * num_features + feature_idx];\n    }\n    mean = warp_reduce_sum(mean);\n    if (warp.meta_group_rank() == 0) {\n        smem[warp.meta_group_rank()] = mean;\n    }\n    block.sync();\n    \n    if (warp.meta_group_rank() == 0) {\n        mean = 0.0f;\n        for (int i = 0; i < warp.meta_group_size(); ++i) {\n            mean += smem[i];\n        }\n        mean /= group_size;\n        smem[0] = mean;\n    }\n    block.sync();\n    mean = smem[0];\n    \n    // Compute variance\n    float variance = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_idx * group_size + i;\n        float diff = x[batch_idx * num_features + feature_idx] - mean;\n        variance += diff * diff;\n    }\n    variance = warp_reduce_sum(variance);\n    if (warp.meta_group_rank() == 0) {\n        smem[warp.meta_group_rank()] = variance;\n    }\n    block.sync();\n    \n    if (warp.meta_group_rank() == 0) {\n        variance = 0.0f;\n        for (int i = 0; i < warp.meta_group_size(); ++i) {\n            variance += smem[i];\n        }\n        variance /= group_size;\n        smem[1] = variance;\n    }\n    block.sync();\n    variance = smem[1];\n    \n    float stddev = sqrtf(variance + 1e-5f);\n    \n    // Apply normalization\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_idx * group_size + i;\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = 2 * sizeof(float) * ((threads + 31) / 32);\n    \n    group_norm_kernel<<<blocks, threads, smem_size>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n                                                     gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n                                                     batch_size, num_features, num_groups, group_size, 0);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize by computing mean and variance per group in parallel using shared memory and warp-level reductions, then applying normalization to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val, const cg::thread_block &cta) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0)\n        shared[wid] = val;\n    \n    cg::sync(cta);\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0;\n    if (wid == 0)\n        val = warp_reduce_sum(val);\n    \n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size) {\n    cg::thread_block cta = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    int tid = threadIdx.x;\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + 1;\n\n    if (tid == 0) {\n        mean_shared[0] = 0.0f;\n        var_shared[0] = 0.0f;\n    }\n    cg::sync(cta);\n\n    float mean = 0.0f;\n    float variance = 0.0f;\n\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_idx * group_size + i;\n        float val = x[batch_idx * num_features + feature_idx];\n        mean += val;\n        variance += val * val;\n    }\n\n    mean = block_reduce_sum(mean, cta);\n    variance = block_reduce_sum(variance, cta);\n    \n    if (tid == 0) {\n        mean /= group_size;\n        variance = variance / group_size - mean * mean;\n        mean_shared[0] = mean;\n        var_shared[0] = sqrtf(variance + 1e-5f);\n    }\n    cg::sync(cta);\n\n    mean = mean_shared[0];\n    float stddev = var_shared[0];\n\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_idx * group_size + i;\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = 2 * sizeof(float);\n\n    group_norm_kernel<<<blocks, threads, smem_size>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n                                                    gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n                                                    batch_size, num_features, num_groups, group_size);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize by computing mean and variance per group in parallel using shared memory and warp-level reductions, then apply normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;\n    if (wid == 0) val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size, \n                                 int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int tid = threadIdx.x;\n\n    int group_start = group_idx * group_size;\n    int group_end = group_start + group_size;\n\n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_start + i;\n        mean += x[batch_idx * num_features + feature_idx];\n    }\n    mean = block_reduce_sum(mean) / group_size;\n\n    // Compute variance\n    float variance = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_start + i;\n        float diff = x[batch_idx * num_features + feature_idx] - mean;\n        variance += diff * diff;\n    }\n    variance = block_reduce_sum(variance) / group_size;\n    float stddev = sqrtf(variance + 1e-5f);\n\n    // Apply normalization\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_start + i;\n        int output_idx = batch_idx * num_features + feature_idx;\n        output[output_idx] = gn_weight[feature_idx] * ((x[output_idx] - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n    dim3 blocks(batch_size, num_groups);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n\n    group_norm_kernel<<<blocks, threads>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n                                          gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n                                          batch_size, num_features, num_groups, group_size,\n                                          batch_size * num_features);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize by computing mean and variance per group in parallel using shared memory and warp-level reductions, then applying normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;\n    if (wid == 0) val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                const float* __restrict__ gn_bias, float* __restrict__ output, \n                                int batch_size, int num_features, int num_groups, \n                                int group_size, int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(block);\n\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    int tid = threadIdx.x;\n\n    __shared__ float s_mean, s_variance;\n\n    if (group_idx >= num_groups || batch_idx >= batch_size) return;\n\n    const float* group_x = x + batch_idx * num_features + group_idx * group_size;\n    float sum = 0.0f, sq_sum = 0.0f;\n\n    // Compute sum and squared sum for mean/variance\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        float val = group_x[i];\n        sum += val;\n        sq_sum += val * val;\n    }\n\n    sum = block_reduce_sum(sum);\n    sq_sum = block_reduce_sum(sq_sum);\n\n    if (tid == 0) {\n        float mean = sum / group_size;\n        float variance = (sq_sum / group_size) - (mean * mean);\n        s_mean = mean;\n        s_variance = sqrtf(variance + 1e-5f);\n    }\n    block.sync();\n\n    // Apply normalization\n    if (tid < group_size) {\n        int feature_idx = group_idx * group_size + tid;\n        float val = group_x[tid];\n        float norm = (val - s_mean) / s_variance;\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * norm + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n\n    group_norm_kernel<<<blocks, threads>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n                                         gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n                                         batch_size, num_features, num_groups, group_size,\n                                         batch_size * num_features);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize by computing mean and variance per group in parallel using shared memory and warp-level reductions, then applying normalization with fused operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val, const cg::thread_block &cta) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0)\n        shared[wid] = val;\n    cg::sync(cta);\n\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, \n                                 int group_size, int num_elements) {\n    cg::thread_block cta = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_groups;\n\n    float mean = 0.0f;\n    float variance = 0.0f;\n\n    for (int i = threadIdx.x; i < group_size; i += blockDim.x) {\n        int feature_idx = group_idx * group_size + i;\n        float val = x[batch_idx * num_features + feature_idx];\n        mean += val;\n        variance += val * val;\n    }\n\n    mean = block_reduce_sum(mean, cta);\n    variance = block_reduce_sum(variance, cta);\n\n    if (threadIdx.x == 0) {\n        mean /= group_size;\n        variance = variance / group_size - mean * mean;\n        mean_shared[group_idx] = mean;\n        var_shared[group_idx] = sqrtf(variance + 1e-5f);\n    }\n    cg::sync(cta);\n\n    mean = mean_shared[group_idx];\n    float stddev = var_shared[group_idx];\n\n    for (int i = threadIdx.x; i < group_size; i += blockDim.x) {\n        int feature_idx = group_idx * group_size + i;\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    int num_elements = batch_size * num_features;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = 2 * num_groups * sizeof(float);\n\n    group_norm_kernel<<<blocks, threads, smem_size>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n                                                     gn_bias.data_ptr<float>(), output.data_ptr<float>(), \n                                                     batch_size, num_features, num_groups, group_size, num_elements);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the group normalization kernel by parallelizing mean and variance calculations across threads within a group, using shared memory for intermediate results, and reducing memory accesses through cooperative loading.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    cg::thread_block cta = cg::this_thread_block();\n    extern __shared__ float shared_data[];\n    \n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    int group_start = group_idx * group_size;\n    int feature_idx = group_start + tid;\n    \n    // Load data into shared memory\n    float val = 0.0f;\n    if (tid < group_size && feature_idx < num_features) {\n        val = x[batch_idx * num_features + feature_idx];\n    }\n    shared_data[tid] = val;\n    cta.sync();\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = 0; i < group_size; i += blockDim.x) {\n        if (tid + i < group_size) {\n            mean += shared_data[tid + i];\n        }\n    }\n    mean = cg::reduce(cta, mean, cg::plus<float>());\n    mean /= group_size;\n    cta.sync();\n    \n    // Compute variance\n    float variance = 0.0f;\n    for (int i = 0; i < group_size; i += blockDim.x) {\n        if (tid + i < group_size) {\n            float diff = shared_data[tid + i] - mean;\n            variance += diff * diff;\n        }\n    }\n    variance = cg::reduce(cta, variance, cg::plus<float>());\n    variance /= group_size;\n    float stddev = sqrtf(variance + 1e-5f);\n    cta.sync();\n    \n    // Apply normalization\n    if (tid < group_size && feature_idx < num_features) {\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    // Configure kernel launch\n    dim3 blocks(batch_size, num_groups);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t shared_mem = threads * sizeof(float);\n    \n    // Allocate output\n    torch::Tensor output = torch::empty_like(x);\n    \n    // Launch kernel\n    group_norm_kernel<<<blocks, threads, shared_mem>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n                                                     gn_bias.data_ptr<float>(), output.data_ptr<float>(), \n                                                     batch_size, num_features, num_groups, group_size, \n                                                     batch_size * num_features);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize by parallelizing within groups, using shared memory for mean/variance calculations, and warp-level reductions for efficiency while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size, \n                                 int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n    \n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    int lane = threadIdx.x % 32;\n    int warp_id = threadIdx.x / 32;\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + 1;\n\n    if (threadIdx.x == 0) {\n        mean_shared[0] = 0.0f;\n        var_shared[0] = 0.0f;\n    }\n    block.sync();\n\n    float mean = 0.0f, variance = 0.0f;\n    int group_start = batch_idx * num_features + group_idx * group_size;\n    int group_end = group_start + group_size;\n\n    // Compute mean\n    for (int i = warp_id * 32 + lane; i < group_size; i += blockDim.x) {\n        mean += x[group_start + i];\n    }\n    mean = warp_reduce_sum(mean);\n    if (warp.meta_group_rank() == 0) {\n        atomicAdd(mean_shared, mean);\n    }\n    block.sync();\n\n    mean = mean_shared[0] / group_size;\n    block.sync();\n\n    // Compute variance\n    for (int i = warp_id * 32 + lane; i < group_size; i += blockDim.x) {\n        float diff = x[group_start + i] - mean;\n        variance += diff * diff;\n    }\n    variance = warp_reduce_sum(variance);\n    if (warp.meta_group_rank() == 0) {\n        atomicAdd(var_shared, variance);\n    }\n    block.sync();\n\n    float stddev = sqrtf(var_shared[0] / group_size + 1e-5f);\n\n    // Apply normalization\n    for (int i = warp_id * 32 + lane; i < group_size; i += blockDim.x) {\n        int feature_idx = group_idx * group_size + i;\n        int out_idx = batch_idx * num_features + feature_idx;\n        output[out_idx] = gn_weight[feature_idx] * ((x[out_idx] - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(num_groups, batch_size);\n    int threads = 256;\n    size_t smem_size = 2 * sizeof(float); // for mean and variance\n\n    group_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), gn_bias.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, num_groups, group_size, \n        batch_size * num_features\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the group normalization kernel by computing mean and variance per group in parallel using shared memory and warp-level reductions, then applying normalization to each element.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0)\n        shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0;\n    if (wid == 0)\n        val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size, \n                                 int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    \n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_groups;\n\n    int tid = threadIdx.x;\n    int group_offset = group_idx * group_size;\n    int feature_idx = group_offset + tid;\n    \n    // Compute mean\n    float sum = 0.0f;\n    if (tid < group_size && feature_idx < num_features) {\n        sum = x[batch_idx * num_features + feature_idx];\n    }\n    sum = block_reduce_sum(sum);\n    if (tid == 0) {\n        mean_shared[group_idx] = sum / group_size;\n    }\n    block.sync();\n\n    // Compute variance\n    float mean = mean_shared[group_idx];\n    float var = 0.0f;\n    if (tid < group_size && feature_idx < num_features) {\n        float diff = x[batch_idx * num_features + feature_idx] - mean;\n        var = diff * diff;\n    }\n    var = block_reduce_sum(var);\n    if (tid == 0) {\n        var_shared[group_idx] = var / group_size;\n    }\n    block.sync();\n\n    // Apply normalization\n    if (tid < group_size && feature_idx < num_features) {\n        float stddev = sqrtf(var_shared[group_idx] + 1e-5f);\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((x[batch_idx * num_features + feature_idx] - mean) / stddev + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    int num_elements = batch_size * num_features;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = 2 * num_groups * sizeof(float);\n\n    group_norm_kernel<<<blocks, threads, smem_size>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n                                                     gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n                                                     batch_size, num_features, num_groups, group_size, num_elements);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize by parallelizing mean and variance calculations per group using shared memory and warp-level reductions, then applying normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0;\n    if (wid == 0) val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias,\n                                 float* output, int batch_size, int num_features,\n                                 int num_groups, int group_size, int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    int tid = threadIdx.x;\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_groups;\n\n    float mean = 0.0f, variance = 0.0f;\n    if (tid < group_size) {\n        int feature_idx = group_idx * group_size + tid;\n        float val = x[batch_idx * num_features + feature_idx];\n        mean += val;\n        variance += val * val;\n    }\n\n    mean = block_reduce_sum(mean);\n    variance = block_reduce_sum(variance);\n\n    if (tid == 0) {\n        mean /= group_size;\n        variance = variance / group_size - mean * mean;\n        mean_shared[group_idx] = mean;\n        var_shared[group_idx] = sqrtf(variance + 1e-5f);\n    }\n    block.sync();\n\n    if (tid < group_size) {\n        int feature_idx = group_idx * group_size + tid;\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean_shared[group_idx]) / var_shared[group_idx]) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, (group_size + 31) / 32 * 32);\n    size_t smem_size = num_groups * 2 * sizeof(float);\n\n    group_norm_kernel<<<blocks, threads, smem_size>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n                                                     gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n                                                     batch_size, num_features, num_groups, group_size, batch_size * num_features);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize by computing mean and variance per group in parallel using shared memory, then applying normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias,\n                                 float* output, int batch_size, int num_features,\n                                 int num_groups, int group_size, int num_elements) {\n    extern __shared__ float shared_data[];\n    \n    int batch_idx = blockIdx.y;\n    int group_idx = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    int group_start = group_idx * group_size;\n    int group_end = group_start + group_size;\n    \n    // Compute mean\n    float sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_start + i;\n        sum += x[batch_idx * num_features + feature_idx];\n    }\n    \n    shared_data[tid] = sum;\n    __syncthreads();\n    \n    // Parallel reduction for mean\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    float mean = shared_data[0] / group_size;\n    __syncthreads();\n    \n    // Compute variance\n    float var_sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_start + i;\n        float diff = x[batch_idx * num_features + feature_idx] - mean;\n        var_sum += diff * diff;\n    }\n    \n    shared_data[tid] = var_sum;\n    __syncthreads();\n    \n    // Parallel reduction for variance\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            shared_data[tid] += shared_data[tid + s];\n        }\n        __syncthreads();\n    }\n    \n    float variance = shared_data[0] / group_size;\n    float stddev = sqrtf(variance + 1e-5f);\n    \n    // Apply normalization\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_start + i;\n        float normalized = (x[batch_idx * num_features + feature_idx] - mean) / stddev;\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * normalized + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, group_size);\n    size_t shared_mem = threads * sizeof(float);\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    group_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, batch_size * num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize by computing group statistics (mean/variance) in parallel using shared memory and warp-level reductions, then applying normalization to each element with proper synchronization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val, const cg::thread_block &cta) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0)\n        shared[wid] = val;\n    \n    cg::sync(cta);\n\n    if (wid == 0)\n        val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0;\n    val = warp_reduce_sum(val);\n    \n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size, \n                                 int num_elements) {\n    cg::thread_block cta = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    int tid = threadIdx.x;\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + 1;\n\n    float mean = 0.0f;\n    float var = 0.0f;\n\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = group_idx * group_size + i;\n        float val = x[batch_idx * num_features + feature_idx];\n        mean += val;\n        var += val * val;\n    }\n\n    mean = block_reduce_sum(mean, cta);\n    var = block_reduce_sum(var, cta);\n\n    if (tid == 0) {\n        mean /= group_size;\n        var = var / group_size - mean * mean;\n        mean_shared[0] = mean;\n        var_shared[0] = sqrtf(var + 1e-5f);\n    }\n\n    cg::sync(cta);\n\n    mean = mean_shared[0];\n    float stddev = var_shared[0];\n\n    if (tid < group_size) {\n        int feature_idx = group_idx * group_size + tid;\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = 2 * sizeof(float);\n\n    group_norm_kernel<<<blocks, threads, smem_size>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(),\n                                                    gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n                                                    batch_size, num_features, num_groups, group_size,\n                                                    batch_size * num_features);\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize by parallelizing mean and variance calculations per group using shared memory and warp-level reductions, then applying normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__inline__ __device__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    extern __shared__ float shared_data[];\n    \n    int batch_idx = blockIdx.y;\n    int group_idx = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    int feature_start = group_idx * group_size;\n    int feature_end = feature_start + group_size;\n    \n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_start + i;\n        mean += x[batch_idx * num_features + feature_idx];\n    }\n    mean = warp_reduce_sum(mean);\n    if (tid % 32 == 0) {\n        shared_data[tid / 32] = mean;\n    }\n    __syncthreads();\n    \n    if (tid < 32) {\n        mean = (tid < blockDim.x / 32) ? shared_data[tid] : 0.0f;\n        mean = warp_reduce_sum(mean);\n    }\n    mean /= group_size;\n    \n    // Compute variance\n    float variance = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_start + i;\n        float diff = x[batch_idx * num_features + feature_idx] - mean;\n        variance += diff * diff;\n    }\n    variance = warp_reduce_sum(variance);\n    if (tid % 32 == 0) {\n        shared_data[tid / 32] = variance;\n    }\n    __syncthreads();\n    \n    if (tid < 32) {\n        variance = (tid < blockDim.x / 32) ? shared_data[tid] : 0.0f;\n        variance = warp_reduce_sum(variance);\n    }\n    variance /= group_size;\n    float stddev = sqrtf(variance + 1e-5f);\n    \n    // Apply normalization\n    if (tid < group_size) {\n        int feature_idx = feature_start + tid;\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    int shared_size = ((threads + 31) / 32) * sizeof(float);\n    \n    group_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n        gn_bias.data_ptr<float>(), output.data_ptr<float>(), \n        batch_size, num_features, num_groups, group_size, 0);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimized implementation using parallel reduction within each group to compute mean and variance, followed by element-wise normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\ntemplate <int GROUP_SIZE>\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    extern __shared__ float shared_data[];\n    \n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    int group_offset = group_idx * group_size;\n    int batch_offset = batch_idx * num_features;\n    \n    // Load data into shared memory\n    if (tid < group_size) {\n        shared_data[tid] = x[batch_offset + group_offset + tid];\n    }\n    __syncthreads();\n    \n    // Parallel reduction for mean\n    float sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        sum += shared_data[i];\n    }\n    \n    typedef cub::BlockReduce<float, 256> BlockReduce;\n    __shared__ typename BlockReduce::TempStorage temp_storage;\n    float mean = BlockReduce(temp_storage).Sum(sum) / group_size;\n    __syncthreads();\n    \n    // Parallel reduction for variance\n    float var_sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        float diff = shared_data[i] - mean;\n        var_sum += diff * diff;\n    }\n    \n    float variance = BlockReduce(temp_storage).Sum(var_sum) / group_size;\n    float stddev = sqrtf(variance + 1e-5f);\n    __syncthreads();\n    \n    // Normalize and store\n    if (tid < group_size) {\n        int feature_idx = group_offset + tid;\n        output[batch_offset + feature_idx] = \n            gn_weight[feature_idx] * ((shared_data[tid] - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    dim3 blocks(batch_size, num_groups);\n    int threads = min(256, group_size);\n    size_t shared_size = group_size * sizeof(float);\n    \n    if (group_size == 32) {\n        group_norm_kernel<32><<<blocks, threads, shared_size>>>(\n            x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n            gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n            batch_size, num_features, num_groups, group_size, batch_size * num_features);\n    } else if (group_size == 64) {\n        group_norm_kernel<64><<<blocks, threads, shared_size>>>(\n            x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n            gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n            batch_size, num_features, num_groups, group_size, batch_size * num_features);\n    } else {\n        group_norm_kernel<0><<<blocks, threads, shared_size>>>(\n            x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n            gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n            batch_size, num_features, num_groups, group_size, batch_size * num_features);\n    }\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize by computing group statistics (mean/variance) in parallel using shared memory and warp-level reductions, then applying normalization with fused operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val, const cg::thread_block &cta) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0)\n        shared[wid] = val;\n    \n    cg::sync(cta);\n\n    if (wid == 0)\n        val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0;\n    val = warp_reduce_sum(val);\n    \n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                  const float* __restrict__ gn_bias, float* __restrict__ output, \n                                  int batch_size, int num_features, int num_groups, int group_size, \n                                  int num_elements) {\n    cg::thread_block cta = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    \n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + 1;\n\n    int group_start = group_idx * group_size;\n    int group_end = group_start + group_size;\n    \n    float mean = 0.0f, var = 0.0f;\n    for (int f = threadIdx.x; f < group_size; f += blockDim.x) {\n        int feature_idx = group_start + f;\n        float val = x[batch_idx * num_features + feature_idx];\n        mean += val;\n        var += val * val;\n    }\n    \n    mean = block_reduce_sum(mean, cta);\n    var = block_reduce_sum(var, cta);\n    \n    if (threadIdx.x == 0) {\n        mean /= group_size;\n        var = var / group_size - mean * mean;\n        mean_shared[0] = mean;\n        var_shared[0] = sqrtf(var + 1e-5f);\n    }\n    \n    cg::sync(cta);\n    \n    mean = mean_shared[0];\n    float inv_std = 1.0f / var_shared[0];\n    \n    for (int f = threadIdx.x; f < group_size; f += blockDim.x) {\n        int feature_idx = group_start + f;\n        float val = x[batch_idx * num_features + feature_idx];\n        float norm = (val - mean) * inv_std;\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * norm + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t smem_size = 2 * sizeof(float);\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    group_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, \n        batch_size * num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize by parallelizing mean and variance calculations across threads within each group and using shared memory for efficient data access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    extern __shared__ float shared_data[];\n    \n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    int group_offset = group_idx * group_size;\n    int batch_offset = batch_idx * num_features;\n    \n    // Compute mean\n    float sum = 0.0f;\n    if (tid < group_size) {\n        sum = x[batch_offset + group_offset + tid];\n    }\n    \n    for (int offset = blockDim.x / 2; offset > 0; offset >>= 1) {\n        if (tid < offset && tid + offset < group_size) {\n            sum += x[batch_offset + group_offset + tid + offset];\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        float mean = sum / group_size;\n        shared_data[0] = mean;\n    }\n    __syncthreads();\n    float mean = shared_data[0];\n    \n    // Compute variance\n    float var_sum = 0.0f;\n    if (tid < group_size) {\n        float diff = x[batch_offset + group_offset + tid] - mean;\n        var_sum = diff * diff;\n    }\n    \n    for (int offset = blockDim.x / 2; offset > 0; offset >>= 1) {\n        if (tid < offset && tid + offset < group_size) {\n            var_sum += var_sum + x[batch_offset + group_offset + tid + offset] - mean;\n        }\n        __syncthreads();\n    }\n    \n    if (tid == 0) {\n        float variance = var_sum / group_size;\n        shared_data[1] = sqrtf(variance + 1e-5f);\n    }\n    __syncthreads();\n    float stddev = shared_data[1];\n    \n    // Apply normalization\n    if (tid < group_size) {\n        int feature_idx = group_offset + tid;\n        output[batch_offset + feature_idx] = \n            gn_weight[feature_idx] * ((x[batch_offset + feature_idx] - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    dim3 blocks(batch_size, num_groups);\n    int threads = min(1024, (group_size + 31) / 32 * 32);\n    size_t shared_size = 2 * sizeof(float);\n    \n    torch::Tensor output = torch::empty_like(x);\n    \n    group_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, batch_size * num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize by parallelizing mean and variance calculations across threads within a group, using shared memory for intermediate results, and reducing memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    extern __shared__ float shared_data[];\n    \n    int batch_idx = blockIdx.x;\n    int group_idx = blockIdx.y;\n    int tid = threadIdx.x;\n    \n    int feature_offset = group_idx * group_size;\n    int batch_offset = batch_idx * num_features;\n    \n    // Load data into shared memory\n    if (tid < group_size) {\n        shared_data[tid] = x[batch_offset + feature_offset + tid];\n    }\n    __syncthreads();\n    \n    // Compute mean\n    float sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        sum += shared_data[i];\n    }\n    \n    __shared__ float mean_shared;\n    float mean = block_reduce_sum(sum) / group_size;\n    if (tid == 0) {\n        mean_shared = mean;\n    }\n    __syncthreads();\n    \n    // Compute variance\n    float var_sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        float diff = shared_data[i] - mean_shared;\n        var_sum += diff * diff;\n    }\n    \n    __shared__ float variance_shared;\n    float variance = block_reduce_sum(var_sum) / group_size;\n    if (tid == 0) {\n        variance_shared = variance;\n    }\n    __syncthreads();\n    \n    // Normalize and store\n    float stddev = sqrtf(variance_shared + 1e-5f);\n    if (tid < group_size) {\n        int feature_idx = feature_offset + tid;\n        output[batch_offset + feature_idx] = \n            gn_weight[feature_idx] * ((shared_data[tid] - mean_shared) / stddev) + gn_bias[feature_idx];\n    }\n}\n\n// Helper function for block-wide sum reduction\n__device__ float block_reduce_sum(float val) {\n    __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n    \n    val = warp_reduce_sum(val);\n    \n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n    \n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        val = warp_reduce_sum(val);\n    }\n    return val;\n}\n\n// Helper function for warp-wide sum reduction\n__device__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    }\n    return val;\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n    \n    // Allocate memory for output tensor\n    torch::Tensor output = torch::empty_like(x);\n    \n    // Launch kernel\n    dim3 blocks(batch_size, num_groups);\n    int threads = min(1024, ((group_size + 31) / 32) * 32);\n    size_t shared_size = group_size * sizeof(float);\n    \n    group_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, batch_size * num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize by parallelizing mean and variance calculations per group using shared memory and warp-level reductions, then applying normalization with proper synchronization.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val, const cg::thread_block& tb) {\n    static __shared__ float shared[32];\n    int lane = tb.thread_rank() % 32;\n    int wid = tb.thread_rank() / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0)\n        shared[wid] = val;\n    tb.sync();\n\n    if (wid == 0) {\n        val = (tb.thread_rank() < tb.size() / 32) ? shared[lane] : 0.0f;\n        val = warp_reduce_sum(val);\n    }\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, int group_size, \n                                 int num_elements) {\n    cg::thread_block tb = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n\n    int feature_start = group_idx * group_size;\n    int feature_end = min(feature_start + group_size, num_features);\n\n    float mean = 0.0f, variance = 0.0f;\n    for (int f = feature_start + threadIdx.x; f < feature_end; f += blockDim.x) {\n        float val = x[batch_idx * num_features + f];\n        mean += val;\n        variance += val * val;\n    }\n\n    mean = block_reduce_sum(mean, tb);\n    variance = block_reduce_sum(variance, tb);\n\n    if (tb.thread_rank() == 0) {\n        mean /= group_size;\n        variance = variance / group_size - mean * mean;\n    }\n\n    tb.sync();\n\n    float inv_std = rsqrtf(variance + 1e-5f);\n    float group_mean = mean;\n    \n    for (int f = feature_start + threadIdx.x; f < feature_end; f += blockDim.x) {\n        float val = x[batch_idx * num_features + f];\n        output[batch_idx * num_features + f] = \n            gn_weight[f] * ((val - group_mean) * inv_std) + gn_bias[f];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, group_size);\n    \n    group_norm_kernel<<<blocks, threads>>>(x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n                                          gn_bias.data_ptr<float>(), output.data_ptr<float>(), \n                                          batch_size, num_features, num_groups, group_size, \n                                          batch_size * num_features);\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize by computing mean and variance per group in parallel using shared memory and warp-level reductions, then apply normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __inline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __inline__ float block_reduce_sum(float val) {\n    static __shared__ float shared[32];\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0)\n        shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;\n    if (wid == 0)\n        val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* x, const float* gn_weight, const float* gn_bias, \n                                 float* output, int batch_size, int num_features, \n                                 int num_groups, int group_size, int num_elements) {\n    cg::thread_block block = cg::this_thread_block();\n    int group_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    \n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_groups;\n\n    int tid = threadIdx.x;\n    int feature_offset = group_idx * group_size;\n    \n    // Compute mean\n    float sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_offset + i;\n        sum += x[batch_idx * num_features + feature_idx];\n    }\n    \n    sum = block_reduce_sum(sum);\n    if (tid == 0) {\n        mean_shared[group_idx] = sum / group_size;\n    }\n    block.sync();\n    \n    // Compute variance\n    float mean = mean_shared[group_idx];\n    float var_sum = 0.0f;\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_offset + i;\n        float diff = x[batch_idx * num_features + feature_idx] - mean;\n        var_sum += diff * diff;\n    }\n    \n    var_sum = block_reduce_sum(var_sum);\n    if (tid == 0) {\n        var_shared[group_idx] = var_sum / group_size;\n    }\n    block.sync();\n    \n    // Apply normalization\n    float stddev = sqrtf(var_shared[group_idx] + 1e-5f);\n    for (int i = tid; i < group_size; i += blockDim.x) {\n        int feature_idx = feature_offset + i;\n        float val = x[batch_idx * num_features + feature_idx];\n        output[batch_idx * num_features + feature_idx] = \n            gn_weight[feature_idx] * ((val - mean) / stddev) + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n    \n    dim3 blocks(num_groups, batch_size);\n    int threads = min(1024, group_size);\n    size_t smem_size = 2 * num_groups * sizeof(float);\n    \n    group_norm_kernel<<<blocks, threads, smem_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, batch_size * num_features\n    );\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize by computing mean and variance per group in parallel using shared memory and warp-level reductions, then apply normalization with coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_bf16.h>\n\nnamespace cg = cooperative_groups;\n\n__device__ __forceinline__ float warp_reduce_sum(float val) {\n    for (int offset = 16; offset > 0; offset /= 2)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n    return val;\n}\n\n__device__ __forceinline__ float block_reduce_sum(float val, float* shared) {\n    int lane = threadIdx.x % 32;\n    int wid = threadIdx.x / 32;\n\n    val = warp_reduce_sum(val);\n\n    if (lane == 0)\n        shared[wid] = val;\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;\n    if (wid == 0)\n        val = warp_reduce_sum(val);\n    return val;\n}\n\n__global__ void group_norm_kernel(const float* __restrict__ x, const float* __restrict__ gn_weight, \n                                 const float* __restrict__ gn_bias, float* __restrict__ output, \n                                 int batch_size, int num_features, int num_groups, \n                                 int group_size, int num_elements) {\n    extern __shared__ float shared[];\n\n    int batch_idx = blockIdx.y;\n    int group_idx = blockIdx.x;\n    int tid = threadIdx.x;\n\n    int feature_offset = group_idx * group_size;\n    int batch_offset = batch_idx * num_features;\n    int total_features = min(group_size, num_features - feature_offset);\n\n    // Compute mean\n    float mean = 0.0f;\n    for (int i = tid; i < total_features; i += blockDim.x) {\n        int feature_idx = feature_offset + i;\n        mean += x[batch_offset + feature_idx];\n    }\n    mean = block_reduce_sum(mean, shared);\n    mean /= total_features;\n\n    // Compute variance\n    float variance = 0.0f;\n    for (int i = tid; i < total_features; i += blockDim.x) {\n        int feature_idx = feature_offset + i;\n        float diff = x[batch_offset + feature_idx] - mean;\n        variance += diff * diff;\n    }\n    variance = block_reduce_sum(variance, shared);\n    variance /= total_features;\n    float stddev = sqrtf(variance + 1e-5f);\n\n    // Normalize and store\n    for (int i = tid; i < total_features; i += blockDim.x) {\n        int feature_idx = feature_offset + i;\n        float val = (x[batch_offset + feature_idx] - mean) / stddev;\n        output[batch_offset + feature_idx] = gn_weight[feature_idx] * val + gn_bias[feature_idx];\n    }\n}\n\ntorch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gn_weight, torch::Tensor gn_bias, int num_groups) {\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int group_size = num_features / num_groups;\n\n    torch::Tensor output = torch::empty_like(x);\n\n    dim3 blocks(num_groups, batch_size);\n    int threads = 256;\n    size_t shared_size = (threads / 32) * sizeof(float);\n\n    group_norm_kernel<<<blocks, threads, shared_size>>>(\n        x.data_ptr<float>(), gn_weight.data_ptr<float>(), \n        gn_bias.data_ptr<float>(), output.data_ptr<float>(),\n        batch_size, num_features, num_groups, group_size, \n        batch_size * num_features\n    );\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &group_norm_cuda, \"Group Normalization (CUDA)\");\n}",
        "score": null
    }
]