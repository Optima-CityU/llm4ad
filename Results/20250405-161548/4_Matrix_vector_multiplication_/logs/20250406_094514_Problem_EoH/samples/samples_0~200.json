[
    {
        "sample_order": 1,
        "algorithm": "{Optimize by using shared memory to cache the vector B and increase thread utilization through tiled matrix multiplication with loop unrolling.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache the vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n\n    if (row < M) {\n        float sum = 0.0f;\n        #pragma unroll(4)\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize by using shared memory to cache the vector B, allowing for coalesced global memory access and reduced redundant loads across threads.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize by using shared memory to cache vector B and unroll the inner loop to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n\n    if (row < M) {\n        float sum = 0.0f;\n        const float* A_row = &A[row * K];\n        #pragma unroll 4\n        for (int k = 0; k < K; ++k) {\n            sum += A_row[k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize by using shared memory to cache the vector B and unrolling the inner loop to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        const float* A_row = &A[row * K];\n        \n        // Unroll the loop by 4\n        int k = 0;\n        for (; k + 3 < K; k += 4) {\n            sum += A_row[k] * shared_B[k] +\n                   A_row[k+1] * shared_B[k+1] +\n                   A_row[k+2] * shared_B[k+2] +\n                   A_row[k+3] * shared_B[k+3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A_row[k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B, reducing global memory accesses, and increasing thread utilization by processing multiple elements per thread.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int tid = threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = tid; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize by using shared memory to cache vector B and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load vector B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        const float* A_row = A + row * K;\n        \n        // Unroll loop by 4\n        int k = 0;\n        for (; k <= K - 4; k += 4) {\n            sum += A_row[k] * shared_B[k] + \n                   A_row[k+1] * shared_B[k+1] + \n                   A_row[k+2] * shared_B[k+2] + \n                   A_row[k+3] * shared_B[k+3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A_row[k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize by using shared memory to cache the vector B and parallelize the reduction across threads within a warp for each row of the matrix A.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n\n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n\n    // Load B into shared memory\n    for (int k = tid; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n\n    // Process 4 rows per thread\n    for (int i = 0; i < 4; ++i) {\n        if (row + i * blockDim.x < M) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; ++k) {\n                sum += A[(row + i * blockDim.x) * K + k] * B_shared[k];\n            }\n            C[row + i * blockDim.x] = sum;\n        }\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads * 4 - 1) / (threads * 4);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the matrix-vector multiplication by using shared memory to cache the vector B, allowing for coalesced memory accesses and reducing global memory bandwidth usage.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the matrix-vector multiplication by using shared memory to cache the vector B and parallelizing the reduction across warps for better memory access patterns and reduced global memory bandwidth.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int i = threadIdx.x; i < K; i += blockDim.x) {\n        shared_B[i] = B[i];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize by using shared memory to cache the vector B and parallelize the reduction across threads within a block for better memory access patterns and reduced global memory bandwidth.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize by using shared memory to cache the vector B and parallelize the reduction across threads within a block for better memory access patterns and reduced global memory bandwidth.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize by using shared memory to cache vector B and unrolling the inner loop for better memory access patterns and reduced loop overhead.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n\n    if (row < M) {\n        float sum = 0.0f;\n        const float* A_row = A + row * K;\n        \n        // Unroll the loop by 4 for better performance\n        int k = 0;\n        for (; k + 3 < K; k += 4) {\n            sum += A_row[k] * shared_B[k] + \n                   A_row[k+1] * shared_B[k+1] + \n                   A_row[k+2] * shared_B[k+2] + \n                   A_row[k+3] * shared_B[k+3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A_row[k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize by using shared memory to cache the vector B, reducing global memory accesses, and increasing thread utilization through warp-level operations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize by using shared memory to cache vector B, reduce global memory accesses, and increase thread utilization through block-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = tid; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    if (row < M) {\n        float sum0 = 0.0f, sum1 = 0.0f, sum2 = 0.0f, sum3 = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            float b_val = B_shared[k];\n            sum0 += A[row * K + k] * b_val;\n            sum1 += A[(row + blockDim.x) * K + k] * b_val;\n            sum2 += A[(row + 2 * blockDim.x) * K + k] * b_val;\n            sum3 += A[(row + 3 * blockDim.x) * K + k] * b_val;\n        }\n        C[row] = sum0;\n        if (row + blockDim.x < M) C[row + blockDim.x] = sum1;\n        if (row + 2 * blockDim.x < M) C[row + 2 * blockDim.x] = sum2;\n        if (row + 3 * blockDim.x < M) C[row + 3 * blockDim.x] = sum3;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + 4 * threads - 1) / (4 * threads);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B, allowing for coalesced memory accesses and reduced global memory bandwidth usage.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int i = threadIdx.x; i < K; i += blockDim.x) {\n        shared_B[i] = B[i];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int i = tid; i < K; i += blockDim.x) {\n        B_shared[i] = B[i];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    if (row < M) {\n        float sum0 = 0.0f, sum1 = 0.0f, sum2 = 0.0f, sum3 = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            float b_val = B_shared[k];\n            sum0 += A[row * K + k] * b_val;\n            if (row + blockDim.x < M) sum1 += A[(row + blockDim.x) * K + k] * b_val;\n            if (row + 2 * blockDim.x < M) sum2 += A[(row + 2 * blockDim.x) * K + k] * b_val;\n            if (row + 3 * blockDim.x < M) sum3 += A[(row + 3 * blockDim.x) * K + k] * b_val;\n        }\n        C[row] = sum0;\n        if (row + blockDim.x < M) C[row + blockDim.x] = sum1;\n        if (row + 2 * blockDim.x < M) C[row + 2 * blockDim.x] = sum2;\n        if (row + 3 * blockDim.x < M) C[row + 3 * blockDim.x] = sum3;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + 4 * threads - 1) / (4 * threads);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = tid; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    if (row < M) {\n        float sum0 = 0.0f, sum1 = 0.0f, sum2 = 0.0f, sum3 = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            float b_val = shared_B[k];\n            sum0 += A[row * K + k] * b_val;\n            sum1 += A[(row + blockDim.x) * K + k] * b_val;\n            sum2 += A[(row + 2 * blockDim.x) * K + k] * b_val;\n            sum3 += A[(row + 3 * blockDim.x) * K + k] * b_val;\n        }\n        C[row] = sum0;\n        if (row + blockDim.x < M) C[row + blockDim.x] = sum1;\n        if (row + 2 * blockDim.x < M) C[row + 2 * blockDim.x] = sum2;\n        if (row + 3 * blockDim.x < M) C[row + 3 * blockDim.x] = sum3;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads * 4 - 1) / (threads * 4);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce memory bandwidth requirements.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n\n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int i = tid; i < K; i += blockDim.x) {\n        shared_B[i] = B[i];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    for (int r = 0; r < 4; ++r) {\n        if (row < M) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; ++k) {\n                sum += A[row * K + k] * shared_B[k];\n            }\n            C[row] = sum;\n            row += blockDim.x;\n        }\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads * 4 - 1) / (threads * 4);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B, reducing global memory accesses, and processing multiple elements per thread to increase arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int tid = threadIdx.x;\n    \n    // Load B into shared memory\n    for (int i = tid; i < K; i += blockDim.x) {\n        B_shared[i] = B[i];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize by using shared memory to cache the vector B and employ thread coarsening to reduce global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = tid; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    if (row < M) {\n        float sum0 = 0.0f, sum1 = 0.0f, sum2 = 0.0f, sum3 = 0.0f;\n        \n        for (int k = 0; k < K; ++k) {\n            float b_val = B_shared[k];\n            sum0 += A[row * K + k] * b_val;\n            if (row + blockDim.x < M) sum1 += A[(row + blockDim.x) * K + k] * b_val;\n            if (row + 2 * blockDim.x < M) sum2 += A[(row + 2 * blockDim.x) * K + k] * b_val;\n            if (row + 3 * blockDim.x < M) sum3 += A[(row + 3 * blockDim.x) * K + k] * b_val;\n        }\n        \n        C[row] = sum0;\n        if (row + blockDim.x < M) C[row + blockDim.x] = sum1;\n        if (row + 2 * blockDim.x < M) C[row + 2 * blockDim.x] = sum2;\n        if (row + 3 * blockDim.x < M) C[row + 3 * blockDim.x] = sum3;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + 4 * threads - 1) / (4 * threads);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.y + threadIdx.y;\n    \n    // Load B into shared memory\n    for (int i = tid; i < K; i += blockDim.x) {\n        shared_B[i] = B[i];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    dim3 threads(32, 8);  // 256 threads total\n    dim3 blocks((M + threads.y - 1) / threads.y);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce memory access overhead.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + tid;\n    \n    // Load B into shared memory\n    for (int k = tid; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    float sum[4] = {0.0f, 0.0f, 0.0f, 0.0f};\n    for (int k = 0; k < K; ++k) {\n        float b_val = B_shared[k];\n        if (row < M) sum[0] += A[row * K + k] * b_val;\n        if (row + blockDim.x < M) sum[1] += A[(row + blockDim.x) * K + k] * b_val;\n        if (row + 2 * blockDim.x < M) sum[2] += A[(row + 2 * blockDim.x) * K + k] * b_val;\n        if (row + 3 * blockDim.x < M) sum[3] += A[(row + 3 * blockDim.x) * K + k] * b_val;\n    }\n    \n    if (row < M) C[row] = sum[0];\n    if (row + blockDim.x < M) C[row + blockDim.x] = sum[1];\n    if (row + 2 * blockDim.x < M) C[row + 2 * blockDim.x] = sum[2];\n    if (row + 3 * blockDim.x < M) C[row + 3 * blockDim.x] = sum[3];\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + 4 * threads - 1) / (4 * threads);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the matrix-vector multiplication by using shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = tid; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    if (row < M) {\n        float sum0 = 0.0f, sum1 = 0.0f, sum2 = 0.0f, sum3 = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            float b_val = B_shared[k];\n            sum0 += A[row * K + k] * b_val;\n            sum1 += A[(row + blockDim.x) * K + k] * b_val;\n            sum2 += A[(row + 2 * blockDim.x) * K + k] * b_val;\n            sum3 += A[(row + 3 * blockDim.x) * K + k] * b_val;\n        }\n        C[row] = sum0;\n        if (row + blockDim.x < M) C[row + blockDim.x] = sum1;\n        if (row + 2 * blockDim.x < M) C[row + 2 * blockDim.x] = sum2;\n        if (row + 3 * blockDim.x < M) C[row + 3 * blockDim.x] = sum3;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads * 4 - 1) / (threads * 4);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize by using shared memory to cache vector B and unrolling the inner loop for better memory access patterns and reduced instruction overhead.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n\n    if (row < M) {\n        float sum = 0.0f;\n        const float* A_row = A + row * K;\n        \n        // Unroll the loop by 4 for better performance\n        int k = 0;\n        for (; k + 3 < K; k += 4) {\n            sum += A_row[k] * B_shared[k] + \n                   A_row[k+1] * B_shared[k+1] + \n                   A_row[k+2] * B_shared[k+2] + \n                   A_row[k+3] * B_shared[k+3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A_row[k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimized algorithm using shared memory to cache vector B and tiling for better memory access patterns while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize by using shared memory to cache the vector B and employ thread coarsening to reduce global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int i = tid; i < K; i += blockDim.x) {\n        B_shared[i] = B[i];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    for (int i = 0; i < 4; ++i) {\n        if (row < M) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; ++k) {\n                sum += A[row * K + k] * B_shared[k];\n            }\n            C[row] = sum;\n            row += blockDim.x;\n        }\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads * 4 - 1) / (threads * 4);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the matrix-vector multiplication by using shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    for (int i = 0; i < 4; ++i) {\n        if (row < M) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; ++k) {\n                sum += A[row * K + k] * shared_B[k];\n            }\n            C[row] = sum;\n            row += blockDim.x;\n        }\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads * 4 - 1) / (threads * 4);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the kernel by using shared memory to cache the vector B, allowing for coalesced memory access and reducing global memory bandwidth usage.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B, allowing for coalesced memory access and reduced global memory bandwidth usage.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B, reducing global memory accesses, and increasing thread utilization through warp-level operations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize by using shared memory to cache vector B and parallelize the reduction across threads within a warp for each row.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x;\n    int tid = threadIdx.x;\n    \n    // Load B into shared memory\n    for (int i = tid; i < K; i += blockDim.x) {\n        B_shared[i] = B[i];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = M;\n    const size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + tid;\n    \n    // Load B into shared memory\n    for (int k = tid; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    float sum[4] = {0.0f, 0.0f, 0.0f, 0.0f};\n    for (int k = 0; k < K; ++k) {\n        float b_val = B_shared[k];\n        if (row < M) sum[0] += A[row * K + k] * b_val;\n        if (row + blockDim.x < M) sum[1] += A[(row + blockDim.x) * K + k] * b_val;\n        if (row + 2 * blockDim.x < M) sum[2] += A[(row + 2 * blockDim.x) * K + k] * b_val;\n        if (row + 3 * blockDim.x < M) sum[3] += A[(row + 3 * blockDim.x) * K + k] * b_val;\n    }\n    \n    if (row < M) C[row] = sum[0];\n    if (row + blockDim.x < M) C[row + blockDim.x] = sum[1];\n    if (row + 2 * blockDim.x < M) C[row + 2 * blockDim.x] = sum[2];\n    if (row + 3 * blockDim.x < M) C[row + 3 * blockDim.x] = sum[3];\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads * 4 - 1) / (threads * 4);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = tid; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    if (row < M) {\n        float sum0 = 0.0f, sum1 = 0.0f, sum2 = 0.0f, sum3 = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            float b_val = B_shared[k];\n            sum0 += A[row * K + k] * b_val;\n            sum1 += A[(row + blockDim.x) * K + k] * b_val;\n            sum2 += A[(row + 2 * blockDim.x) * K + k] * b_val;\n            sum3 += A[(row + 3 * blockDim.x) * K + k] * b_val;\n        }\n        C[row] = sum0;\n        if (row + blockDim.x < M) C[row + blockDim.x] = sum1;\n        if (row + 2 * blockDim.x < M) C[row + 2 * blockDim.x] = sum2;\n        if (row + 3 * blockDim.x < M) C[row + 3 * blockDim.x] = sum3;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads * 4 - 1) / (threads * 4);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{M, 1}",
        "function": "{\nOptimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.\n}\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int i = tid; i < K; i += blockDim.x) {\n        B_shared[i] = B[i];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    for (int r = 0; r < 4; ++r) {\n        int current_row = row + r * blockDim.x;\n        if (current_row < M) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; ++k) {\n                sum += A[current_row * K + k] * B_shared[k];\n            }\n            C[current_row] = sum;\n        }\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads * 4 - 1) / (threads * 4);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize by using shared memory to cache the vector B, allowing coalesced memory access and reducing global memory bandwidth usage.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    const size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the kernel by using shared memory to cache the vector B, allowing for coalesced memory accesses and reduced global memory bandwidth usage.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    const size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{M, 1}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize by using shared memory to cache the vector B, reducing global memory accesses, and increase parallelism by processing multiple elements per thread with loop unrolling.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n\n    if (row < M) {\n        float sum = 0.0f;\n        // Unroll loop by 4 for better performance\n        int k = 0;\n        for (; k <= K - 4; k += 4) {\n            sum += A[row * K + k] * B_shared[k];\n            sum += A[row * K + k + 1] * B_shared[k + 1];\n            sum += A[row * K + k + 2] * B_shared[k + 2];\n            sum += A[row * K + k + 3] * B_shared[k + 3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize by using shared memory to cache vector B elements and unrolling the inner loop to reduce memory accesses and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define THREADS_PER_BLOCK 256\n#define UNROLL_FACTOR 4\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    \n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        int k = 0;\n        \n        // Unrolled loop\n        for (; k + UNROLL_FACTOR - 1 < K; k += UNROLL_FACTOR) {\n            sum += A[row * K + k] * B_shared[k] +\n                   A[row * K + k + 1] * B_shared[k + 1] +\n                   A[row * K + k + 2] * B_shared[k + 2] +\n                   A[row * K + k + 3] * B_shared[k + 3];\n        }\n        \n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        \n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = THREADS_PER_BLOCK;\n    const int blocks = (M + threads - 1) / threads;\n    const size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize by using shared memory to cache the vector B and tiling the matrix A for better memory access patterns and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and unrolling the inner loop for better memory access patterns and reduced overhead.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        const float* A_row = A + row * K;\n        \n        // Unroll the loop by 4 for better performance\n        int k = 0;\n        for (; k + 3 < K; k += 4) {\n            sum += A_row[k] * shared_B[k] +\n                   A_row[k+1] * shared_B[k+1] +\n                   A_row[k+2] * shared_B[k+2] +\n                   A_row[k+3] * shared_B[k+3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A_row[k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize by using shared memory to cache the vector B and unrolling the inner loop to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define THREADS_PER_BLOCK 256\n#define UNROLL_FACTOR 4\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        int k = 0;\n        // Unrolled loop\n        for (; k + UNROLL_FACTOR <= K; k += UNROLL_FACTOR) {\n            sum += A[row * K + k] * B_shared[k];\n            sum += A[row * K + k + 1] * B_shared[k + 1];\n            sum += A[row * K + k + 2] * B_shared[k + 2];\n            sum += A[row * K + k + 3] * B_shared[k + 3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = THREADS_PER_BLOCK;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and unrolling the inner loop for better memory access patterns and reduced overhead.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int i = threadIdx.x; i < K; i += blockDim.x) {\n        shared_B[i] = B[i];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        const float* A_row = A + row * K;\n        #pragma unroll 4\n        for (int k = 0; k < K; ++k) {\n            sum += A_row[k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    size_t shared_mem_size = K * sizeof(float);\n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize by using shared memory to cache vector B and unrolling the inner loop to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n\n    if (row < M) {\n        float sum = 0.0f;\n        const float* A_row = &A[row * K];\n        \n        // Unroll the loop by 4\n        int k = 0;\n        for (; k + 3 < K; k += 4) {\n            sum += A_row[k] * B_shared[k] + \n                   A_row[k+1] * B_shared[k+1] + \n                   A_row[k+2] * B_shared[k+2] + \n                   A_row[k+3] * B_shared[k+3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A_row[k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and employing thread coarsening to reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int tid = threadIdx.x;\n    int row = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = tid; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    // Process 4 rows per thread\n    for (int i = 0; i < 4; ++i) {\n        if (row < M) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; ++k) {\n                sum += A[row * K + k] * B_shared[k];\n            }\n            C[row] = sum;\n            row += blockDim.x;\n        }\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads * 4 - 1) / (threads * 4);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize by using shared memory to cache vector B and unrolling the inner loop for better memory access patterns and reduced arithmetic overhead.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        const float* A_row = &A[row * K];\n        \n        // Unroll the loop by 4\n        int k = 0;\n        for (; k <= K - 4; k += 4) {\n            sum += A_row[k] * B_shared[k] + \n                   A_row[k+1] * B_shared[k+1] + \n                   A_row[k+2] * B_shared[k+2] + \n                   A_row[k+3] * B_shared[k+3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A_row[k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize by using shared memory to cache the vector B and unrolling the inner loop to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        const float* A_row = A + row * K;\n        \n        // Unroll the loop by 4\n        int k = 0;\n        for (; k + 3 < K; k += 4) {\n            sum += A_row[k] * shared_B[k] + \n                   A_row[k+1] * shared_B[k+1] + \n                   A_row[k+2] * shared_B[k+2] + \n                   A_row[k+3] * shared_B[k+3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A_row[k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    \n    matrix_vector_mul_kernel<<<blocks, threads, K * sizeof(float)>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimize by using shared memory to cache vector B and unrolling the inner loop to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Cache vector B in shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n\n    if (row < M) {\n        float sum = 0.0f;\n        int k = 0;\n        // Unroll the loop by 4\n        for (; k + 3 < K; k += 4) {\n            sum += A[row * K + k] * B_shared[k] + \n                   A[row * K + k + 1] * B_shared[k + 1] + \n                   A[row * K + k + 2] * B_shared[k + 2] + \n                   A[row * K + k + 3] * B_shared[k + 3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimize the matrix-vector multiplication by utilizing shared memory to cache the vector B and unrolling the inner loop for better memory access patterns and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define THREADS_PER_BLOCK 256\n#define UNROLL_FACTOR 4\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float B_shared[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        B_shared[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        int k = 0;\n        // Unrolled loop\n        for (; k + UNROLL_FACTOR <= K; k += UNROLL_FACTOR) {\n            sum += A[row * K + k] * B_shared[k];\n            sum += A[row * K + k + 1] * B_shared[k + 1];\n            sum += A[row * K + k + 2] * B_shared[k + 2];\n            sum += A[row * K + k + 3] * B_shared[k + 3];\n        }\n        // Handle remaining elements\n        for (; k < K; ++k) {\n            sum += A[row * K + k] * B_shared[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = THREADS_PER_BLOCK;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize by using shared memory to cache the vector B and parallelizing the reduction across threads within a block for better memory access patterns and reduced global memory bandwidth.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matrix_vector_mul_kernel(const float* A, const float* B, float* C, int M, int K) {\n    extern __shared__ float shared_B[];\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    // Load B into shared memory\n    for (int k = threadIdx.x; k < K; k += blockDim.x) {\n        shared_B[k] = B[k];\n    }\n    __syncthreads();\n    \n    if (row < M) {\n        float sum = 0.0f;\n        for (int k = 0; k < K; ++k) {\n            sum += A[row * K + k] * shared_B[k];\n        }\n        C[row] = sum;\n    }\n}\n\nat::Tensor matrix_vector_mul_forward(const at::Tensor& A, const at::Tensor& B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    \n    auto C = at::zeros({M, 1}, A.options());\n    \n    const int threads = 256;\n    const int blocks = (M + threads - 1) / threads;\n    size_t shared_mem_size = K * sizeof(float);\n    \n    matrix_vector_mul_kernel<<<blocks, threads, shared_mem_size>>>(\n        A.data_ptr<float>(),\n        B.data_ptr<float>(),\n        C.data_ptr<float>(),\n        M, K\n    );\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matrix_vector_mul_forward, \"Matrix-vector multiplication forward (CUDA)\");\n}",
        "score": null
    }
]