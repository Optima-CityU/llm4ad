[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the batch normalization kernel by parallelizing across features and spatial dimensions, using shared memory for feature-specific parameters, and minimizing redundant computations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* s_mean = shared_params;\n    float* s_var = &shared_params[num_features];\n    float* s_weight = &shared_params[2 * num_features];\n    float* s_bias = &shared_params[3 * num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        s_mean[feature_idx] = bn_running_mean[feature_idx];\n        s_var[feature_idx] = bn_running_var[feature_idx];\n        s_weight[feature_idx] = bn_weight[feature_idx];\n        s_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.y + threadIdx.y;\n    int spatial_idx = threadIdx.x;\n\n    if (idx < batch_size * num_features && spatial_idx < spatial_dim) {\n        int batch_idx = idx / num_features;\n        feature_idx = idx % num_features;\n\n        int input_idx = (batch_idx * num_features + feature_idx) * spatial_dim + spatial_idx;\n        float x_val = x[input_idx];\n        float normalized_val = (x_val - s_mean[feature_idx]) * rsqrtf(s_var[feature_idx] + eps);\n        output[input_idx] = s_weight[feature_idx] * normalized_val + s_bias[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block(min(1024, spatial_dim), min(1024 / spatial_dim, batch_size * num_features));\n    dim3 grid((batch_size * num_features + block.y - 1) / block.y);\n\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n    \n    batch_norm_kernel<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the batch normalization kernel by parallelizing across features and spatial dimensions, using shared memory for running mean/var, and avoiding redundant computations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_features;\n    float* weight_shared = smem + 2 * num_features;\n    float* bias_shared = smem + 3 * num_features;\n\n    int tid = threadIdx.x;\n    int feature_idx = tid;\n\n    // Load shared memory\n    if (feature_idx < num_features) {\n        mean_shared[feature_idx] = bn_running_mean[feature_idx];\n        var_shared[feature_idx] = bn_running_var[feature_idx];\n        weight_shared[feature_idx] = bn_weight[feature_idx];\n        bias_shared[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int spatial_idx = idx % spatial_dim;\n    int batch_idx = idx / spatial_dim;\n    feature_idx = (batch_idx % num_features);\n\n    if (batch_idx < batch_size * num_features && spatial_idx < spatial_dim) {\n        float mean = mean_shared[feature_idx];\n        float var = var_shared[feature_idx];\n        float weight = weight_shared[feature_idx];\n        float bias = bias_shared[feature_idx];\n\n        float x_val = x[batch_idx * spatial_dim + spatial_idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[batch_idx * spatial_dim + spatial_idx] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t smem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, smem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize by processing features in parallel with shared memory for running statistics and warp-level reductions for training mode statistics.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* x, const float* bn_weight, const float* bn_bias,\n    const float* bn_running_mean, const float* bn_running_var,\n    float* output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    int feature_idx = blockIdx.x;\n    int spatial_idx = threadIdx.x;\n    \n    if (feature_idx >= num_features) return;\n    \n    float mean = bn_running_mean[feature_idx];\n    float var = bn_running_var[feature_idx];\n    float weight = bn_weight[feature_idx];\n    float bias = bn_bias[feature_idx];\n    \n    for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        \n        if (spatial_idx < spatial_dim) {\n            float x_val = x[idx];\n            float normalized_val = (x_val - mean) / sqrtf(var + eps);\n            output[idx] = weight * normalized_val + bias;\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 blocks(num_features);\n    dim3 threads(min(1024, spatial_dim));\n    \n    batch_norm_kernel<<<blocks, threads, 0>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the batch normalization kernel by parallelizing across features and using shared memory to cache running statistics, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* x, const float* bn_weight, const float* bn_bias,\n    const float* bn_running_mean, const float* bn_running_var,\n    float* output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_stats[];\n    float* shared_mean = shared_stats;\n    float* shared_var = &shared_stats[num_features];\n    float* shared_weight = &shared_var[num_features];\n    float* shared_bias = &shared_weight[num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int spatial_idx = idx % spatial_dim;\n    int batch_idx = idx / spatial_dim;\n    int global_feature_idx = (batch_idx % num_features);\n\n    if (batch_idx < batch_size * num_features && spatial_idx < spatial_dim) {\n        float mean = shared_mean[global_feature_idx];\n        float var = shared_var[global_feature_idx];\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[idx] = shared_weight[global_feature_idx] * normalized_val + shared_bias[global_feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, spatial_dim);\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize by computing mean and variance across spatial dimensions per feature in parallel, then normalizing each element with fused operations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* x, const float* bn_weight, const float* bn_bias,\n    const float* bn_running_mean, const float* bn_running_var,\n    float* output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    int feature_idx = blockIdx.x;\n    int spatial_idx = threadIdx.x;\n    \n    float mean = bn_running_mean[feature_idx];\n    float inv_std = rsqrtf(bn_running_var[feature_idx] + eps);\n    float weight = bn_weight[feature_idx];\n    float bias = bn_bias[feature_idx];\n    \n    for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        if (spatial_idx < spatial_dim) {\n            float x_val = x[idx];\n            float normalized_val = (x_val - mean) * inv_std;\n            output[idx] = weight * normalized_val + bias;\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 blocks(num_features);\n    dim3 threads(min(spatial_dim, 1024));\n    \n    batch_norm_kernel<<<blocks, threads, 0>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize memory access patterns by processing features in parallel and using shared memory for batch statistics, while maintaining correct batch normalization computation.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* mean_shared = shared_data;\n    float* var_shared = &shared_data[num_features];\n    float* weight_shared = &var_shared[num_features];\n    float* bias_shared = &weight_shared[num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        mean_shared[feature_idx] = bn_running_mean[feature_idx];\n        var_shared[feature_idx] = bn_running_var[feature_idx];\n        weight_shared[feature_idx] = bn_weight[feature_idx];\n        bias_shared[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int spatial_idx = idx % spatial_dim;\n    int batch_idx = idx / spatial_dim;\n    int global_feature_idx = (batch_idx % num_features);\n\n    if (batch_idx < batch_size * num_features && spatial_idx < spatial_dim) {\n        float mean = mean_shared[global_feature_idx];\n        float var = var_shared[global_feature_idx];\n        float weight = weight_shared[global_feature_idx];\n        float bias = bias_shared[global_feature_idx];\n\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[idx] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, spatial_dim);\n    int shared_mem_size = 4 * num_features * sizeof(float);\n\n    dim3 grid((batch_size * num_features * spatial_dim + block_size - 1) / block_size);\n    dim3 block(block_size);\n\n    batch_norm_kernel<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the batch normalization kernel by vectorizing memory accesses, using shared memory for feature statistics, and parallelizing across features and spatial dimensions while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel_optimized(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = &smem[num_features];\n    float* weight_shared = &var_shared[num_features];\n    float* bias_shared = &weight_shared[num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        mean_shared[feature_idx] = bn_running_mean[feature_idx];\n        var_shared[feature_idx] = bn_running_var[feature_idx];\n        weight_shared[feature_idx] = bn_weight[feature_idx];\n        bias_shared[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int spatial_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    \n    if (spatial_idx < spatial_dim && batch_idx < batch_size) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        float x_val = x[idx];\n        \n        float mean = mean_shared[feature_idx];\n        float var = var_shared[feature_idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        \n        output[idx] = weight_shared[feature_idx] * normalized_val + bias_shared[feature_idx];\n    }\n}\n\nvoid batch_norm_forward_optimized(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block(num_features);\n    dim3 grid(spatial_dim, batch_size);\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel_optimized<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the batch normalization kernel by utilizing shared memory for feature-wise statistics and vectorized memory access to reduce global memory bandwidth usage.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_stats[];\n    float* shared_mean = shared_stats;\n    float* shared_var = &shared_stats[num_features];\n    float* shared_weight = &shared_stats[2 * num_features];\n    float* shared_bias = &shared_stats[3 * num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = idx; i < elements; i += stride) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n\n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the CUDA kernel by vectorizing memory accesses, using shared memory for feature-wise statistics, and reducing thread divergence with warp-level operations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, \n    const float* __restrict__ bn_bias, const float* __restrict__ bn_running_mean, \n    const float* __restrict__ bn_running_var, float* __restrict__ output, \n    int batch_size, int num_features, int spatial_dim, float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* shared_mean = shared_data;\n    float* shared_var = &shared_data[num_features];\n    float* shared_weight = &shared_data[2 * num_features];\n    float* shared_bias = &shared_data[3 * num_features];\n\n    int tid = threadIdx.x;\n    int feature_idx = tid % num_features;\n\n    if (tid < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n\n    for (int i = idx; i < elements; i += blockDim.x * gridDim.x) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n        float x_val = x[i];\n\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the batch normalization kernel by coalescing memory accesses, using shared memory for feature-wise parameters, and vectorizing operations to reduce memory bandwidth usage and improve parallelism.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* s_mean = shared_params;\n    float* s_var = &shared_params[num_features];\n    float* s_weight = &shared_params[2 * num_features];\n    float* s_bias = &shared_params[3 * num_features];\n\n    // Load feature-wise parameters into shared memory\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        s_mean[i] = bn_running_mean[i];\n        s_var[i] = bn_running_var[i];\n        s_weight[i] = bn_weight[i];\n        s_bias[i] = bn_bias[i];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    int stride = gridDim.x * blockDim.x;\n\n    for (int i = idx; i < elements; i += stride) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = s_mean[feature_idx];\n        float var = s_var[feature_idx];\n        float weight = s_weight[feature_idx];\n        float bias = s_bias[feature_idx];\n\n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = min(65535, (batch_size * num_features * spatial_dim + block_size - 1) / block_size);\n\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the batch normalization kernel by coalescing memory accesses, using shared memory for running statistics, and reducing thread divergence with warp-level operations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_stats[];\n    float* shared_mean = shared_stats;\n    float* shared_var = &shared_stats[num_features];\n    float* shared_weight = &shared_stats[2*num_features];\n    float* shared_bias = &shared_stats[3*num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int total_elements = batch_size * num_features * spatial_dim;\n\n    for (int i = idx; i < total_elements; i += gridDim.x * blockDim.x) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int f_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = shared_mean[f_idx];\n        float var = shared_var[f_idx];\n        float weight = shared_weight[f_idx];\n        float bias = shared_bias[f_idx];\n\n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimized algorithm computes batch normalization by vectorizing feature-wise operations and using shared memory to reduce global memory accesses, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_features;\n    float* weight_shared = smem + 2 * num_features;\n    float* bias_shared = smem + 3 * num_features;\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        mean_shared[feature_idx] = bn_running_mean[feature_idx];\n        var_shared[feature_idx] = bn_running_var[feature_idx];\n        weight_shared[feature_idx] = bn_weight[feature_idx];\n        bias_shared[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.y + threadIdx.y;\n    int spatial_idx = threadIdx.x;\n    \n    if (idx < batch_size * num_features && spatial_idx < spatial_dim) {\n        int batch_idx = idx / num_features;\n        feature_idx = idx % num_features;\n        \n        int input_idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        float x_val = x[input_idx];\n        \n        float mean = mean_shared[feature_idx];\n        float inv_std = rsqrtf(var_shared[feature_idx] + eps);\n        float normalized_val = (x_val - mean) * inv_std;\n        \n        float output_val = weight_shared[feature_idx] * normalized_val + bias_shared[feature_idx];\n        \n        output[input_idx] = output_val;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block(32, 8);\n    dim3 grid((batch_size * num_features + block.y - 1) / block.y);\n    \n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n    \n    batch_norm_kernel<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the CUDA kernel by coalescing memory accesses, using shared memory for batch statistics, and vectorizing operations to improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* shared_mean = shared_data;\n    float* shared_var = &shared_data[num_features];\n    float* shared_weight = &shared_data[2 * num_features];\n    float* shared_bias = &shared_data[3 * num_features];\n\n    // Load batch norm parameters into shared memory\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        shared_mean[i] = bn_running_mean[i];\n        shared_var[i] = bn_running_var[i];\n        shared_weight[i] = bn_weight[i];\n        shared_bias[i] = bn_bias[i];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = idx; i < elements; i += stride) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n\n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the kernel by processing features in parallel across threads, using shared memory for running mean/var, and avoiding redundant computations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = &smem[num_features];\n    float* weight_shared = &smem[2*num_features];\n    float* bias_shared = &smem[3*num_features];\n\n    int tid = threadIdx.x;\n    int feature_idx = tid;\n    \n    // Load running stats and parameters into shared memory\n    if (feature_idx < num_features) {\n        mean_shared[feature_idx] = bn_running_mean[feature_idx];\n        var_shared[feature_idx] = bn_running_var[feature_idx];\n        weight_shared[feature_idx] = bn_weight[feature_idx];\n        bias_shared[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    \n    for (int i = idx; i < elements; i += blockDim.x * gridDim.x) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float x_val = x[i];\n        float mean = mean_shared[feature_idx];\n        float inv_std = rsqrtf(var_shared[feature_idx] + eps);\n        float normalized_val = (x_val - mean) * inv_std;\n        output[i] = weight_shared[feature_idx] * normalized_val + bias_shared[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int grid_size = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t smem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid_size, block_size, smem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the kernel by computing the inverse standard deviation once per feature and using vectorized memory accesses to reduce redundant computations and improve memory efficiency.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* x, const float* bn_weight, const float* bn_bias,\n    const float* bn_running_mean, const float* bn_running_var,\n    float* output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* inv_std = shared_data;\n\n    int feature_idx = blockIdx.x;\n    if (feature_idx >= num_features) return;\n\n    // Precompute inverse standard deviation for this feature\n    if (threadIdx.x == 0) {\n        inv_std[0] = rsqrtf(bn_running_var[feature_idx] + eps);\n    }\n    __syncthreads();\n\n    float mean = bn_running_mean[feature_idx];\n    float weight = bn_weight[feature_idx];\n    float bias = bn_bias[feature_idx];\n    float current_inv_std = inv_std[0];\n\n    // Process spatial elements for this feature\n    for (int spatial_idx = threadIdx.x; spatial_idx < spatial_dim; spatial_idx += blockDim.x) {\n        for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n            int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n            float x_val = x[idx];\n            float normalized_val = (x_val - mean) * current_inv_std;\n            output[idx] = weight * normalized_val + bias;\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, spatial_dim);\n    int shared_mem_size = sizeof(float);\n\n    batch_norm_kernel<<<num_features, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the batch normalization kernel by parallelizing across features and spatial dimensions, using shared memory for running statistics, and minimizing global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_stats[];\n    float* shared_mean = shared_stats;\n    float* shared_var = &shared_mean[num_features];\n    float* shared_weight = &shared_var[num_features];\n    float* shared_bias = &shared_weight[num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int spatial_idx = idx % spatial_dim;\n    int batch_feature_idx = idx / spatial_dim;\n    int batch_idx = batch_feature_idx / num_features;\n    feature_idx = batch_feature_idx % num_features;\n\n    if (batch_idx < batch_size && feature_idx < num_features) {\n        float x_val = x[idx];\n        float normalized_val = (x_val - shared_mean[feature_idx]) * rsqrtf(shared_var[feature_idx] + eps);\n        output[idx] = shared_weight[feature_idx] * normalized_val + shared_bias[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, spatial_dim);\n    int grid_size = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid_size, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize memory access patterns by processing features in parallel while maintaining thread coherency, and precompute normalization factors to reduce redundant calculations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* inv_std = shared_data;\n\n    int feature_idx = blockIdx.x;\n    int spatial_idx = threadIdx.x;\n\n    if (feature_idx < num_features) {\n        float mean = bn_running_mean[feature_idx];\n        float var = bn_running_var[feature_idx];\n        inv_std[feature_idx] = rsqrtf(var + eps);\n    }\n\n    __syncthreads();\n\n    for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        if (feature_idx < num_features && spatial_idx < spatial_dim) {\n            float x_val = x[idx];\n            float normalized_val = (x_val - bn_running_mean[feature_idx]) * inv_std[feature_idx];\n            output[idx] = bn_weight[feature_idx] * normalized_val + bn_bias[feature_idx];\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 blocks(num_features);\n    dim3 threads(min(spatial_dim, 1024));\n    size_t shared_mem = num_features * sizeof(float);\n\n    batch_norm_kernel<<<blocks, threads, shared_mem>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize memory access patterns by processing features in parallel and using shared memory for batch statistics, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = &smem[num_features];\n    float* weight_shared = &smem[2*num_features];\n    float* bias_shared = &smem[3*num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        mean_shared[feature_idx] = bn_running_mean[feature_idx];\n        var_shared[feature_idx] = bn_running_var[feature_idx];\n        weight_shared[feature_idx] = bn_weight[feature_idx];\n        bias_shared[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int spatial_idx = idx % spatial_dim;\n    int batch_idx = idx / spatial_dim;\n    int global_feature_idx = (batch_idx % num_features);\n\n    if (batch_idx < batch_size * num_features && spatial_idx < spatial_dim) {\n        float x_val = x[batch_idx * spatial_dim + spatial_idx];\n        float mean = mean_shared[global_feature_idx];\n        float var = var_shared[global_feature_idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[batch_idx * spatial_dim + spatial_idx] = \n            weight_shared[global_feature_idx] * normalized_val + bias_shared[global_feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, spatial_dim);\n    int num_blocks = (batch_size * num_features + block_size - 1) / block_size;\n    size_t smem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, smem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize memory access patterns by processing features in parallel and using shared memory to cache batch norm parameters, while maintaining correct numerical results.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* shared_mean = shared_params;\n    float* shared_var = shared_params + num_features;\n    float* shared_weight = shared_params + 2 * num_features;\n    float* shared_bias = shared_params + 3 * num_features;\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int spatial_idx = idx % spatial_dim;\n    int batch_feature_idx = idx / spatial_dim;\n    int batch_idx = batch_feature_idx / num_features;\n    feature_idx = batch_feature_idx % num_features;\n\n    if (batch_idx < batch_size && feature_idx < num_features) {\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[idx] = shared_weight[feature_idx] * normalized_val + shared_bias[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, num_features * 4);\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize by vectorizing memory accesses and using warp-level operations to reduce redundant computations, while maintaining the same numerical precision and correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, \n    const float* __restrict__ bn_bias, const float* __restrict__ bn_running_mean, \n    const float* __restrict__ bn_running_var, float* __restrict__ output, \n    int batch_size, int num_features, int spatial_dim, float eps, bool training) {\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int feature_stride = blockDim.x * gridDim.x;\n\n    for (int i = idx; i < batch_size * num_features * spatial_dim; i += feature_stride) {\n        int feature_idx = (i / spatial_dim) % num_features;\n        \n        float mean = bn_running_mean[feature_idx];\n        float var = bn_running_var[feature_idx];\n        float inv_std = rsqrtf(var + eps);\n        float weight = bn_weight[feature_idx];\n        float bias = bn_bias[feature_idx];\n        \n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * inv_std;\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = min(65535, (batch_size * num_features * spatial_dim + block_size - 1) / block_size);\n\n    batch_norm_kernel<<<num_blocks, block_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the batch normalization kernel by parallelizing across features and spatial dimensions, using shared memory for running statistics, and avoiding redundant computations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_stats[];\n    float* shared_mean = shared_stats;\n    float* shared_var = &shared_stats[num_features];\n    float* shared_weight = &shared_stats[2*num_features];\n    float* shared_bias = &shared_stats[3*num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.y + threadIdx.y;\n    if (idx < batch_size * spatial_dim) {\n        int batch_idx = idx / spatial_dim;\n        int spatial_idx = idx % spatial_dim;\n        \n        if (feature_idx < num_features) {\n            int input_idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n            float x_val = x[input_idx];\n            float normalized_val = (x_val - shared_mean[feature_idx]) * rsqrtf(shared_var[feature_idx] + eps);\n            output[input_idx] = shared_weight[feature_idx] * normalized_val + shared_bias[feature_idx];\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block_dim(num_features, 32);\n    int grid_dim = (batch_size * spatial_dim + block_dim.y - 1) / block_dim.y;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid_dim, block_dim, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the CUDA kernel by coalescing memory accesses, using shared memory for batch statistics, and vectorizing operations to reduce memory bandwidth and improve parallelism.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* shared_mean = shared_data;\n    float* shared_var = &shared_data[num_features];\n    float* shared_weight = &shared_data[2 * num_features];\n    float* shared_bias = &shared_data[3 * num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = idx; i < elements; i += stride) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n\n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize memory access patterns by processing features in parallel and using shared memory for batch statistics during training, while maintaining correctness for both training and inference modes.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_mem[];\n    float* mean_shared = shared_mem;\n    float* var_shared = &shared_mem[num_features];\n    float* weight_shared = &shared_mem[2 * num_features];\n    float* bias_shared = &shared_mem[3 * num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        mean_shared[feature_idx] = bn_running_mean[feature_idx];\n        var_shared[feature_idx] = bn_running_var[feature_idx];\n        weight_shared[feature_idx] = bn_weight[feature_idx];\n        bias_shared[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int spatial_idx = idx % spatial_dim;\n    int batch_feature_idx = idx / spatial_dim;\n    int batch_idx = batch_feature_idx / num_features;\n    feature_idx = batch_feature_idx % num_features;\n\n    if (batch_idx < batch_size && feature_idx < num_features && spatial_idx < spatial_dim) {\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean_shared[feature_idx]) * rsqrtf(var_shared[feature_idx] + eps);\n        output[idx] = weight_shared[feature_idx] * normalized_val + bias_shared[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, num_features * spatial_dim);\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize by precomputing normalization factors per feature and using shared memory to reduce redundant memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* x, const float* bn_weight, const float* bn_bias,\n    const float* bn_running_mean, const float* bn_running_var,\n    float* output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* inv_std = shared_data;\n    \n    int feature_idx = blockIdx.x;\n    if (threadIdx.x == 0) {\n        inv_std[0] = rsqrtf(bn_running_var[feature_idx] + eps);\n    }\n    __syncthreads();\n\n    int spatial_idx = threadIdx.x;\n    while (spatial_idx < spatial_dim) {\n        for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n            int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n            float x_val = x[idx];\n            float normalized_val = (x_val - bn_running_mean[feature_idx]) * inv_std[0];\n            output[idx] = bn_weight[feature_idx] * normalized_val + bn_bias[feature_idx];\n        }\n        spatial_idx += blockDim.x;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, spatial_dim);\n    size_t shared_mem_size = sizeof(float);\n\n    batch_norm_kernel<<<num_features, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the batch normalization kernel by vectorizing memory accesses, using fast math operations, and minimizing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = idx; i < batch_size * num_features * spatial_dim; i += stride) {\n        int feature_idx = (i / spatial_dim) % num_features;\n        \n        float mean = bn_running_mean[feature_idx];\n        float inv_std = rsqrtf(bn_running_var[feature_idx] + eps);\n        float weight = bn_weight[feature_idx];\n        float bias = bn_bias[feature_idx];\n        \n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * inv_std;\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = min(65535, (batch_size * num_features * spatial_dim + block_size - 1) / block_size);\n\n    batch_norm_kernel<<<num_blocks, block_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the batch normalization kernel by vectorizing memory accesses, using fast math operations, and minimizing thread divergence while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__global__ void batch_norm_kernel_optimized(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, \n    const float* __restrict__ bn_bias, const float* __restrict__ bn_running_mean, \n    const float* __restrict__ bn_running_var, float* __restrict__ output, \n    int batch_size, int num_features, int spatial_dim, float eps, bool training) {\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int feature_stride = batch_size * spatial_dim;\n    int spatial_stride = spatial_dim;\n\n    if (idx < batch_size * num_features * spatial_dim) {\n        int feature_idx = idx / feature_stride;\n        int spatial_idx = idx % spatial_stride;\n        int batch_idx = (idx / spatial_stride) % batch_size;\n\n        float mean = __ldg(&bn_running_mean[feature_idx]);\n        float var = __ldg(&bn_running_var[feature_idx]);\n        float inv_std = rsqrtf(var + eps);\n        float weight = __ldg(&bn_weight[feature_idx]);\n        float bias = __ldg(&bn_bias[feature_idx]);\n\n        float x_val = __ldg(&x[idx]);\n        float normalized_val = (x_val - mean) * inv_std;\n        output[idx] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward_optimized(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n\n    batch_norm_kernel_optimized<<<num_blocks, block_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    C10_CUDA_KERNEL_LAUNCH_CHECK();\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the batch normalization kernel by vectorizing memory accesses, using fast math operations, and minimizing thread divergence while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, \n    const float* __restrict__ bn_bias, const float* __restrict__ bn_running_mean, \n    const float* __restrict__ bn_running_var, float* __restrict__ output, \n    int batch_size, int num_features, int spatial_dim, float eps, bool training) {\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = idx; i < batch_size * num_features * spatial_dim; i += stride) {\n        int feature_idx = (i / spatial_dim) % num_features;\n        \n        float mean = __ldg(&bn_running_mean[feature_idx]);\n        float var = __ldg(&bn_running_var[feature_idx]);\n        float weight = __ldg(&bn_weight[feature_idx]);\n        float bias = __ldg(&bn_bias[feature_idx]);\n        float x_val = __ldg(&x[i]);\n\n        float inv_std = rsqrtf(var + eps);\n        float normalized_val = (x_val - mean) * inv_std;\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = min(65535, (batch_size * num_features * spatial_dim + block_size - 1) / block_size);\n\n    batch_norm_kernel<<<num_blocks, block_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the batch normalization kernel by leveraging shared memory for feature-wise statistics and parallelizing across spatial dimensions to reduce redundant memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* shared_mean = shared_data;\n    float* shared_var = &shared_data[num_features];\n    float* shared_weight = &shared_data[2 * num_features];\n    float* shared_bias = &shared_data[3 * num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int spatial_idx = blockIdx.x * blockDim.y + threadIdx.y;\n    if (spatial_idx >= spatial_dim) return;\n\n    for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        float x_val = x[idx];\n        float normalized_val = (x_val - shared_mean[feature_idx]) * rsqrtf(shared_var[feature_idx] + eps);\n        output[idx] = shared_weight[feature_idx] * normalized_val + shared_bias[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block(256, 1);\n    if (num_features <= 256) {\n        block.x = num_features;\n        block.y = 256 / num_features;\n    }\n\n    int grid_size = (spatial_dim + block.y - 1) / block.y;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid_size, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the CUDA kernel by vectorizing memory accesses, using shared memory for batch norm parameters, and parallelizing across features and spatial dimensions while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* shared_mean = shared_params;\n    float* shared_var = shared_params + num_features;\n    float* shared_weight = shared_params + 2 * num_features;\n    float* shared_bias = shared_params + 3 * num_features;\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.y + threadIdx.y;\n    int spatial_idx = threadIdx.x;\n    \n    if (feature_idx < num_features && idx < batch_size && spatial_idx < spatial_dim) {\n        int input_idx = idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        float x_val = x[input_idx];\n        float normalized_val = (x_val - shared_mean[feature_idx]) * rsqrtf(shared_var[feature_idx] + eps);\n        output[input_idx] = shared_weight[feature_idx] * normalized_val + shared_bias[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block(num_features, 32);\n    dim3 grid((batch_size + block.y - 1) / block.y);\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize memory access patterns by processing features in parallel while maintaining thread coherency, and precompute normalization terms to reduce redundant calculations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* mean_shared = shared_data;\n    float* var_shared = &shared_data[num_features];\n    float* weight_shared = &var_shared[num_features];\n    float* bias_shared = &weight_shared[num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        mean_shared[feature_idx] = bn_running_mean[feature_idx];\n        var_shared[feature_idx] = 1.0f / sqrtf(bn_running_var[feature_idx] + eps);\n        weight_shared[feature_idx] = bn_weight[feature_idx];\n        bias_shared[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.y + threadIdx.y;\n    if (idx < batch_size * spatial_dim) {\n        int batch_idx = idx / spatial_dim;\n        int spatial_idx = idx % spatial_dim;\n        \n        if (feature_idx < num_features) {\n            int input_idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n            float x_val = x[input_idx];\n            float normalized_val = (x_val - mean_shared[feature_idx]) * var_shared[feature_idx];\n            output[input_idx] = weight_shared[feature_idx] * normalized_val + bias_shared[feature_idx];\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block(256, 1);\n    if (num_features <= 64) {\n        block.x = num_features;\n        block.y = 256 / num_features;\n    }\n\n    int grid = (batch_size * spatial_dim + block.y - 1) / block.y;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the kernel by vectorizing memory accesses, using shared memory for batch statistics, and reducing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, \n    const float* __restrict__ bn_bias, const float* __restrict__ bn_running_mean, \n    const float* __restrict__ bn_running_var, float* __restrict__ output, \n    int batch_size, int num_features, int spatial_dim, float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* shared_mean = shared_data;\n    float* shared_var = &shared_data[num_features];\n    float* shared_weight = &shared_data[2 * num_features];\n    float* shared_bias = &shared_data[3 * num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = idx; i < elements; i += stride) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n\n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int grid_size = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid_size, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the batch normalization kernel by coalescing memory accesses, using shared memory for feature-wise parameters, and parallelizing across spatial dimensions with warp-level operations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, \n    const float* __restrict__ bn_bias, const float* __restrict__ bn_running_mean, \n    const float* __restrict__ bn_running_var, float* __restrict__ output, \n    int batch_size, int num_features, int spatial_dim, float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* s_mean = shared_params;\n    float* s_var = &s_mean[num_features];\n    float* s_weight = &s_var[num_features];\n    float* s_bias = &s_weight[num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        s_mean[feature_idx] = bn_running_mean[feature_idx];\n        s_var[feature_idx] = bn_running_var[feature_idx];\n        s_weight[feature_idx] = bn_weight[feature_idx];\n        s_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int spatial_idx = idx % spatial_dim;\n    int batch_feature_idx = idx / spatial_dim;\n\n    if (batch_feature_idx < batch_size * num_features) {\n        int batch_idx = batch_feature_idx / num_features;\n        feature_idx = batch_feature_idx % num_features;\n\n        float mean = s_mean[feature_idx];\n        float var = s_var[feature_idx];\n        float weight = s_weight[feature_idx];\n        float bias = s_bias[feature_idx];\n\n        float x_val = x[batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, spatial_dim);\n    int num_blocks = (batch_size * num_features + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the batch normalization kernel by vectorizing memory accesses and using warp-level operations to reduce redundant computations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int feature_idx = (idx / spatial_dim) % num_features;\n    \n    if (idx < batch_size * num_features * spatial_dim) {\n        float mean = bn_running_mean[feature_idx];\n        float var = bn_running_var[feature_idx];\n        float inv_std = rsqrtf(var + eps);\n        float weight = bn_weight[feature_idx];\n        float bias = bn_bias[feature_idx];\n\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean) * inv_std;\n        output[idx] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int grid_size = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n\n    batch_norm_kernel<<<grid_size, block_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the batch normalization kernel by using shared memory for feature statistics, vectorized memory accesses, and warp-level reductions to minimize global memory accesses and improve parallelism.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, \n    const float* __restrict__ bn_bias, const float* __restrict__ bn_running_mean, \n    const float* __restrict__ bn_running_var, float* __restrict__ output, \n    int batch_size, int num_features, int spatial_dim, float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* shared_mean = shared_data;\n    float* shared_var = &shared_data[num_features];\n    float* shared_weight = &shared_data[2 * num_features];\n    float* shared_bias = &shared_data[3 * num_features];\n\n    int tid = threadIdx.x;\n    int feature_idx = blockIdx.x;\n\n    if (tid == 0) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    float mean = shared_mean[feature_idx];\n    float var = shared_var[feature_idx];\n    float weight = shared_weight[feature_idx];\n    float bias = shared_bias[feature_idx];\n\n    for (int batch_idx = tid; batch_idx < batch_size; batch_idx += blockDim.x) {\n        for (int spatial_idx = 0; spatial_idx < spatial_dim; ++spatial_idx) {\n            int idx = batch_idx * num_features * spatial_dim + \n                     feature_idx * spatial_dim + spatial_idx;\n            float x_val = x[idx];\n            float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n            output[idx] = weight * normalized_val + bias;\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 blocks(num_features);\n    dim3 threads(min(1024, batch_size));\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<blocks, threads, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize by vectorizing computations across spatial dimensions for each feature, reducing thread divergence and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* x, const float* bn_weight, const float* bn_bias,\n    const float* bn_running_mean, const float* bn_running_var,\n    float* output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    int feature_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    int spatial_idx = threadIdx.x;\n\n    if (feature_idx < num_features && batch_idx < batch_size && spatial_idx < spatial_dim) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n\n        float mean = bn_running_mean[feature_idx];\n        float var = bn_running_var[feature_idx];\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[idx] = bn_weight[feature_idx] * normalized_val + bn_bias[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 blocks(num_features, batch_size);\n    int threads = min(1024, spatial_dim);\n\n    batch_norm_kernel<<<blocks, threads>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the batch normalization kernel by using shared memory for feature statistics and vectorized memory access to reduce global memory transactions.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_stats[];\n    float* shared_mean = shared_stats;\n    float* shared_var = &shared_stats[num_features];\n    float* shared_weight = &shared_stats[2 * num_features];\n    float* shared_bias = &shared_stats[3 * num_features];\n\n    // Load feature statistics into shared memory\n    for (int i = threadIdx.x; i < num_features; i += blockDim.x) {\n        shared_mean[i] = bn_running_mean[i];\n        shared_var[i] = bn_running_var[i];\n        shared_weight[i] = bn_weight[i];\n        shared_bias[i] = bn_bias[i];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    int stride = gridDim.x * blockDim.x;\n\n    for (int i = idx; i < elements; i += stride) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n\n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = min(65535, (batch_size * num_features * spatial_dim + block_size - 1) / block_size);\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimized algorithm: Vectorize computations across spatial dimensions, precompute inverse standard deviation, and use shared memory for feature-specific parameters to reduce redundant calculations and memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* x, const float* bn_weight, const float* bn_bias,\n    const float* bn_running_mean, const float* bn_running_var,\n    float* output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* s_mean = shared_params;\n    float* s_var = &shared_params[num_features];\n    float* s_weight = &shared_params[2 * num_features];\n    float* s_bias = &shared_params[3 * num_features];\n\n    int feature_idx = blockIdx.x;\n    if (threadIdx.x == 0) {\n        s_mean[feature_idx] = bn_running_mean[feature_idx];\n        s_var[feature_idx] = 1.0f / sqrtf(bn_running_var[feature_idx] + eps);\n        s_weight[feature_idx] = bn_weight[feature_idx];\n        s_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    float mean = s_mean[feature_idx];\n    float inv_std = s_var[feature_idx];\n    float weight = s_weight[feature_idx];\n    float bias = s_bias[feature_idx];\n\n    for (int batch_idx = threadIdx.x; batch_idx < batch_size; batch_idx += blockDim.x) {\n        int base_idx = (batch_idx * num_features + feature_idx) * spatial_dim;\n        for (int spatial_idx = 0; spatial_idx < spatial_dim; ++spatial_idx) {\n            int idx = base_idx + spatial_idx;\n            float x_val = x[idx];\n            float normalized_val = (x_val - mean) * inv_std;\n            output[idx] = weight * normalized_val + bias;\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, batch_size);\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_features, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the batch normalization kernel by leveraging shared memory for feature-wise statistics and parallelizing across spatial dimensions within each feature.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_stats[];\n    float* shared_mean = shared_stats;\n    float* shared_var = &shared_stats[num_features];\n    float* shared_weight = &shared_var[num_features];\n    float* shared_bias = &shared_weight[num_features];\n\n    int feature_idx = blockIdx.x;\n    int spatial_idx = threadIdx.x;\n\n    if (spatial_idx < num_features) {\n        shared_mean[spatial_idx] = bn_running_mean[spatial_idx];\n        shared_var[spatial_idx] = bn_running_var[spatial_idx];\n        shared_weight[spatial_idx] = bn_weight[spatial_idx];\n        shared_bias[spatial_idx] = bn_bias[spatial_idx];\n    }\n    __syncthreads();\n\n    if (feature_idx < num_features && spatial_idx < spatial_dim) {\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n\n        for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n            int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n            float x_val = x[idx];\n            float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n            output[idx] = weight * normalized_val + bias;\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n    batch_norm_kernel<<<num_features, spatial_dim, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize by vectorizing computations across spatial dimensions for each feature, reducing thread divergence and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    int feature_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    int spatial_idx = threadIdx.x;\n\n    if (feature_idx < num_features && batch_idx < batch_size && spatial_idx < spatial_dim) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        \n        float mean = bn_running_mean[feature_idx];\n        float var = bn_running_var[feature_idx];\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        \n        output[idx] = bn_weight[feature_idx] * normalized_val + bn_bias[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 blocks(num_features, batch_size);\n    int threads = min(1024, spatial_dim);\n\n    batch_norm_kernel<<<blocks, threads>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the batch normalization kernel by using shared memory for feature-wise statistics and vectorized memory access to reduce global memory bandwidth usage.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_stats[];\n    float* shared_mean = shared_stats;\n    float* shared_var = &shared_mean[num_features];\n    float* shared_weight = &shared_var[num_features];\n    float* shared_bias = &shared_weight[num_features];\n\n    int tid = threadIdx.x;\n    int feature_idx = tid;\n\n    // Load feature-wise stats into shared memory\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < batch_size * num_features * spatial_dim) {\n        int batch_idx = idx / (num_features * spatial_dim);\n        feature_idx = (idx / spatial_dim) % num_features;\n        int spatial_idx = idx % spatial_dim;\n\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[idx] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the CUDA kernel by parallelizing over features and spatial dimensions, using shared memory for running mean/var, and vectorized memory access for coalesced reads/writes.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float smem[];\n    float* mean_shared = smem;\n    float* var_shared = smem + num_features;\n    float* weight_shared = smem + 2 * num_features;\n    float* bias_shared = smem + 3 * num_features;\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        mean_shared[feature_idx] = bn_running_mean[feature_idx];\n        var_shared[feature_idx] = bn_running_var[feature_idx];\n        weight_shared[feature_idx] = bn_weight[feature_idx];\n        bias_shared[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int spatial_idx = blockIdx.x * blockDim.y + threadIdx.y;\n    if (spatial_idx >= spatial_dim) return;\n\n    for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean_shared[feature_idx]) * rsqrtf(var_shared[feature_idx] + eps);\n        output[idx] = weight_shared[feature_idx] * normalized_val + bias_shared[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block(256, 1);\n    while (block.x / 2 >= num_features && block.x > 32) {\n        block.x /= 2;\n        block.y *= 2;\n    }\n\n    dim3 grid((spatial_dim + block.y - 1) / block.y);\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimized algorithm: Vectorize feature dimension processing, use shared memory for BN parameters, and fuse mean/variance computation in training mode while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_bn_params[];\n    float* shared_mean = shared_bn_params;\n    float* shared_var = shared_bn_params + num_features;\n    float* shared_weight = shared_bn_params + 2 * num_features;\n    float* shared_bias = shared_bn_params + 3 * num_features;\n\n    int tid = threadIdx.x;\n    if (tid < num_features) {\n        shared_mean[tid] = bn_running_mean[tid];\n        shared_var[tid] = bn_running_var[tid];\n        shared_weight[tid] = bn_weight[tid];\n        shared_bias[tid] = bn_bias[tid];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    \n    for (int i = idx; i < elements; i += blockDim.x * gridDim.x) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n\n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int shared_mem_size = 4 * num_features * sizeof(float);\n    int num_blocks = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n\n    batch_norm_kernel<<<min(num_blocks, 65535), block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the CUDA kernel by vectorizing memory accesses, using shared memory for feature-wise statistics, and parallelizing across both batch and spatial dimensions while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_stats[];\n    float* shared_mean = shared_stats;\n    float* shared_var = &shared_stats[num_features];\n    float* shared_weight = &shared_var[num_features];\n    float* shared_bias = &shared_weight[num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.y + threadIdx.y;\n    int spatial_idx = threadIdx.x;\n    \n    if (idx < batch_size && spatial_idx < spatial_dim) {\n        for (int feature_idx = 0; feature_idx < num_features; ++feature_idx) {\n            int input_idx = idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n            float x_val = x[input_idx];\n            float normalized_val = (x_val - shared_mean[feature_idx]) * rsqrtf(shared_var[feature_idx] + eps);\n            output[input_idx] = shared_weight[feature_idx] * normalized_val + shared_bias[feature_idx];\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block(spatial_dim, 32);\n    dim3 grid((batch_size + block.y - 1) / block.y);\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the batch normalization kernel by parallelizing over features and spatial dimensions, precomputing normalization factors, and using shared memory for feature-specific parameters to reduce redundant memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* s_mean = shared_params;\n    float* s_var = &shared_params[num_features];\n    float* s_weight = &shared_params[2*num_features];\n    float* s_bias = &shared_params[3*num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        s_mean[feature_idx] = bn_running_mean[feature_idx];\n        s_var[feature_idx] = 1.0f / sqrtf(bn_running_var[feature_idx] + eps);\n        s_weight[feature_idx] = bn_weight[feature_idx];\n        s_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.y + threadIdx.y;\n    int spatial_idx = threadIdx.x;\n    \n    if (idx < batch_size && spatial_idx < spatial_dim) {\n        for (int f = 0; f < num_features; ++f) {\n            int input_idx = idx * num_features * spatial_dim + f * spatial_dim + spatial_idx;\n            float x_val = x[input_idx];\n            float normalized_val = (x_val - s_mean[f]) * s_var[f];\n            output[input_idx] = s_weight[f] * normalized_val + s_bias[f];\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block(32, 32);\n    dim3 grid((batch_size + block.y - 1) / block.y);\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize memory access patterns by processing features in parallel within warps and using shared memory for batch statistics during training, while maintaining correctness for both training and inference modes.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    int feature_idx = blockIdx.x;\n    int spatial_idx = threadIdx.x;\n    \n    if (feature_idx >= num_features || spatial_idx >= spatial_dim) return;\n\n    float mean = bn_running_mean[feature_idx];\n    float var = bn_running_var[feature_idx];\n    float weight = bn_weight[feature_idx];\n    float bias = bn_bias[feature_idx];\n    float inv_std = rsqrtf(var + eps);\n\n    for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean) * inv_std;\n        output[idx] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 blocks(num_features);\n    dim3 threads(min(spatial_dim, 1024));\n\n    batch_norm_kernel<<<blocks, threads, 0>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the CUDA kernel by vectorizing memory accesses, using shared memory for feature-specific parameters, and reducing thread divergence through warp-level operations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel_optimized(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, \n    const float* __restrict__ bn_bias, const float* __restrict__ bn_running_mean, \n    const float* __restrict__ bn_running_var, float* __restrict__ output, \n    int batch_size, int num_features, int spatial_dim, float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* s_mean = shared_params;\n    float* s_var = &shared_params[num_features];\n    float* s_weight = &shared_params[2 * num_features];\n    float* s_bias = &shared_params[3 * num_features];\n\n    int tid = threadIdx.x;\n    if (tid < num_features) {\n        s_mean[tid] = bn_running_mean[tid];\n        s_var[tid] = bn_running_var[tid];\n        s_weight[tid] = bn_weight[tid];\n        s_bias[tid] = bn_bias[tid];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    \n    for (int i = idx; i < elements; i += blockDim.x * gridDim.x) {\n        int feature_idx = (i / spatial_dim) % num_features;\n        \n        float mean = s_mean[feature_idx];\n        float var = s_var[feature_idx];\n        float x_val = x[i];\n        float inv_std = rsqrtf(var + eps);\n        float normalized_val = (x_val - mean) * inv_std;\n        \n        output[i] = s_weight[feature_idx] * normalized_val + s_bias[feature_idx];\n    }\n}\n\nvoid batch_norm_forward_optimized(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int shared_mem_size = 4 * num_features * sizeof(float);\n    int num_blocks = min(65535, (batch_size * num_features * spatial_dim + block_size - 1) / block_size);\n\n    batch_norm_kernel_optimized<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimize memory access patterns by processing features in parallel and using shared memory for batch statistics during training, while maintaining the original numerical precision and correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* shared_mean = shared_data;\n    float* shared_var = &shared_data[num_features];\n    float* shared_weight = &shared_data[2 * num_features];\n    float* shared_bias = &shared_data[3 * num_features];\n\n    int feature_idx = blockIdx.x;\n    int spatial_idx = threadIdx.x;\n\n    if (feature_idx < num_features && spatial_idx < spatial_dim) {\n        if (threadIdx.x == 0) {\n            shared_mean[feature_idx] = bn_running_mean[feature_idx];\n            shared_var[feature_idx] = bn_running_var[feature_idx];\n            shared_weight[feature_idx] = bn_weight[feature_idx];\n            shared_bias[feature_idx] = bn_bias[feature_idx];\n        }\n        __syncthreads();\n\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n\n        for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {\n            int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n            float x_val = x[idx];\n            float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n            output[idx] = weight * normalized_val + bias;\n        }\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 blocks(num_features);\n    dim3 threads(min(spatial_dim, 1024));\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<blocks, threads, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize by vectorizing the computation across spatial dimensions for each feature and batch, reducing thread divergence and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* x, const float* bn_weight, const float* bn_bias,\n    const float* bn_running_mean, const float* bn_running_var,\n    float* output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    int feature_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    int spatial_idx = threadIdx.x;\n\n    if (feature_idx < num_features && batch_idx < batch_size && spatial_idx < spatial_dim) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        \n        float mean = bn_running_mean[feature_idx];\n        float var = bn_running_var[feature_idx];\n        float x_val = x[idx];\n        \n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[idx] = bn_weight[feature_idx] * normalized_val + bn_bias[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 blocks(num_features, batch_size);\n    int threads = min(1024, spatial_dim);\n\n    batch_norm_kernel<<<blocks, threads>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize the kernel by using shared memory for batch statistics, vectorized memory access, and warp-level reductions for training mode calculations, while maintaining the original numerical precision.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float s_data[];\n    float* s_mean = s_data;\n    float* s_var = &s_data[num_features];\n    float* s_weight = &s_data[2*num_features];\n    float* s_bias = &s_data[3*num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        s_mean[feature_idx] = bn_running_mean[feature_idx];\n        s_var[feature_idx] = bn_running_var[feature_idx];\n        s_weight[feature_idx] = bn_weight[feature_idx];\n        s_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int total_elements = batch_size * num_features * spatial_dim;\n\n    for (int i = idx; i < total_elements; i += blockDim.x * gridDim.x) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = s_mean[feature_idx];\n        float var = s_var[feature_idx];\n        float weight = s_weight[feature_idx];\n        float bias = s_bias[feature_idx];\n\n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = min(1024, num_features);\n    int shared_mem_size = 4 * num_features * sizeof(float);\n\n    dim3 grid((batch_size * num_features * spatial_dim + block_size - 1) / block_size);\n    dim3 block(block_size);\n\n    batch_norm_kernel<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the kernel by coalescing memory accesses, using shared memory for batch norm parameters, and parallelizing across features and spatial dimensions with warp-level operations.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, \n    const float* __restrict__ bn_bias, const float* __restrict__ bn_running_mean, \n    const float* __restrict__ bn_running_var, float* __restrict__ output, \n    int batch_size, int num_features, int spatial_dim, float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* s_mean = shared_params;\n    float* s_var = &shared_params[num_features];\n    float* s_weight = &shared_params[2 * num_features];\n    float* s_bias = &shared_params[3 * num_features];\n\n    int tid = threadIdx.x;\n    int feature_idx = tid % num_features;\n    \n    // Load batch norm parameters into shared memory\n    if (tid < num_features) {\n        s_mean[feature_idx] = bn_running_mean[feature_idx];\n        s_var[feature_idx] = bn_running_var[feature_idx];\n        s_weight[feature_idx] = bn_weight[feature_idx];\n        s_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    \n    for (int i = idx; i < elements; i += blockDim.x * gridDim.x) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int f_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float x_val = x[i];\n        float mean = s_mean[f_idx];\n        float var = s_var[f_idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = s_weight[f_idx] * normalized_val + s_bias[f_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int grid_size = (batch_size * num_features * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid_size, block_size, shared_mem>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize the batch normalization kernel by vectorizing memory accesses, using shared memory for feature-specific parameters, and avoiding redundant computations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* shared_mean = shared_params;\n    float* shared_var = &shared_mean[num_features];\n    float* shared_weight = &shared_var[num_features];\n    float* shared_bias = &shared_weight[num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements_per_block = blockDim.x;\n    int total_elements = batch_size * num_features * spatial_dim;\n\n    for (int i = idx; i < total_elements; i += elements_per_block * gridDim.x) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float x_val = x[i];\n        float inv_std = rsqrtf(shared_var[feature_idx] + eps);\n        float normalized_val = (x_val - shared_mean[feature_idx]) * inv_std;\n        output[i] = shared_weight[feature_idx] * normalized_val + shared_bias[feature_idx];\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int shared_mem_size = 4 * num_features * sizeof(float);\n    int num_blocks = min(65535, (batch_size * num_features * spatial_dim + block_size - 1) / block_size);\n\n    batch_norm_kernel<<<num_blocks, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimize the CUDA kernel by coalescing memory accesses, using shared memory for batch statistics, and parallelizing across features and spatial dimensions while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_data[];\n    float* mean_shared = shared_data;\n    float* var_shared = &shared_data[num_features];\n    float* weight_shared = &shared_data[2 * num_features];\n    float* bias_shared = &shared_data[3 * num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        mean_shared[feature_idx] = bn_running_mean[feature_idx];\n        var_shared[feature_idx] = bn_running_var[feature_idx];\n        weight_shared[feature_idx] = bn_weight[feature_idx];\n        bias_shared[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.y + threadIdx.y;\n    int spatial_idx = threadIdx.x;\n    \n    if (idx < batch_size * num_features && spatial_idx < spatial_dim) {\n        int batch_idx = idx / num_features;\n        feature_idx = idx % num_features;\n        \n        int input_idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        float x_val = x[input_idx];\n        \n        float mean = mean_shared[feature_idx];\n        float var = var_shared[feature_idx];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        \n        float output_val = weight_shared[feature_idx] * normalized_val + bias_shared[feature_idx];\n        output[input_idx] = output_val;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 block(32, 32);\n    int grid = (batch_size * num_features + block.y - 1) / block.y;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel<<<grid, block, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimize the batch normalization kernel by vectorizing memory accesses, using shared memory for feature-wise parameters, and reducing thread divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel_optimized(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    extern __shared__ float shared_params[];\n    float* shared_mean = shared_params;\n    float* shared_var = &shared_mean[num_features];\n    float* shared_weight = &shared_var[num_features];\n    float* shared_bias = &shared_weight[num_features];\n\n    int feature_idx = threadIdx.x;\n    if (feature_idx < num_features) {\n        shared_mean[feature_idx] = bn_running_mean[feature_idx];\n        shared_var[feature_idx] = bn_running_var[feature_idx];\n        shared_weight[feature_idx] = bn_weight[feature_idx];\n        shared_bias[feature_idx] = bn_bias[feature_idx];\n    }\n    __syncthreads();\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int elements = batch_size * num_features * spatial_dim;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = idx; i < elements; i += stride) {\n        int batch_idx = i / (num_features * spatial_dim);\n        int feature_idx = (i / spatial_dim) % num_features;\n        int spatial_idx = i % spatial_dim;\n\n        float mean = shared_mean[feature_idx];\n        float var = shared_var[feature_idx];\n        float weight = shared_weight[feature_idx];\n        float bias = shared_bias[feature_idx];\n\n        float x_val = x[i];\n        float normalized_val = (x_val - mean) * rsqrtf(var + eps);\n        output[i] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward_optimized(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    int block_size = 256;\n    int grid_size = (batch_size * spatial_dim + block_size - 1) / block_size;\n    size_t shared_mem_size = 4 * num_features * sizeof(float);\n\n    batch_norm_kernel_optimized<<<grid_size, block_size, shared_mem_size>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize by vectorizing the computation across spatial dimensions for each feature and batch, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n__global__ void batch_norm_kernel(\n    const float* __restrict__ x, const float* __restrict__ bn_weight, const float* __restrict__ bn_bias,\n    const float* __restrict__ bn_running_mean, const float* __restrict__ bn_running_var,\n    float* __restrict__ output, int batch_size, int num_features, int spatial_dim,\n    float eps, bool training) {\n\n    int feature_idx = blockIdx.x;\n    int batch_idx = blockIdx.y;\n    int spatial_start = threadIdx.x;\n    int spatial_stride = blockDim.x;\n\n    if (feature_idx >= num_features || batch_idx >= batch_size) return;\n\n    float mean = bn_running_mean[feature_idx];\n    float var = bn_running_var[feature_idx];\n    float weight = bn_weight[feature_idx];\n    float bias = bn_bias[feature_idx];\n    float inv_std = rsqrtf(var + eps);\n\n    for (int spatial_idx = spatial_start; spatial_idx < spatial_dim; spatial_idx += spatial_stride) {\n        int idx = batch_idx * num_features * spatial_dim + feature_idx * spatial_dim + spatial_idx;\n        float x_val = x[idx];\n        float normalized_val = (x_val - mean) * inv_std;\n        output[idx] = weight * normalized_val + bias;\n    }\n}\n\nvoid batch_norm_forward(\n    torch::Tensor x, torch::Tensor bn_weight, torch::Tensor bn_bias,\n    torch::Tensor bn_running_mean, torch::Tensor bn_running_var,\n    torch::Tensor output, float eps, bool training) {\n\n    int batch_size = x.size(0);\n    int num_features = x.size(1);\n    int spatial_dim = x.numel() / (batch_size * num_features);\n\n    dim3 blocks(num_features, batch_size);\n    int threads = min(1024, spatial_dim);\n\n    batch_norm_kernel<<<blocks, threads>>>(\n        x.data_ptr<float>(), bn_weight.data_ptr<float>(), bn_bias.data_ptr<float>(),\n        bn_running_mean.data_ptr<float>(), bn_running_var.data_ptr<float>(),\n        output.data_ptr<float>(), batch_size, num_features, spatial_dim,\n        eps, training);\n\n    cudaDeviceSynchronize();\n}",
        "score": null
    }
]