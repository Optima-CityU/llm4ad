[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the matrix multiplication with transpose (A.T @ B) by using shared memory for data reuse, increasing thread block size, and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_row = tile_offset + threadIdx.y;\n        int a_col = row;\n        int b_row = tile_offset + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (tiled_i < K && row < M) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (tiled_i < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.y] * Bs[threadIdx.x][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.y] * Bs[threadIdx.x][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimized implementation using shared memory for tiling and loop unrolling to improve memory access patterns and reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[a_col * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n            value += As[threadIdx.x][k+1] * Bs[k+1][threadIdx.y];\n            value += As[threadIdx.x][k+2] * Bs[k+2][threadIdx.y];\n            value += As[threadIdx.x][k+3] * Bs[k+3][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the matrix multiplication with A transposed by using shared memory to cache tiles of A and B, and performing the computation with improved memory access patterns and thread utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (tiled_i < K && row < M) {\n            As[threadIdx.y][threadIdx.x] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (tiled_i < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.x] * Bs[threadIdx.y][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((N + blockDim.x - 1) / blockDim.x, (M + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B, increasing thread block size, and unrolling the inner loop for better memory access patterns and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_row = tile_offset + threadIdx.y;\n        int a_col = row;\n        int b_row = tile_offset + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the matrix multiplication by using shared memory to cache tiles of input matrices A and B, and perform the computation with better memory locality and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B matrices, reducing global memory accesses, and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.y] * Bs[threadIdx.x][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tiling, loop unrolling, and increasing block size to improve memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int b_col = tiled_k + threadIdx.x;\n\n        if (row < M && a_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_col < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[k][threadIdx.x] * Bs[threadIdx.y][k];\n            if (k + 1 < TILE_SIZE) value += As[k+1][threadIdx.x] * Bs[threadIdx.y][k+1];\n            if (k + 2 < TILE_SIZE) value += As[k+2][threadIdx.x] * Bs[threadIdx.y][k+2];\n            if (k + 3 < TILE_SIZE) value += As[k+3][threadIdx.x] * Bs[threadIdx.y][k+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize by using shared memory for tiled matrix multiplication, transposing A during loading to shared memory to improve memory coalescing, and increasing block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the kernel by using shared memory for tiling, loop unrolling, and increasing block dimensions for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_row = tile_offset + threadIdx.y;\n        int b_row = tile_offset + threadIdx.x;\n\n        if (row < M && a_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_row < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll UNROLL_FACTOR\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tiling, loop unrolling, and increasing block size to improve memory access patterns and arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int load_row = tiled_k + threadIdx.y;\n        int load_col_A = row;\n        int load_col_B = col;\n\n        if (load_row < K && load_col_A < M) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * M + load_col_A];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (load_row < K && load_col_B < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col_B];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n            if (i+1 < TILE_SIZE) value += As[i+1][threadIdx.x] * Bs[i+1][threadIdx.y];\n            if (i+2 < TILE_SIZE) value += As[i+2][threadIdx.x] * Bs[i+2][threadIdx.y];\n            if (i+3 < TILE_SIZE) value += As[i+3][threadIdx.x] * Bs[i+3][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimized implementation using shared memory for tiling, loop unrolling, and increased block size for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int load_row = tiled_k + threadIdx.y;\n        int load_col_A = row;\n        int load_col_B = col;\n\n        if (load_row < K && load_col_A < M) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * M + load_col_A];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (load_row < K && load_col_B < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col_B];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n            sum += As[k+1][threadIdx.x] * Bs[k+1][threadIdx.y];\n            sum += As[k+2][threadIdx.x] * Bs[k+2][threadIdx.y];\n            sum += As[k+3][threadIdx.x] * Bs[k+3][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B, and increase thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tile-based matrix multiplication, loop unrolling, and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n            value += As[k+1][threadIdx.x] * Bs[k+1][threadIdx.y];\n            value += As[k+2][threadIdx.x] * Bs[k+2][threadIdx.y];\n            value += As[k+3][threadIdx.x] * Bs[k+3][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize by using shared memory for tile-based matrix multiplication, transposing matrix A in shared memory to improve memory coalescing, and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B matrices, unrolling the inner loop, and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        int b_row = tiled_k + threadIdx.y;\n        int b_col = col;\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimized implementation using shared memory for tile-based matrix multiplication with transposed A, improved thread block configuration, and loop unrolling for better memory access patterns and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (a_row < K && row < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[k][threadIdx.x] * Bs[threadIdx.y][k];\n            sum += As[k+1][threadIdx.x] * Bs[threadIdx.y][k+1];\n            sum += As[k+2][threadIdx.x] * Bs[threadIdx.y][k+2];\n            sum += As[k+3][threadIdx.x] * Bs[threadIdx.y][k+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B matrices, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tiling, loop unrolling, and increasing thread block size to improve memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        int b_row = tiled_k + threadIdx.y;\n        int b_col = col;\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the kernel by using shared memory to cache tiles of matrices A and B, and performing the matrix multiplication with improved memory access patterns and thread utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_row = threadIdx.y + tile_offset;\n        int a_col = threadIdx.x;\n        if (a_row < K && row < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        int b_row = threadIdx.y + tile_offset;\n        int b_col = threadIdx.x;\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the kernel by using shared memory for tiling to reduce global memory accesses and increase memory bandwidth utilization, while maintaining correct matrix multiplication with transposed A.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int b_col = tiled_k + threadIdx.x;\n\n        if (row < M && a_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && b_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B matrices, increasing thread block size, and unrolling the inner loop for better memory access patterns and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define BLOCK_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int aRow = tiledK + threadIdx.y;\n        int aCol = row;\n        int bRow = tiledK + threadIdx.x;\n        int bCol = col;\n\n        if (aRow < K && aCol < M) {\n            As[threadIdx.y][threadIdx.x] = A[aRow * M + aCol];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (bRow < K && bCol < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[bRow * N + bCol];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tiling, loop unrolling, and increasing block dimensions to better utilize the RTX 4090's resources while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int load_row = tiled_k + threadIdx.y;\n        int load_col = tiled_k + threadIdx.x;\n\n        if (row < M && load_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && load_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n            if (k + 1 < TILE_SIZE) value += As[k+1][threadIdx.x] * Bs[k+1][threadIdx.y];\n            if (k + 2 < TILE_SIZE) value += As[k+2][threadIdx.x] * Bs[k+2][threadIdx.y];\n            if (k + 3 < TILE_SIZE) value += As[k+3][threadIdx.x] * Bs[k+3][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the kernel by using shared memory to cache tiles of A and B matrices, reducing global memory accesses, and increasing thread block utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the kernel by using shared memory to cache tiles of A and B, increasing thread block size, and unrolling the inner loop to reduce memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (tiled_i < K && row < M) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (tiled_i < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.y] * Bs[threadIdx.x][i];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tiled matrix multiplication, loop unrolling, and adjusting block/grid dimensions for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int load_row = tiled_k + threadIdx.y;\n        int load_col = tiled_k + threadIdx.x;\n\n        if (row < M && load_row < K) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && load_col < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_col * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n            if (k + 1 < TILE_SIZE) value += As[k+1][threadIdx.x] * Bs[k+1][threadIdx.y];\n            if (k + 2 < TILE_SIZE) value += As[k+2][threadIdx.x] * Bs[k+2][threadIdx.y];\n            if (k + 3 < TILE_SIZE) value += As[k+3][threadIdx.x] * Bs[k+3][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the matrix multiplication with A transposed by using shared memory to cache tiles of A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.y] * Bs[threadIdx.x][i];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper thread indexing and memory coalescing for A.T @ B operation.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (tiled_k < K && row < M) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (tiled_k < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[k][threadIdx.y] * Bs[threadIdx.x][k];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tiling, loop unrolling, and increasing thread block size for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n            value += As[k+1][threadIdx.x] * Bs[k+1][threadIdx.y];\n            value += As[k+2][threadIdx.x] * Bs[k+2][threadIdx.y];\n            value += As[k+3][threadIdx.x] * Bs[k+3][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B, increasing thread block size, and unrolling the inner loop for better memory access patterns and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[k][threadIdx.x] * Bs[threadIdx.y][k];\n            if (k+1 < TILE_SIZE) value += As[k+1][threadIdx.x] * Bs[threadIdx.y][k+1];\n            if (k+2 < TILE_SIZE) value += As[k+2][threadIdx.x] * Bs[threadIdx.y][k+2];\n            if (k+3 < TILE_SIZE) value += As[k+3][threadIdx.x] * Bs[threadIdx.y][k+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper thread coarsening and memory access patterns to reduce global memory accesses and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        int b_row = tiled_k + threadIdx.y;\n        int b_col = col;\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B, and increasing thread block size to 32x32 for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_i = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_i < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_i * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_i = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_i < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_i * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.y] * Bs[threadIdx.x][i];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimized implementation using shared memory for tiling to reduce global memory accesses and increase memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and thread block configuration for better utilization of GPU resources.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tiled matrix multiplication and adjusting block dimensions for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tiling, loop unrolling, and increasing block dimensions for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int load_row = tiled_k + threadIdx.y;\n        int load_col_A = row;\n        int load_col_B = col;\n\n        if (load_row < K && load_col_A < M) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * M + load_col_A];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (load_row < K && load_col_B < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col_B];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll UNROLL_FACTOR\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tiled matrix multiplication, better memory coalescing, and increased thread block size for improved occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the matrix multiplication with transposed A by using shared memory for tiling, loop unrolling, and increased occupancy with larger block sizes.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load A tile (transposed)\n        if (row < M && (tiled_k + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[(tiled_k + threadIdx.y) * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        // Load B tile\n        if (col < N && (tiled_k + threadIdx.x) < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[(tiled_k + threadIdx.x) * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n            value += As[threadIdx.x][k+1] * Bs[k+1][threadIdx.y];\n            value += As[threadIdx.x][k+2] * Bs[k+2][threadIdx.y];\n            value += As[threadIdx.x][k+3] * Bs[k+3][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the kernel by using shared memory for tiled matrix multiplication, transposing matrix B for better memory access patterns, and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n\n        // Load A tile (transposed)\n        if (row < M && (tile_offset + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[(tile_offset + threadIdx.y) * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        // Load B tile\n        if (col < N && (tile_offset + threadIdx.x) < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[(tile_offset + threadIdx.x) * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for tiling, loop unrolling, and increasing block dimensions for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel_optimized(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int b_col = tiled_k + threadIdx.x;\n\n        if (a_row < K && row < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_col < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[k][threadIdx.x] * Bs[threadIdx.y][k];\n            sum += As[k+1][threadIdx.x] * Bs[threadIdx.y][k+1];\n            sum += As[k+2][threadIdx.x] * Bs[threadIdx.y][k+2];\n            sum += As[k+3][threadIdx.x] * Bs[threadIdx.y][k+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel_optimized<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B, increasing thread block size to 32x32, and unrolling the inner loop for better memory access patterns and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the matrix multiplication by using shared memory for tiling, loop unrolling, and increasing thread block size to improve memory access patterns and arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[a_col * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n            value += As[threadIdx.x][k+1] * Bs[k+1][threadIdx.y];\n            value += As[threadIdx.x][k+2] * Bs[k+2][threadIdx.y];\n            value += As[threadIdx.x][k+3] * Bs[k+3][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimized implementation using shared memory for tiling to reduce global memory accesses and improve memory coalescing, with careful thread indexing for correct transpose operation.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        \n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory to cache tiles of A and B, unrolling loops, and increasing block size for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        int b_row = tiled_k + threadIdx.y;\n        int b_col = col;\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[i][threadIdx.x] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimize by using shared memory to cache tiles of A and B, increasing data reuse and reducing global memory accesses, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize the matrix multiplication with A transposed by using shared memory to cache tiles of A and B, and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper thread coarsening and bank conflict avoidance for A.T @ B operation.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n        int a_row = tile_offset + threadIdx.y;\n        int a_col = row;\n        int b_row = tile_offset + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the matmul_transpose kernel by utilizing shared memory for tiling, loop unrolling, and increasing thread block size to improve memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.x;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[k][threadIdx.x] * Bs[threadIdx.y][k];\n            if (k+1 < TILE_SIZE) value += As[k+1][threadIdx.x] * Bs[threadIdx.y][k+1];\n            if (k+2 < TILE_SIZE) value += As[k+2][threadIdx.x] * Bs[threadIdx.y][k+2];\n            if (k+3 < TILE_SIZE) value += As[k+3][threadIdx.x] * Bs[threadIdx.y][k+3];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize the matmul_transpose kernel by using shared memory for data reuse, loop unrolling, and adjusting block dimensions for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.y;\n        int b_col = tiled_k + threadIdx.x;\n\n        if (a_row < K && row < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + row];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_col < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_col * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll UNROLL_FACTOR\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.x] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimized implementation using shared memory for tiling to reduce global memory accesses and improve memory coalescing, with careful handling of thread block dimensions and matrix boundaries.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + blockDim.x - 1) / blockDim.x, (N + blockDim.y - 1) / blockDim.y);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper thread indexing and memory coalescing for A.T access pattern.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.y * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[tiled_k * M + row];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((M + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimized implementation using shared memory for tiled matrix multiplication with proper memory coalescing and bank conflict avoidance, while maintaining correctness for A.T @ B operation.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_transpose_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_row = tiled_k + threadIdx.x;\n        int a_col = row;\n        int b_row = tiled_k + threadIdx.y;\n        int b_col = col;\n\n        if (a_row < K && a_col < M) {\n            As[threadIdx.y][threadIdx.x] = A[a_row * M + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[k][threadIdx.y] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_transpose(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(1);\n    int K = A.size(0);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_transpose_kernel<<<gridDim, blockDim>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_transpose, \"Matrix multiplication with transpose (A.T @ B)\");\n}",
        "score": null
    }
]