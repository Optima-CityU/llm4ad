[
    {
        "sample_order": 1,
        "algorithm": "{Optimized using shared memory tiling with 16x16 blocks to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -3.5636927604675295
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimized matrix multiplication using shared memory tiling with 16x16 tiles to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -3.5631999492645265
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimized implementation using shared memory tiling with 16x16 thread blocks and 16x16 tile sizes to reduce global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -3.5618655681610107
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimized using tiling with shared memory to reduce global memory accesses and increase memory bandwidth utilization, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -3.5641151666641235
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimized matrix multiplication using shared memory tiling with 16x16 thread blocks to improve memory access patterns and reduce global memory bandwidth.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -3.571798396110535
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimized implementation using shared memory for tile-based matrix multiplication with thread coarsening and improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int A_col = tiled_k + threadIdx.x;\n        int B_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && A_col < K) ? A[row * K + A_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (B_row < K && col < N) ? B[B_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.066105604171753
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimized using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -3.5668927907943724
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimized using shared memory tiling with 16x16 blocks to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -3.5716192007064818
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimized matrix multiplication using warp-level tiling and register accumulation to reduce shared memory usage and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_id = threadIdx.y;\n    int lane_id = threadIdx.x;\n    \n    int row = blockIdx.y * TILE_SIZE + warp_id;\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + WARP_SIZE - 1) / WARP_SIZE; ++t) {\n        int tiled_k = t * WARP_SIZE + lane_id;\n        float a_val = (row < M && tiled_k < K) ? A[row * K + tiled_k] : 0.0f;\n        float b_val = (col < N && tiled_k < K) ? B[tiled_k * N + col] : 0.0f;\n\n        for (int k = 0; k < WARP_SIZE; ++k) {\n            sum += __shfl_sync(0xffffffff, a_val, k) * __shfl_sync(0xffffffff, b_val, k);\n        }\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARP_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimized using register tiling with warp-level matrix multiplication and double buffering to hide memory latency while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 2\n#define THREADS_PER_WARP 32\n#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * THREADS_PER_WARP)\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / THREADS_PER_WARP;\n    const int lane_id = threadIdx.x % THREADS_PER_WARP;\n    \n    const int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    const int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float accum[TILE_SIZE / WARPS_PER_BLOCK] = {0.0f};\n    \n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int stage = 0;\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row + threadIdx.y < M && tiled_k + lane_id < K) {\n            As[stage][threadIdx.y][lane_id] = A[(row + threadIdx.y) * K + tiled_k + lane_id];\n        } else {\n            As[stage][threadIdx.y][lane_id] = 0.0f;\n        }\n        \n        if (col < N && tiled_k + threadIdx.y < K) {\n            Bs[stage][threadIdx.y][lane_id] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[stage][threadIdx.y][lane_id] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n                accum[i] += As[stage][warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + i][k] * Bs[stage][k][lane_id];\n            }\n        }\n        \n        stage = 1 - stage;\n        __syncthreads();\n    }\n    \n    for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n        if (row + i < M && col < N) {\n            C[(row + i) * N + col] = accum[i];\n        }\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(THREADS_PER_BLOCK, TILE_SIZE / WARPS_PER_BLOCK);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimized using register tiling with 4x4 thread blocks and loop unrolling to maximize instruction-level parallelism and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n#define THREAD_TILE 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int row = blockIdx.y * TILE_SIZE + threadIdx.y * THREAD_TILE;\n    const int col = blockIdx.x * TILE_SIZE + threadIdx.x * THREAD_TILE;\n\n    float accum[THREAD_TILE][THREAD_TILE] = {0.0f};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        __shared__ float As[TILE_SIZE][TILE_SIZE];\n        __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n        for (int i = 0; i < THREAD_TILE; ++i) {\n            for (int j = 0; j < THREAD_TILE; ++j) {\n                int load_row = row + i;\n                int load_col = t * TILE_SIZE + threadIdx.x * THREAD_TILE + j;\n                if (load_row < M && load_col < K) {\n                    As[threadIdx.y * THREAD_TILE + i][threadIdx.x * THREAD_TILE + j] = A[load_row * K + load_col];\n                } else {\n                    As[threadIdx.y * THREAD_TILE + i][threadIdx.x * THREAD_TILE + j] = 0.0f;\n                }\n\n                load_row = t * TILE_SIZE + threadIdx.y * THREAD_TILE + i;\n                load_col = col + j;\n                if (load_row < K && load_col < N) {\n                    Bs[threadIdx.y * THREAD_TILE + i][threadIdx.x * THREAD_TILE + j] = B[load_row * N + load_col];\n                } else {\n                    Bs[threadIdx.y * THREAD_TILE + i][threadIdx.x * THREAD_TILE + j] = 0.0f;\n                }\n            }\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            for (int i = 0; i < THREAD_TILE; ++i) {\n                for (int j = 0; j < THREAD_TILE; ++j) {\n                    accum[i][j] += As[threadIdx.y * THREAD_TILE + i][k] * Bs[k][threadIdx.x * THREAD_TILE + j];\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    for (int i = 0; i < THREAD_TILE; ++i) {\n        for (int j = 0; j < THREAD_TILE; ++j) {\n            if (row + i < M && col + j < N) {\n                C[(row + i) * N + (col + j)] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE / THREAD_TILE, TILE_SIZE / THREAD_TILE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimized matrix multiplication using register tiling with 4x4 thread-level tiles and warp-level communication to maximize register reuse and minimize shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n#define THREAD_TILE 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_row = blockIdx.y * TILE_SIZE + threadIdx.y / (TILE_SIZE/THREAD_TILE) * THREAD_TILE;\n    int warp_col = blockIdx.x * TILE_SIZE + threadIdx.x / (TILE_SIZE/THREAD_TILE) * THREAD_TILE;\n    \n    float accum[THREAD_TILE][THREAD_TILE] = {0};\n    float a_frag[THREAD_TILE];\n    float b_frag[THREAD_TILE];\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        for (int i = 0; i < THREAD_TILE; ++i) {\n            int row = warp_row + i;\n            int col = tiled_k + threadIdx.x % (TILE_SIZE/THREAD_TILE) + (threadIdx.y % (TILE_SIZE/THREAD_TILE)) * (TILE_SIZE/THREAD_TILE);\n            a_frag[i] = (row < M && col < K) ? A[row * K + col] : 0.0f;\n        }\n\n        for (int i = 0; i < THREAD_TILE; ++i) {\n            int row = tiled_k + threadIdx.y % (TILE_SIZE/THREAD_TILE) + (threadIdx.x % (TILE_SIZE/THREAD_TILE)) * (TILE_SIZE/THREAD_TILE);\n            int col = warp_col + i;\n            b_frag[i] = (row < K && col < N) ? B[row * N + col] : 0.0f;\n        }\n\n        for (int i = 0; i < THREAD_TILE; ++i) {\n            for (int j = 0; j < THREAD_TILE; ++j) {\n                accum[i][j] += a_frag[i] * b_frag[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < THREAD_TILE; ++i) {\n        for (int j = 0; j < THREAD_TILE; ++j) {\n            int row = warp_row + i;\n            int col = warp_col + j;\n            if (row < M && col < N) {\n                C[row * N + col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimized matrix multiplication using shared memory tiling with 32x32 thread blocks and loop unrolling to further improve memory access efficiency and computation throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.098480081558227
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize by using a warp-level approach with register tiling to reduce shared memory usage and improve instruction-level parallelism while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 2\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    \n    int row = blockIdx.y * (TILE_SIZE / WARPS_PER_BLOCK) + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        float a_val = 0.0f;\n        if (row < M && (tiled_k + lane_id) < K) {\n            a_val = A[row * K + tiled_k + lane_id];\n        }\n        \n        float b_val = 0.0f;\n        if (col < N && (tiled_k + warp_id * (TILE_SIZE / WARPS_PER_BLOCK)) < K) {\n            b_val = B[(tiled_k + warp_id * (TILE_SIZE / WARPS_PER_BLOCK)) * N + col];\n        }\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += __shfl_sync(0xffffffff, a_val, k) * \n                   __shfl_sync(0xffffffff, b_val, k);\n        }\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARP_SIZE * WARPS_PER_BLOCK);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + (TILE_SIZE / WARPS_PER_BLOCK) - 1) / (TILE_SIZE / WARPS_PER_BLOCK));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to further reduce global memory accesses and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.070483207702637
    },
    {
        "sample_order": 16,
        "algorithm": "{New implementation using shared memory tiling with 32x32 thread blocks and 32x32 tile sizes to increase parallelism and better utilize the RTX 4090's resources while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.064787244796753
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to increase thread block size and reduce memory access overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimized using register tiling and loop unrolling to reduce shared memory bank conflicts and increase instruction-level parallelism while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum[UNROLL_FACTOR] = {0.0f};\n    \n    for (int t = 0; t < K; t += TILE_SIZE * UNROLL_FACTOR) {\n        #pragma unroll\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            int tiled_k = t + u * TILE_SIZE;\n            \n            if (row < M && (tiled_k + threadIdx.x) < K) {\n                As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n            \n            if (col < N && (tiled_k + threadIdx.y) < K) {\n                Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n            \n            __syncthreads();\n            \n            #pragma unroll\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum[u] += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n            \n            __syncthreads();\n        }\n    }\n    \n    float total = 0.0f;\n    #pragma unroll\n    for (int u = 0; u < UNROLL_FACTOR; ++u) {\n        total += sum[u];\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = total;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to increase occupancy and reduce memory access overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.07480640411377
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to increase thread utilization and reduce synchronization overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimized using larger tile size (32x32) and loop unrolling to increase thread utilization and reduce memory access overhead while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.048627185821533
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks to increase occupancy and reduce global memory accesses while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks to increase occupancy and reduce memory transactions while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimized using larger tiling with shared memory (TILE_SIZE=32) and loop unrolling to further reduce global memory accesses and improve occupancy, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.0461407661437985
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimized using warp-level matrix multiplication with register tiling and cooperative fetching to maximize instruction-level parallelism and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define SUB_TILE_SIZE 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) / (WARP_SIZE / SUB_TILE_SIZE);\n    int warp_col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (warp_row >= M || warp_col >= N) return;\n    \n    float accum[SUB_TILE_SIZE] = {0.0f};\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int warp_k = lane_id / SUB_TILE_SIZE;\n    int sub_k = lane_id % SUB_TILE_SIZE;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        float a_frag[SUB_TILE_SIZE];\n        float b_frag[SUB_TILE_SIZE];\n        \n        int tiled_k = t * TILE_SIZE + warp_k * SUB_TILE_SIZE;\n        #pragma unroll\n        for (int i = 0; i < SUB_TILE_SIZE; ++i) {\n            int k = tiled_k + i;\n            a_frag[i] = (k < K) ? A[warp_row * K + k] : 0.0f;\n            b_frag[i] = (k < K) ? B[k * N + warp_col] : 0.0f;\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < SUB_TILE_SIZE; ++i) {\n            #pragma unroll\n            for (int j = 0; j < SUB_TILE_SIZE; ++j) {\n                accum[i] += a_frag[j] * b_frag[(j + i) % SUB_TILE_SIZE];\n            }\n        }\n    }\n    \n    if (lane_id < SUB_TILE_SIZE) {\n        C[warp_row * N + warp_col] = accum[lane_id];\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(128, 1);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + (WARP_SIZE / SUB_TILE_SIZE) - 1) / (WARP_SIZE / SUB_TILE_SIZE));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimized using register tiling with 4x4 thread blocks and loop unrolling to maximize register usage and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int row = blockIdx.y * blockDim.y * TILE_SIZE + threadIdx.y * TILE_SIZE;\n    const int col = blockIdx.x * blockDim.x * TILE_SIZE + threadIdx.x * TILE_SIZE;\n\n    float sum[TILE_SIZE][TILE_SIZE] = {0.0f};\n\n    for (int k = 0; k < K; ++k) {\n        float a[TILE_SIZE], b[TILE_SIZE];\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            a[i] = (row + i < M) ? A[(row + i) * K + k] : 0.0f;\n        }\n        \n        #pragma unroll\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            b[j] = (col + j < N) ? B[k * N + (col + j)] : 0.0f;\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            #pragma unroll\n            for (int j = 0; j < TILE_SIZE; ++j) {\n                sum[i][j] += a[i] * b[j];\n            }\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        #pragma unroll\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            if (row + i < M && col + j < N) {\n                C[(row + i) * N + (col + j)] = sum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(8, 8);\n    dim3 blocks((N + threads.x * TILE_SIZE - 1) / (threads.x * TILE_SIZE), \n                (M + threads.y * TILE_SIZE - 1) / (threads.y * TILE_SIZE));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimized matrix multiplication using register tiling with 4x4 tiles per thread and loop unrolling to maximize instruction-level parallelism and reduce shared memory pressure.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n#define SUB_TILE 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y * SUB_TILE;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x * SUB_TILE;\n\n    float c[SUB_TILE][SUB_TILE] = {0};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        __shared__ float As[TILE_SIZE][TILE_SIZE];\n        __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x * SUB_TILE;\n        int b_row = tiled_k + threadIdx.y * SUB_TILE;\n\n        #pragma unroll\n        for (int i = 0; i < SUB_TILE; ++i) {\n            if (row + i < M && a_col < K) {\n                As[threadIdx.y * SUB_TILE + i][threadIdx.x * SUB_TILE] = A[(row + i) * K + a_col];\n            }\n            if (b_row < K && col + i < N) {\n                Bs[threadIdx.y * SUB_TILE][threadIdx.x * SUB_TILE + i] = B[b_row * N + (col + i)];\n            }\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            #pragma unroll\n            for (int i = 0; i < SUB_TILE; ++i) {\n                #pragma unroll\n                for (int j = 0; j < SUB_TILE; ++j) {\n                    c[i][j] += As[threadIdx.y * SUB_TILE + i][k] * Bs[k][threadIdx.x * SUB_TILE + j];\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    #pragma unroll\n    for (int i = 0; i < SUB_TILE; ++i) {\n        #pragma unroll\n        for (int j = 0; j < SUB_TILE; ++j) {\n            if (row + i < M && col + j < N) {\n                C[(row + i) * N + (col + j)] = c[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE/SUB_TILE, TILE_SIZE/SUB_TILE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimized using register tiling with warp-level matrix multiplication and cooperative fetching to maximize compute throughput while minimizing shared memory usage.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / THREADS_PER_WARP;\n    const int lane_id = threadIdx.x % THREADS_PER_WARP;\n    \n    const int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    const int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float reg_a[TILE_SIZE / WARPS_PER_BLOCK] = {0};\n    float reg_b[TILE_SIZE / THREADS_PER_WARP] = {0};\n    float reg_c[TILE_SIZE / WARPS_PER_BLOCK][TILE_SIZE / THREADS_PER_WARP] = {0};\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            int fetch_row = row + i;\n            int fetch_col = tiled_k + lane_id;\n            if (fetch_row < M && fetch_col < K) {\n                reg_a[i] = A[fetch_row * K + fetch_col];\n            }\n        }\n        \n        for (int j = 0; j < TILE_SIZE / THREADS_PER_WARP; ++j) {\n            int fetch_row = tiled_k + warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + j;\n            int fetch_col = col;\n            if (fetch_row < K && fetch_col < N) {\n                reg_b[j] = B[fetch_row * N + fetch_col];\n            }\n        }\n        \n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            for (int j = 0; j < TILE_SIZE / THREADS_PER_WARP; ++j) {\n                reg_c[i][j] += reg_a[i] * reg_b[j];\n            }\n        }\n    }\n    \n    for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n        for (int j = 0; j < TILE_SIZE / THREADS_PER_WARP; ++j) {\n            int out_row = row + i;\n            int out_col = col + j * WARPS_PER_BLOCK;\n            if (out_row < M && out_col < N) {\n                atomicAdd(&C[out_row * N + out_col], reg_c[i][j]);\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARPS_PER_BLOCK * THREADS_PER_WARP);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to further reduce global memory accesses and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            sum += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            sum += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            sum += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.106739139556884
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to further improve memory access efficiency and reduce synchronization overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && tiled_k + threadIdx.x < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && tiled_k + threadIdx.y < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.049216032028198
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimized matrix multiplication using shared memory tiling with 32x32 blocks and loop unrolling to further reduce global memory accesses and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            sum += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            sum += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            sum += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.0626976013183596
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimized matrix multiplication using shared memory tiling with 32x32 tiles and loop unrolling to further reduce global memory accesses and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.062246370315552
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to increase occupancy and reduce memory accesses.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to increase occupancy and reduce memory latency.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.06023678779602
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to increase occupancy and reduce memory access overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.067590427398682
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to increase occupancy and reduce memory latency.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimized using larger tiling with shared memory (32x32 tiles) and loop unrolling to further reduce global memory accesses and improve occupancy, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.044076728820801
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks to increase thread block size and improve occupancy while maintaining coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.068249607086182
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks to increase thread block size and improve occupancy while maintaining coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.0676192283630375
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimized using larger tiling with shared memory (32x32 tiles) and loop unrolling to further reduce global memory accesses and improve occupancy, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimized using register tiling with warp-level matrix multiply and memory coalescing for improved throughput and reduced shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / THREADS_PER_WARP;\n    const int lane_id = threadIdx.x % THREADS_PER_WARP;\n    \n    const int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    const int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float accum[TILE_SIZE / WARPS_PER_BLOCK] = {0.0f};\n    \n    for (int t = 0; t < K; t += THREADS_PER_WARP) {\n        float a_frag[TILE_SIZE / WARPS_PER_BLOCK];\n        float b_frag = 0.0f;\n        \n        if (row + threadIdx.y < M && t + lane_id < K) {\n            a_frag[threadIdx.y] = A[(row + threadIdx.y) * K + t + lane_id];\n        } else {\n            a_frag[threadIdx.y] = 0.0f;\n        }\n        \n        if (col < N && t + lane_id < K) {\n            b_frag = B[(t + lane_id) * N + col];\n        }\n        \n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            accum[i] += a_frag[i] * b_frag;\n        }\n    }\n    \n    if (row + threadIdx.y < M && col < N) {\n        C[(row + threadIdx.y) * N + col] = accum[threadIdx.y];\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(THREADS_PER_WARP, WARPS_PER_BLOCK);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimized using register tiling with warp-level matrix multiplication and double buffering to hide memory latency and maximize compute throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / THREADS_PER_WARP;\n    const int lane_id = threadIdx.x % THREADS_PER_WARP;\n    const int row = blockIdx.y * WARPS_PER_BLOCK * TILE_SIZE + warp_id * TILE_SIZE;\n    const int col = blockIdx.x * TILE_SIZE;\n\n    float reg_a[2][TILE_SIZE/WARPS_PER_BLOCK] = {0};\n    float reg_b[2][TILE_SIZE] = {0};\n    float reg_c[TILE_SIZE/WARPS_PER_BLOCK][TILE_SIZE] = {0};\n\n    int load_a = 0;\n    int load_b = 0;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n            int a_row = row + i * WARPS_PER_BLOCK + lane_id / (TILE_SIZE/WARPS_PER_BLOCK);\n            int a_col = tiled_k + lane_id % (TILE_SIZE/WARPS_PER_BLOCK);\n            if (a_row < M && a_col < K) {\n                reg_a[load_a][i] = A[a_row * K + a_col];\n            } else {\n                reg_a[load_a][i] = 0.0f;\n            }\n        }\n\n        #pragma unroll\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int b_row = tiled_k + lane_id;\n            int b_col = col + j;\n            if (b_row < K && b_col < N) {\n                reg_b[load_b][j] = B[b_row * N + b_col];\n            } else {\n                reg_b[load_b][j] = 0.0f;\n            }\n        }\n\n        __syncwarp();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            #pragma unroll\n            for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n                reg_c[i][k] += reg_a[load_a][i] * reg_b[load_b][k];\n            }\n        }\n\n        load_a ^= 1;\n        load_b ^= 1;\n        __syncwarp();\n    }\n\n    #pragma unroll\n    for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n        #pragma unroll\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int c_row = row + i * WARPS_PER_BLOCK + lane_id / (TILE_SIZE/WARPS_PER_BLOCK);\n            int c_col = col + j;\n            if (c_row < M && c_col < N) {\n                C[c_row * N + c_col] = reg_c[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(THREADS_PER_WARP * WARPS_PER_BLOCK);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimized using warp-level matrix multiplication with register tiling and shared memory to maximize computational throughput while minimizing shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    \n    int warp_id = threadIdx.y;\n    int lane_id = threadIdx.x;\n    int row = blockIdx.y * TILE_SIZE * WARPS_PER_BLOCK + warp_id * TILE_SIZE;\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float sum[TILE_SIZE] = {0.0f};\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        for (int i = 0; i < TILE_SIZE; i += THREADS_PER_WARP) {\n            int load_row = row + i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK));\n            int load_col = tiled_k + (lane_id % (TILE_SIZE / WARPS_PER_BLOCK));\n            if (load_row < M && load_col < K) {\n                As[warp_id][i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = A[load_row * K + load_col];\n            } else {\n                As[warp_id][i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = 0.0f;\n            }\n            \n            if (col < N && (tiled_k + i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))) < K) {\n                Bs[warp_id][i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = B[(tiled_k + i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))) * N + col];\n            } else {\n                Bs[warp_id][i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = 0.0f;\n            }\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                sum[i] += As[warp_id][i][k] * Bs[warp_id][k][lane_id];\n            }\n        }\n        \n        __syncthreads();\n    }\n    \n    for (int i = 0; i < TILE_SIZE; ++i) {\n        if ((row + i) < M && col < N) {\n            C[(row + i) * N + col] = sum[i];\n        }\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(THREADS_PER_WARP, WARPS_PER_BLOCK);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimized using register tiling with 4x4 thread blocks and loop unrolling to maximize instruction-level parallelism and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 16\n#define THREAD_TILE 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y * THREAD_TILE;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x * THREAD_TILE;\n    \n    float sum[THREAD_TILE][THREAD_TILE] = {0};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        #pragma unroll\n        for (int i = 0; i < THREAD_TILE; ++i) {\n            int tiled_k = t * TILE_SIZE + threadIdx.x * THREAD_TILE + i;\n            if (row + threadIdx.y * THREAD_TILE < M && tiled_k < K) {\n                As[threadIdx.y * THREAD_TILE + i][threadIdx.x * THREAD_TILE + i] = A[(row + threadIdx.y * THREAD_TILE) * K + tiled_k];\n            }\n        }\n\n        #pragma unroll\n        for (int i = 0; i < THREAD_TILE; ++i) {\n            int tiled_k = t * TILE_SIZE + threadIdx.y * THREAD_TILE + i;\n            if (col + threadIdx.x * THREAD_TILE < N && tiled_k < K) {\n                Bs[threadIdx.y * THREAD_TILE + i][threadIdx.x * THREAD_TILE + i] = B[tiled_k * N + (col + threadIdx.x * THREAD_TILE)];\n            }\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            #pragma unroll\n            for (int i = 0; i < THREAD_TILE; ++i) {\n                #pragma unroll\n                for (int j = 0; j < THREAD_TILE; ++j) {\n                    sum[i][j] += As[threadIdx.y * THREAD_TILE + i][k] * Bs[k][threadIdx.x * THREAD_TILE + j];\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    #pragma unroll\n    for (int i = 0; i < THREAD_TILE; ++i) {\n        #pragma unroll\n        for (int j = 0; j < THREAD_TILE; ++j) {\n            if (row + i < M && col + j < N) {\n                C[(row + i) * N + (col + j)] = sum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE/THREAD_TILE, TILE_SIZE/THREAD_TILE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimized using shared memory tiling with 32x32 blocks and loop unrolling to further reduce global memory accesses and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul(torch::Tensor A, torch::Tensor B) {\n    const auto M = A.size(0);\n    const auto K = A.size(1);\n    const auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul, \"Matrix multiplication (CUDA)\");\n}",
        "score": -4.063539218902588
    }
]