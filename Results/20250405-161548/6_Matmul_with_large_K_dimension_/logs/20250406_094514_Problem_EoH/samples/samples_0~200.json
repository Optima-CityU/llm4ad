[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling to reduce global memory accesses and improve memory coalescing, with a tile size of 16x16.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int load_row = row;\n        int load_col = tiled_k + threadIdx.x;\n        if (load_row < M && load_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * K + load_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        load_row = tiled_k + threadIdx.y;\n        load_col = col;\n        if (load_row < K && load_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + threads_per_block.x - 1) / threads_per_block.x,\n                    (M + threads_per_block.y - 1) / threads_per_block.y);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -4.108595132827759
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiled matrix multiplication, increasing thread block size, and improving memory access patterns for better utilization of the RTX 4090's memory bandwidth and compute capabilities.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.85950722694397
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the CUDA matmul kernel using shared memory tiling to reduce global memory accesses and improve memory coalescing, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + threads_per_block.x - 1) / threads_per_block.x,\n                    (M + threads_per_block.y - 1) / threads_per_block.y);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -4.082886362075806
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling to reduce global memory accesses and improve memory coalescing, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -4.080774402618408
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the matmul kernel using shared memory tiling to reduce global memory accesses and improve memory coalescing, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -4.08064637184143
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimized implementation using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -4.069084787368775
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the CUDA matmul kernel using shared memory tiling to improve memory access patterns and reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -4.05567364692688
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimized implementation using shared memory tiling to reduce global memory accesses and improve memory coalescing, with block size tuned for RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.859705591201783
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the matrix multiplication using register tiling with warp-level operations and shared memory for better utilization of GPU resources and reduced shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n\n    int warp_id = threadIdx.y / (THREADS_PER_WARP / TILE_SIZE);\n    int lane_id = threadIdx.x;\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float reg_A[TILE_SIZE/WARPS_PER_BLOCK] = {0.0f};\n    float reg_B[TILE_SIZE/WARPS_PER_BLOCK] = {0.0f};\n    float reg_C[TILE_SIZE/WARPS_PER_BLOCK][TILE_SIZE/WARPS_PER_BLOCK] = {{0.0f}};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n\n        // Load A tile\n        if (row < M && (tiled_k + lane_id) < K) {\n            As[warp_id][threadIdx.y % (TILE_SIZE/WARPS_PER_BLOCK)][lane_id] = A[row * K + tiled_k + lane_id];\n        } else {\n            As[warp_id][threadIdx.y % (TILE_SIZE/WARPS_PER_BLOCK)][lane_id] = 0.0f;\n        }\n\n        // Load B tile\n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[warp_id][threadIdx.y % (TILE_SIZE/WARPS_PER_BLOCK)][lane_id] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[warp_id][threadIdx.y % (TILE_SIZE/WARPS_PER_BLOCK)][lane_id] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute with register tiling\n        for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n            reg_A[i] = As[warp_id][i][lane_id];\n            reg_B[i] = Bs[warp_id][i][lane_id];\n        }\n\n        for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n            for (int j = 0; j < TILE_SIZE/WARPS_PER_BLOCK; ++j) {\n                reg_C[i][j] += reg_A[i] * reg_B[j];\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // Write results\n    if (row < M && col < N) {\n        for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n            for (int j = 0; j < TILE_SIZE/WARPS_PER_BLOCK; ++j) {\n                int out_row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE/WARPS_PER_BLOCK) + i;\n                int out_col = blockIdx.x * TILE_SIZE + j;\n                if (out_row < M && out_col < N) {\n                    atomicAdd(&C[out_row * N + out_col], reg_C[i][j]);\n                }\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(THREADS_PER_WARP, WARPS_PER_BLOCK * (TILE_SIZE/WARPS_PER_BLOCK));\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the matrix multiplication using register tiling and loop unrolling to maximize instruction-level parallelism and reduce shared memory bank conflicts, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float accum[TILE_SIZE/UNROLL_FACTOR] = {0.0f};\n\n    for (int t = 0; t < K; t += TILE_SIZE) {\n        float a_frag[TILE_SIZE/UNROLL_FACTOR];\n        float b_frag[TILE_SIZE/UNROLL_FACTOR];\n\n        for (int i = 0; i < TILE_SIZE/UNROLL_FACTOR; i++) {\n            if (row < M && (t + threadIdx.x + i*UNROLL_FACTOR) < K) {\n                a_frag[i] = A[row * K + t + threadIdx.x + i*UNROLL_FACTOR];\n            } else {\n                a_frag[i] = 0.0f;\n            }\n\n            if ((t + threadIdx.y + i*UNROLL_FACTOR) < K && col < N) {\n                b_frag[i] = B[(t + threadIdx.y + i*UNROLL_FACTOR) * N + col];\n            } else {\n                b_frag[i] = 0.0f;\n            }\n        }\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE/UNROLL_FACTOR; i++) {\n            #pragma unroll\n            for (int j = 0; j < TILE_SIZE/UNROLL_FACTOR; j++) {\n                accum[i] += a_frag[i] * b_frag[j];\n            }\n        }\n    }\n\n    if (row < M && col < N) {\n        float sum = 0.0f;\n        for (int i = 0; i < TILE_SIZE/UNROLL_FACTOR; i++) {\n            sum += accum[i];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the matrix multiplication using warp-level tiling and register accumulation to reduce shared memory bank conflicts and improve instruction-level parallelism, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x;\n    const int warp_id = warp_row * blockDim.x + warp_col;\n    const int lane_id = warp_id % WARP_SIZE;\n\n    const int row = blockIdx.y * TILE_SIZE + warp_row;\n    const int col = blockIdx.x * TILE_SIZE + warp_col;\n\n    float accum = 0.0f;\n\n    for (int t = 0; t < (K + WARP_SIZE - 1) / WARP_SIZE; ++t) {\n        const int k = t * WARP_SIZE + lane_id;\n        const float a_val = (row < M && k < K) ? A[row * K + k] : 0.0f;\n        const float b_val = (col < N && k < K) ? B[k * N + col] : 0.0f;\n\n        for (int offset = 0; offset < WARP_SIZE; ++offset) {\n            const float a_shfl = __shfl_sync(0xffffffff, a_val, offset);\n            const float b_shfl = __shfl_sync(0xffffffff, b_val, offset);\n            accum += a_shfl * b_shfl;\n        }\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = accum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the CUDA matmul kernel using register tiling and warp-level matrix multiplication to maximize instruction-level parallelism and minimize shared memory usage.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n#define TILE_SIZE 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n\n    int row = blockIdx.y * BLOCK_SIZE + warp_id * TILE_SIZE;\n    int col = blockIdx.x * BLOCK_SIZE + lane_id * TILE_SIZE;\n\n    float reg_A[TILE_SIZE];\n    float reg_B[TILE_SIZE];\n    float reg_C[TILE_SIZE][TILE_SIZE] = {0.0f};\n\n    for (int t = 0; t < K; t += BLOCK_SIZE) {\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            int a_row = row + i;\n            int a_col = t + lane_id;\n            reg_A[i] = (a_row < M && a_col < K) ? A[a_row * K + a_col] : 0.0f;\n        }\n\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int b_row = t + lane_id;\n            int b_col = col + j;\n            reg_B[j] = (b_row < K && b_col < N) ? B[b_row * N + b_col] : 0.0f;\n        }\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            for (int j = 0; j < TILE_SIZE; ++j) {\n                reg_C[i][j] += reg_A[i] * reg_B[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int c_row = row + i;\n            int c_col = col + j;\n            if (c_row < M && c_col < N) {\n                C[c_row * N + c_col] = reg_C[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(WARP_SIZE * WARPS_PER_BLOCK);\n    dim3 num_blocks((N + BLOCK_SIZE - 1) / BLOCK_SIZE,\n                    (M + BLOCK_SIZE - 1) / BLOCK_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the matrix multiplication by using a warp-level tiling approach with 32x32 tiles and warp-level reductions to better utilize the RTX 4090's warp scheduling and shared memory bandwidth.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 2\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK * TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][WARPS_PER_BLOCK * TILE_SIZE];\n\n    int warp_id = threadIdx.y / 32;\n    int lane_id = threadIdx.y % 32;\n    int row = blockIdx.y * (WARPS_PER_BLOCK * TILE_SIZE) + warp_id * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.x * (WARPS_PER_BLOCK * TILE_SIZE) + lane_id;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + lane_id;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[warp_id * TILE_SIZE + threadIdx.x][lane_id] = A[row * K + a_col];\n        } else {\n            As[warp_id * TILE_SIZE + threadIdx.x][lane_id] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.x][warp_id * TILE_SIZE + lane_id] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.x][warp_id * TILE_SIZE + lane_id] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[warp_id * TILE_SIZE + threadIdx.x][k] * Bs[k][warp_id * TILE_SIZE + lane_id];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, WARPS_PER_BLOCK * 32);\n    dim3 num_blocks((N + WARPS_PER_BLOCK * TILE_SIZE - 1) / (WARPS_PER_BLOCK * TILE_SIZE),\n                    (M + WARPS_PER_BLOCK * TILE_SIZE - 1) / (WARPS_PER_BLOCK * TILE_SIZE));\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the CUDA matmul kernel using shared memory tiling with double buffering to overlap memory transfers with computation, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n        }\n\n        __syncthreads();\n\n        load_phase = 1 - load_phase;\n        compute_phase = 1 - compute_phase;\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + threads_per_block.x - 1) / threads_per_block.x,\n                    (M + threads_per_block.y - 1) / threads_per_block.y);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the CUDA matmul kernel by transposing matrix B for better memory access patterns and using shared memory tiling with 16x16 tile size to reduce global memory accesses and improve memory coalescing, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + tiled_k];  // B is accessed column-wise (transposed)\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[threadIdx.x][k];  // Dot product with transposed B\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    // Transpose B for better memory access pattern\n    auto B_t = B.t().contiguous();\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B_t.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -7.750246334075928
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the matrix multiplication using a warp-level tiling approach with 8x8 tiles and warp-level matrix multiply-accumulate operations to better utilize the GPU's warp-level parallelism while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) / warpSize;\n    int warp_col = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;\n    int lane_row = threadIdx.y % warpSize;\n    int lane_col = threadIdx.x % warpSize;\n\n    int row = warp_row * TILE_SIZE + lane_row;\n    int col = warp_col * TILE_SIZE + lane_col;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + lane_col;\n        int b_row = tiled_k + lane_row;\n\n        if (row < M && a_col < K) {\n            As[lane_row][lane_col] = A[row * K + a_col];\n        } else {\n            As[lane_row][lane_col] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[lane_row][lane_col] = B[b_row * N + col];\n        } else {\n            Bs[lane_row][lane_col] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[lane_row][k] * Bs[k][lane_col];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(32, 32);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the matrix multiplication using a larger tile size (32x32) with shared memory and loop unrolling to better utilize the RTX 4090's resources while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.851839971542359
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the CUDA matmul kernel using shared memory tiling with increased tile size (32x32) to better utilize the RTX 4090's resources while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + threads_per_block.x - 1) / threads_per_block.x,\n                    (M + threads_per_block.y - 1) / threads_per_block.y);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.9117760181427
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with double buffering to overlap memory transfers with computation, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_stage = 0;\n    int compute_stage = 1;\n\n    // Preload first tile\n    int tiled_k = 0;\n    int a_col = tiled_k + threadIdx.x;\n    int b_row = tiled_k + threadIdx.y;\n\n    if (row < M && a_col < K) {\n        As[load_stage][threadIdx.y][threadIdx.x] = A[row * K + a_col];\n    } else {\n        As[load_stage][threadIdx.y][threadIdx.x] = 0.0f;\n    }\n\n    if (b_row < K && col < N) {\n        Bs[load_stage][threadIdx.y][threadIdx.x] = B[b_row * N + col];\n    } else {\n        Bs[load_stage][threadIdx.y][threadIdx.x] = 0.0f;\n    }\n\n    __syncthreads();\n\n    for (int t = 1; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        tiled_k = t * TILE_SIZE;\n        a_col = tiled_k + threadIdx.x;\n        b_row = tiled_k + threadIdx.y;\n\n        // Load next tile while computing current\n        if (row < M && a_col < K) {\n            As[load_stage][threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[load_stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[load_stage][threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[load_stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_stage][threadIdx.y][k] * Bs[compute_stage][k][threadIdx.x];\n        }\n\n        __syncthreads();\n\n        // Swap stages\n        int temp = load_stage;\n        load_stage = compute_stage;\n        compute_stage = temp;\n    }\n\n    // Compute last tile\n    for (int k = 0; k < TILE_SIZE; ++k) {\n        value += As[compute_stage][threadIdx.y][k] * Bs[compute_stage][k][threadIdx.x];\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the CUDA matmul kernel using shared memory tiling with double buffering to overlap memory transfers with computation, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Preload next tile while computing current tile\n        if (row < M && a_col < K) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n        }\n\n        __syncthreads();\n\n        // Swap phases\n        load_phase = 1 - load_phase;\n        compute_phase = 1 - compute_phase;\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + threads_per_block.x - 1) / threads_per_block.x,\n                    (M + threads_per_block.y - 1) / threads_per_block.y);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the matrix multiplication using register tiling and loop unrolling to maximize instruction-level parallelism and reduce shared memory bank conflicts, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float accum[UNROLL_FACTOR] = {0.0f};\n    float a_reg[UNROLL_FACTOR];\n    float b_reg[UNROLL_FACTOR];\n\n    for (int t = 0; t < K; t += TILE_SIZE * UNROLL_FACTOR) {\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            int k = t + u * TILE_SIZE;\n            if (row < M && k + threadIdx.x < K) {\n                a_reg[u] = A[row * K + k + threadIdx.x];\n            } else {\n                a_reg[u] = 0.0f;\n            }\n            if (k + threadIdx.y < K && col < N) {\n                b_reg[u] = B[(k + threadIdx.y) * N + col];\n            } else {\n                b_reg[u] = 0.0f;\n            }\n        }\n\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                accum[u] += __shfl_sync(0xFFFFFFFF, a_reg[u], k) * \n                           __shfl_sync(0xFFFFFFFF, b_reg[u], k);\n            }\n        }\n    }\n\n    if (row < M && col < N) {\n        float sum = 0.0f;\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            sum += accum[u];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with double buffering to overlap memory transfers with computation, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Prefetch next tile while computing current tile\n        if (row < M && a_col + TILE_SIZE < K) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * K + a_col + TILE_SIZE];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row + TILE_SIZE < K && col < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[(b_row + TILE_SIZE) * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n        }\n\n        __syncthreads();\n\n        // Swap phases\n        load_phase = 1 - load_phase;\n        compute_phase = 1 - compute_phase;\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the matrix multiplication using larger shared memory tiles (32x32) and loop unrolling to improve memory access patterns and computational efficiency, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.859420824050903
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the CUDA matmul kernel using shared memory tiling with increased tile size (32x32) and loop unrolling to improve memory access patterns and computational efficiency while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + threads_per_block.x - 1) / threads_per_block.x,\n                    (M + threads_per_block.y - 1) / threads_per_block.y);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.921155261993408
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with a larger tile size of 32x32 to increase memory reuse and improve occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int load_row = row;\n        int load_col = tiled_k + threadIdx.x;\n        if (load_row < M && load_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * K + load_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        load_row = tiled_k + threadIdx.y;\n        load_col = col;\n        if (load_row < K && load_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + threads_per_block.x - 1) / threads_per_block.x,\n                    (M + threads_per_block.y - 1) / threads_per_block.y);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the matmul kernel by using a warp-level tiling approach with register accumulation to reduce shared memory bank conflicts and improve instruction-level parallelism, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) / WARP_SIZE;\n    int warp_col = blockIdx.x * blockDim.x + threadIdx.x;\n    int lane_id = threadIdx.y * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load A tile\n        int a_row = warp_row * WARP_SIZE + (lane_id / TILE_SIZE);\n        int a_col = tiled_k + (lane_id % TILE_SIZE);\n        if (a_row < M && a_col < K) {\n            As[lane_id / TILE_SIZE][lane_id % TILE_SIZE] = A[a_row * K + a_col];\n        } else {\n            As[lane_id / TILE_SIZE][lane_id % TILE_SIZE] = 0.0f;\n        }\n\n        // Load B tile\n        int b_row = tiled_k + (lane_id / TILE_SIZE);\n        int b_col = warp_col;\n        if (b_row < K && b_col < N) {\n            Bs[lane_id / TILE_SIZE][lane_id % TILE_SIZE] = B[b_row * N + b_col];\n        } else {\n            Bs[lane_id / TILE_SIZE][lane_id % TILE_SIZE] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Warp-level accumulation\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.899561548233033
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the matrix multiplication using warp-level tiling and register accumulation to reduce shared memory bank conflicts and improve instruction-level parallelism, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_id = threadIdx.y;\n    int lane_id = threadIdx.x;\n    \n    int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float accum[4] = {0.0f, 0.0f, 0.0f, 0.0f};\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        float a_frag[4];\n        float b_frag[4];\n        \n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            int a_row = row + i;\n            int a_col = tiled_k + lane_id;\n            a_frag[i] = (a_row < M && a_col < K) ? A[a_row * K + a_col] : 0.0f;\n            \n            int b_row = tiled_k + lane_id;\n            int b_col = col + i * THREADS_PER_WARP;\n            b_frag[i] = (b_row < K && b_col < N) ? B[b_row * N + b_col] : 0.0f;\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            #pragma unroll\n            for (int j = 0; j < 4; ++j) {\n                accum[i] += a_frag[j] * b_frag[j];\n            }\n        }\n    }\n    \n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int out_row = row + i;\n        int out_col = col;\n        if (out_row < M && out_col < N) {\n            C[out_row * N + out_col] = accum[i];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(THREADS_PER_WARP, WARPS_PER_BLOCK);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize matrix multiplication using warp-level tiling and register accumulation to reduce shared memory bank conflicts and improve thread utilization, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    const int row = blockIdx.y * TILE_SIZE + warp_id * 8;\n    const int col = blockIdx.x * TILE_SIZE + lane_id * 4;\n\n    float accum[8][4] = {{0.0f}};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        const int tiled_k = t * TILE_SIZE;\n        const int a_col = tiled_k + lane_id;\n        const int b_row = tiled_k + lane_id;\n\n        float a_frag[8];\n        float b_frag[4];\n\n        #pragma unroll\n        for (int i = 0; i < 8; ++i) {\n            if (row + i < M && a_col < K) {\n                a_frag[i] = A[(row + i) * K + a_col];\n            } else {\n                a_frag[i] = 0.0f;\n            }\n        }\n\n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            if (b_row < K && col + j < N) {\n                b_frag[j] = B[b_row * N + (col + j)];\n            } else {\n                b_frag[j] = 0.0f;\n            }\n        }\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            #pragma unroll\n            for (int i = 0; i < 8; ++i) {\n                #pragma unroll\n                for (int j = 0; j < 4; ++j) {\n                    accum[i][j] += a_frag[i] * b_frag[j];\n                }\n            }\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < 8; ++i) {\n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            if (row + i < M && col + j < N) {\n                C[(row + i) * N + (col + j)] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(WARP_SIZE * WARPS_PER_BLOCK);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the matrix multiplication using register tiling with warp-level operations and shared memory to maximize memory throughput and minimize bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n\n    int warp_id = threadIdx.y;\n    int lane_id = threadIdx.x;\n    int row = blockIdx.y * (WARPS_PER_BLOCK * TILE_SIZE) + warp_id * TILE_SIZE;\n    int col = blockIdx.x * TILE_SIZE;\n\n    float reg_A[TILE_SIZE];\n    float reg_B[TILE_SIZE];\n    float reg_C[TILE_SIZE] = {0.0f};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; i += THREADS_PER_WARP) {\n            int load_row = row + i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK));\n            int load_col = tiled_k + (lane_id % (TILE_SIZE / WARPS_PER_BLOCK));\n            if (load_row < M && load_col < K) {\n                As[warp_id][i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = A[load_row * K + load_col];\n            } else {\n                As[warp_id][i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = 0.0f;\n            }\n\n            load_row = tiled_k + i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK));\n            load_col = col + (lane_id % (TILE_SIZE / WARPS_PER_BLOCK));\n            if (load_row < K && load_col < N) {\n                Bs[warp_id][i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = B[load_row * N + load_col];\n            } else {\n                Bs[warp_id][i + (lane_id / (TILE_SIZE / WARPS_PER_BLOCK))][lane_id % (TILE_SIZE / WARPS_PER_BLOCK)] = 0.0f;\n            }\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            #pragma unroll\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                reg_C[i] += As[warp_id][i][k] * Bs[warp_id][k][lane_id];\n            }\n        }\n\n        __syncthreads();\n    }\n\n    #pragma unroll\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        int write_row = row + i;\n        int write_col = col + lane_id;\n        if (write_row < M && write_col < N) {\n            C[write_row * N + write_col] = reg_C[i];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(THREADS_PER_WARP, WARPS_PER_BLOCK);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + (WARPS_PER_BLOCK * TILE_SIZE) - 1) / (WARPS_PER_BLOCK * TILE_SIZE));\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with double buffering to overlap memory transfers with computation, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Prefetch next tile while computing current tile\n        if (row < M && a_col + TILE_SIZE < K) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * K + a_col + TILE_SIZE];\n        }\n        if (b_row + TILE_SIZE < K && col < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[(b_row + TILE_SIZE) * N + col];\n        }\n\n        // Load current tile\n        if (row < M && a_col < K) {\n            As[compute_phase][threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[compute_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[compute_phase][threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[compute_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n        }\n\n        __syncthreads();\n\n        // Swap phases\n        load_phase = 1 - load_phase;\n        compute_phase = 1 - compute_phase;\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -4.07807674407959
    },
    {
        "sample_order": 31,
        "algorithm": "{New kernel implementation using register tiling with warp-level matrix multiplication to reduce shared memory bank conflicts and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    \n    int row = blockIdx.y * TILE_SIZE * WARPS_PER_BLOCK + warp_id * TILE_SIZE;\n    int col = blockIdx.x * TILE_SIZE + (lane_id % TILE_SIZE);\n\n    float reg_A[TILE_SIZE] = {0.0f};\n    float reg_B[TILE_SIZE] = {0.0f};\n    float reg_C[TILE_SIZE] = {0.0f};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            int a_row = row + i;\n            int a_col = tiled_k + (lane_id % TILE_SIZE);\n            if (a_row < M && a_col < K) {\n                reg_A[i] = A[a_row * K + a_col];\n            }\n            \n            int b_row = tiled_k + i;\n            int b_col = col;\n            if (b_row < K && b_col < N) {\n                reg_B[i] = B[b_row * N + b_col];\n            }\n        }\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            #pragma unroll\n            for (int j = 0; j < TILE_SIZE; ++j) {\n                reg_C[i] += reg_A[j] * reg_B[(j + i) % TILE_SIZE];\n            }\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        int c_row = row + i;\n        int c_col = col;\n        if (c_row < M && c_col < N) {\n            C[c_row * N + c_col] = reg_C[i];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(WARP_SIZE * WARPS_PER_BLOCK);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK));\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with double buffering to overlap memory transfers with computation, using a tile size of 16x16.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int t = 0;\n    int tiled_k = t * TILE_SIZE;\n    int a_col = tiled_k + threadIdx.x;\n    int b_row = tiled_k + threadIdx.y;\n\n    As[0][threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n    Bs[0][threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n    __syncthreads();\n\n    for (t = 1; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        tiled_k = t * TILE_SIZE;\n        a_col = tiled_k + threadIdx.x;\n        b_row = tiled_k + threadIdx.y;\n\n        int next = t % 2;\n        int curr = (t - 1) % 2;\n\n        if (t < (K + TILE_SIZE - 1) / TILE_SIZE - 1) {\n            As[next][threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n            Bs[next][threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n        }\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[curr][threadIdx.y][k] * Bs[curr][k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the matrix multiplication by using a larger tile size (32x32) with warp-level tiling and register accumulation to better utilize the RTX 4090's resources while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n\n    int warp_id = threadIdx.y / (THREADS_PER_WARP / TILE_SIZE);\n    int lane_id = threadIdx.x + (threadIdx.y % (THREADS_PER_WARP / TILE_SIZE)) * TILE_SIZE;\n\n    int row = blockIdx.y * TILE_SIZE * WARPS_PER_BLOCK + warp_id * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + lane_id;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[warp_id][threadIdx.x][lane_id] = A[row * K + a_col];\n        } else {\n            As[warp_id][threadIdx.x][lane_id] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[warp_id][threadIdx.x][lane_id] = B[b_row * N + col];\n        } else {\n            Bs[warp_id][threadIdx.x][lane_id] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[warp_id][threadIdx.x][k] * Bs[warp_id][k][lane_id];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, WARPS_PER_BLOCK * (THREADS_PER_WARP / TILE_SIZE));\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK));\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the matmul kernel using larger shared memory tiles (32x32) and loop unrolling to improve memory access patterns and arithmetic intensity, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.8795839786529545
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with double buffering to overlap memory transfers with computation, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Load next tile while computing current tile\n        if (row < M && a_col < K) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n        }\n\n        __syncthreads();\n\n        // Swap phases\n        load_phase = 1 - load_phase;\n        compute_phase = 1 - compute_phase;\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with double buffering to overlap memory transfers with computation, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_stage = 0;\n    int compute_stage = 0;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[load_stage][threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[load_stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[load_stage][threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[load_stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (t > 0) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                value += As[compute_stage][threadIdx.y][k] * Bs[compute_stage][k][threadIdx.x];\n            }\n        }\n\n        compute_stage = load_stage;\n        load_stage = 1 - load_stage;\n        __syncthreads();\n    }\n\n    for (int k = 0; k < TILE_SIZE; ++k) {\n        value += As[compute_stage][threadIdx.y][k] * Bs[compute_stage][k][threadIdx.x];\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -4.104908847808838
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the matrix multiplication using register tiling and warp-level operations to maximize instruction-level parallelism and reduce shared memory usage, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.y;\n    const int lane_id = threadIdx.x;\n    const int warp_row = warp_id / (TILE_SIZE / THREADS_PER_WARP);\n    const int warp_col = warp_id % (TILE_SIZE / THREADS_PER_WARP);\n\n    const int row = blockIdx.y * TILE_SIZE + warp_row * THREADS_PER_WARP + lane_id;\n    const int col = blockIdx.x * TILE_SIZE + warp_col * THREADS_PER_WARP + lane_id;\n\n    float reg_a[THREADS_PER_WARP];\n    float reg_b[THREADS_PER_WARP];\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + THREADS_PER_WARP - 1) / THREADS_PER_WARP; ++t) {\n        int tiled_k = t * THREADS_PER_WARP;\n        int a_col = tiled_k + lane_id;\n        int b_row = tiled_k + lane_id;\n\n        reg_a[lane_id] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        reg_b[lane_id] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        for (int k = 0; k < THREADS_PER_WARP; ++k) {\n            value += reg_a[k] * reg_b[k];\n        }\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(THREADS_PER_WARP, WARPS_PER_BLOCK);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with double buffering to overlap memory transfers with computation, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Prefetch next tile while computing current tile\n        if (row < M && a_col < K) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n        }\n\n        __syncthreads();\n\n        // Swap phases\n        int temp = load_phase;\n        load_phase = compute_phase;\n        compute_phase = temp;\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with a larger 32x32 tile size and loop unrolling to further reduce global memory accesses and improve instruction-level parallelism, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.854137563705445
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with a larger tile size (32x32) and loop unrolling to improve memory access efficiency and computation throughput, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Load current tile\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute with loop unrolling\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.8577759742736815
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with increased tile size (32x32) and loop unrolling to improve memory access patterns and computational efficiency, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.853548908233643
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the matmul kernel using shared memory tiling with double buffering to overlap memory transfers with computation, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int load_phase = 0;\n    int compute_phase = 1;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        // Prefetch next tile while computing current\n        int next_t = t + 1;\n        int next_tiled_k = next_t * TILE_SIZE;\n        int next_a_col = next_tiled_k + threadIdx.x;\n        int next_b_row = next_tiled_k + threadIdx.y;\n\n        if (next_t < (K + TILE_SIZE - 1) / TILE_SIZE) {\n            As[load_phase][threadIdx.y][threadIdx.x] = (row < M && next_a_col < K) ? A[row * K + next_a_col] : 0.0f;\n            Bs[load_phase][threadIdx.y][threadIdx.x] = (next_b_row < K && col < N) ? B[next_b_row * N + col] : 0.0f;\n        }\n\n        // Compute current tile\n        if (t > 0) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n            }\n        }\n\n        __syncthreads();\n\n        // Swap phases\n        int temp = load_phase;\n        load_phase = compute_phase;\n        compute_phase = temp;\n    }\n\n    // Compute last tile\n    for (int k = 0; k < TILE_SIZE; ++k) {\n        value += As[compute_phase][threadIdx.y][k] * Bs[compute_phase][k][threadIdx.x];\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the matrix multiplication using register tiling and loop unrolling to maximize instruction-level parallelism and reduce shared memory bank conflicts, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float accum[UNROLL_FACTOR] = {0.0f};\n    float a_reg[UNROLL_FACTOR];\n    float b_reg[UNROLL_FACTOR];\n\n    for (int t = 0; t < K; t += TILE_SIZE * UNROLL_FACTOR) {\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            int tiled_k = t + u * TILE_SIZE;\n            if (row < M && tiled_k + threadIdx.x < K) {\n                a_reg[u] = A[row * K + tiled_k + threadIdx.x];\n            } else {\n                a_reg[u] = 0.0f;\n            }\n            if (tiled_k + threadIdx.y < K && col < N) {\n                b_reg[u] = B[(tiled_k + threadIdx.y) * N + col];\n            } else {\n                b_reg[u] = 0.0f;\n            }\n        }\n\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            accum[u] += a_reg[u] * b_reg[u];\n        }\n    }\n\n    float sum = 0.0f;\n    for (int u = 0; u < UNROLL_FACTOR; ++u) {\n        sum += accum[u];\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the matrix multiplication using register tiling and loop unrolling to maximize instruction-level parallelism and reduce shared memory bank conflicts, while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float regA[UNROLL_FACTOR];\n    float regB[UNROLL_FACTOR];\n    float value = 0.0f;\n\n    for (int t = 0; t < K; t += TILE_SIZE * UNROLL_FACTOR) {\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            int k = t + u * TILE_SIZE;\n            if (row < M && k + threadIdx.x < K) {\n                regA[u] = A[row * K + k + threadIdx.x];\n            } else {\n                regA[u] = 0.0f;\n            }\n            if (col < N && k + threadIdx.y < K) {\n                regB[u] = B[(k + threadIdx.y) * N + col];\n            } else {\n                regB[u] = 0.0f;\n            }\n        }\n\n        for (int u = 0; u < UNROLL_FACTOR; ++u) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                value += regA[u] * __shfl_sync(0xffffffff, regB[u], k);\n            }\n        }\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the matmul kernel using shared memory tiling with increased tile size (32x32) to better utilize the RTX 4090's resources while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        As[threadIdx.y][threadIdx.x] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;\n        Bs[threadIdx.y][threadIdx.x] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);\n}",
        "score": -5.854012823104858
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the matrix multiplication using shared memory tiling with a larger tile size (32x32) to better utilize the RTX 4090's resources while maintaining correctness through proper boundary checks.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads_per_block(TILE_SIZE, TILE_SIZE);\n    dim3 num_blocks((N + TILE_SIZE - 1) / TILE_SIZE,\n                    (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<num_blocks, threads_per_block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda);",
        "score": null
    }
]