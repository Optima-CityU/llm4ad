[
    {
        "algorithm": "{Optimize the CUDA matmul kernel using tiling with shared memory to reduce global memory accesses and improve memory coalescing, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; k++) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -39.980879592895505
    },
    {
        "algorithm": "{Optimized matrix multiplication using shared memory tiling with 16x16 blocks to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -40.440996170043945
    },
    {
        "algorithm": "{Optimize the CUDA matmul kernel by implementing tiled matrix multiplication with shared memory to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -40.82377891540527
    },
    {
        "algorithm": "{Optimized implementation using shared memory tiling to reduce global memory accesses and improve memory coalescing, with 16x16 thread blocks and appropriate grid sizing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -93.08566513061524
    },
    {
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling and loop unrolling to improve memory access patterns and reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int aRow = row;\n        int aCol = tiledK + threadIdx.y;\n        int bRow = tiledK + threadIdx.x;\n        int bCol = col;\n\n        if (aRow < M && aCol < K) {\n            As[threadIdx.x][threadIdx.y] = A[aRow * K + aCol];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (bRow < K && bCol < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[bRow * N + bCol];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -93.10267562866211
    }
]