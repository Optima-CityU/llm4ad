[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing tiled matrix multiplication with shared memory to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -40.82377891540527
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimized matrix multiplication using shared memory tiling with 16x16 blocks to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -40.440996170043945
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the CUDA matmul kernel using tiling with shared memory to reduce global memory accesses and improve memory coalescing, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; k++) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -39.980879592895505
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimized implementation using shared memory tiling to reduce global memory accesses and improve memory coalescing, with 16x16 thread blocks and appropriate grid sizing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.y;\n        int b_row = tiled_k + threadIdx.x;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + a_col];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -93.08566513061524
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling and loop unrolling to improve memory access patterns and reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int aRow = row;\n        int aCol = tiledK + threadIdx.y;\n        int bRow = tiledK + threadIdx.x;\n        int bCol = col;\n\n        if (aRow < M && aCol < K) {\n            As[threadIdx.x][threadIdx.y] = A[aRow * K + aCol];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if (bRow < K && bCol < N) {\n            Bs[threadIdx.x][threadIdx.y] = B[bRow * N + bCol];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -93.10267562866211
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        if (row < M && (t * TILE_SIZE + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && (t * TILE_SIZE + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -40.58547172546387
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the CUDA matmul kernel using shared memory tiling to reduce global memory accesses and improve memory coalescing, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.x][threadIdx.y] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.x][i] * Bs[i][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -93.05554885864258
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling, increasing block size for better occupancy, and unrolling the inner loop for improved instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define BLOCK_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -42.655907440185544
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize matrix multiplication using register tiling with warp-level operations and cooperative fetching to maximize memory throughput and minimize shared memory usage.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) * 2;\n    const int warp_col = (blockIdx.x * blockDim.x + threadIdx.x) * 2;\n    \n    float reg_A[2], reg_B[2];\n    float reg_C[2][2] = {{0.0f, 0.0f}, {0.0f, 0.0f}};\n\n    for (int t = 0; t < K; ++t) {\n        if (warp_row < M && t < K) {\n            reg_A[0] = A[warp_row * K + t];\n            reg_A[1] = A[(warp_row + 1) * K + t];\n        } else {\n            reg_A[0] = 0.0f;\n            reg_A[1] = 0.0f;\n        }\n\n        if (warp_col < N && t < K) {\n            reg_B[0] = B[t * N + warp_col];\n            reg_B[1] = B[t * N + (warp_col + 1)];\n        } else {\n            reg_B[0] = 0.0f;\n            reg_B[1] = 0.0f;\n        }\n\n        reg_C[0][0] += reg_A[0] * reg_B[0];\n        reg_C[0][1] += reg_A[0] * reg_B[1];\n        reg_C[1][0] += reg_A[1] * reg_B[0];\n        reg_C[1][1] += reg_A[1] * reg_B[1];\n    }\n\n    if (warp_row < M && warp_col < N) {\n        C[warp_row * N + warp_col] = reg_C[0][0];\n        if (warp_col + 1 < N) C[warp_row * N + warp_col + 1] = reg_C[0][1];\n        if (warp_row + 1 < M) C[(warp_row + 1) * N + warp_col] = reg_C[1][0];\n        if (warp_row + 1 < M && warp_col + 1 < N) C[(warp_row + 1) * N + warp_col + 1] = reg_C[1][1];\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE / 2, WARP_SIZE / 2);\n    dim3 grid((N + (2 * block.x) - 1) / (2 * block.x), (M + (2 * block.y) - 1) / (2 * block.y));\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiplication with register tiling and cooperative loading to maximize memory throughput and minimize shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n\n    int row = blockIdx.y * TILE_SIZE + warp_id * 2;\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n\n    float reg_a[2][4];\n    float reg_b[4];\n    float reg_c[2][2] = {{0.0f}};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        #pragma unroll\n        for (int i = 0; i < 2; ++i) {\n            int load_row = row + i;\n            #pragma unroll\n            for (int j = 0; j < 4; ++j) {\n                int load_col = tiled_k + lane_id + j * WARP_SIZE;\n                if (load_row < M && load_col < K) {\n                    reg_a[i][j] = A[load_row * K + load_col];\n                } else {\n                    reg_a[i][j] = 0.0f;\n                }\n            }\n        }\n\n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            int load_row = tiled_k + lane_id + j * WARP_SIZE;\n            if (load_row < K && col < N) {\n                reg_b[j] = B[load_row * N + col];\n            } else {\n                reg_b[j] = 0.0f;\n            }\n        }\n\n        #pragma unroll\n        for (int i = 0; i < 2; ++i) {\n            #pragma unroll\n            for (int j = 0; j < 2; ++j) {\n                #pragma unroll\n                for (int k = 0; k < 4; ++k) {\n                    reg_c[i][j] += reg_a[i][k] * reg_b[k];\n                }\n            }\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < 2; ++i) {\n        int write_row = row + i;\n        if (write_row < M && col < N) {\n            C[write_row * N + col] = reg_c[i][0];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(128);  // 4 warps per block\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Implement a warp-level matrix multiplication approach using CUDA's warp-level operations and register tiling to reduce shared memory usage and improve thread utilization, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int row = blockIdx.y * TILE_SIZE + warp_id * 8;\n    int col = blockIdx.x * TILE_SIZE + (lane_id % 8) * 4;\n\n    float accum[4][4] = {0.0f};\n\n    for (int t = 0; t < K; t += TILE_SIZE) {\n        float a_tile[4];\n        float b_tile[4];\n\n        for (int i = 0; i < 4; ++i) {\n            int a_row = row + i;\n            int a_col = t + (lane_id / 8);\n            if (a_row < M && a_col < K) {\n                a_tile[i] = A[a_row * K + a_col];\n            } else {\n                a_tile[i] = 0.0f;\n            }\n\n            int b_row = t + (lane_id / 8);\n            int b_col = col + i;\n            if (b_row < K && b_col < N) {\n                b_tile[i] = B[b_row * N + b_col];\n            } else {\n                b_tile[i] = 0.0f;\n            }\n        }\n\n        for (int i = 0; i < 4; ++i) {\n            for (int j = 0; j < 4; ++j) {\n                accum[i][j] += a_tile[i] * b_tile[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < 4; ++i) {\n        for (int j = 0; j < 4; ++j) {\n            int out_row = row + i;\n            int out_col = col + j;\n            if (out_row < M && out_col < N) {\n                C[out_row * N + out_col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(256);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimized matrix multiplication using register tiling with warp-level operations and vectorized memory access to maximize memory throughput and computational efficiency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define THREADS_PER_BLOCK 256\n#define WARPS_PER_BLOCK (THREADS_PER_BLOCK / 32)\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / 32;\n    const int lane_id = threadIdx.x % 32;\n    const int row = blockIdx.y * TILE_SIZE + warp_id * 8;\n    const int col = blockIdx.x * TILE_SIZE + lane_id * 4;\n\n    float4 c[8][4] = {{{0.0f}}};\n\n    for (int t = 0; t < K; t += TILE_SIZE) {\n        __shared__ float As[TILE_SIZE][TILE_SIZE];\n        __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n        for (int i = 0; i < 8; ++i) {\n            int a_row = row + i;\n            int a_col = t + lane_id;\n            if (a_row < M && a_col < K) {\n                As[warp_id * 8 + i][lane_id] = A[a_row * K + a_col];\n            } else {\n                As[warp_id * 8 + i][lane_id] = 0.0f;\n            }\n        }\n\n        for (int j = 0; j < 4; ++j) {\n            int b_row = t + warp_id * 8 + lane_id;\n            int b_col = col + j * 4;\n            if (b_row < K && b_col < N) {\n                Bs[lane_id][warp_id * 8 + j] = B[b_row * N + b_col];\n            } else {\n                Bs[lane_id][warp_id * 8 + j] = 0.0f;\n            }\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            float a = As[warp_id * 8 + lane_id][k];\n            float4 b = reinterpret_cast<float4*>(Bs[k])[warp_id];\n            \n            for (int i = 0; i < 8; ++i) {\n                for (int j = 0; j < 4; ++j) {\n                    c[i][j].x += a * b.x;\n                    c[i][j].y += a * b.y;\n                    c[i][j].z += a * b.z;\n                    c[i][j].w += a * b.w;\n                }\n            }\n        }\n\n        __syncthreads();\n    }\n\n    for (int i = 0; i < 8; ++i) {\n        for (int j = 0; j < 4; ++j) {\n            int out_row = row + i;\n            int out_col = col + j * 4;\n            if (out_row < M && out_col < N) {\n                float4 val = c[i][j];\n                C[out_row * N + out_col] = val.x;\n                if (out_col + 1 < N) C[out_row * N + out_col + 1] = val.y;\n                if (out_col + 2 < N) C[out_row * N + out_col + 2] = val.z;\n                if (out_col + 3 < N) C[out_row * N + out_col + 3] = val.w;\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(THREADS_PER_BLOCK);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize matrix multiplication by using a warp-level approach with 32x8 tile sizes to better utilize warp-level parallelism and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_M 32\n#define TILE_N 8\n#define TILE_K 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x % (WARP_SIZE / 4);\n    const int warp_id = threadIdx.x / (WARP_SIZE / 4);\n\n    __shared__ float As[TILE_M][TILE_K];\n    __shared__ float Bs[TILE_K][TILE_N];\n\n    int row = blockIdx.y * TILE_M + warp_row * 4;\n    int col = blockIdx.x * TILE_N + warp_col;\n\n    float sum[4] = {0.0f};\n\n    for (int t = 0; t < (K + TILE_K - 1) / TILE_K; ++t) {\n        int tiled_k = t * TILE_K;\n\n        #pragma unroll\n        for (int load = 0; load < 4; ++load) {\n            int a_row = row + load;\n            int a_col = tiled_k + threadIdx.x % TILE_K;\n            if (a_row < M && a_col < K) {\n                As[warp_row * 4 + load][threadIdx.x % TILE_K] = A[a_row * K + a_col];\n            } else {\n                As[warp_row * 4 + load][threadIdx.x % TILE_K] = 0.0f;\n            }\n        }\n\n        int b_row = tiled_k + threadIdx.x / TILE_N;\n        int b_col = col + threadIdx.x % TILE_N;\n        if (b_row < K && b_col < N) {\n            Bs[threadIdx.x / TILE_N][threadIdx.x % TILE_N] = B[b_row * N + b_col];\n        } else {\n            Bs[threadIdx.x / TILE_N][threadIdx.x % TILE_N] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_K; ++k) {\n            float a_val = As[warp_row * 4 + warp_id][k];\n            #pragma unroll\n            for (int i = 0; i < 4; ++i) {\n                sum[i] += a_val * Bs[k][warp_col + i * (WARP_SIZE / 4)];\n            }\n        }\n\n        __syncthreads();\n    }\n\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int c_row = row + i;\n        if (c_row < M && col < N) {\n            C[c_row * N + col] = sum[i];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE, 8);\n    dim3 grid((N + TILE_N - 1) / TILE_N, (M + TILE_M - 1) / TILE_M);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing a warp-level tiled matrix multiplication using shared memory and register accumulation to reduce shared memory bank conflicts and improve memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n\n    int warp_id = threadIdx.y / (WARP_SIZE / TILE_SIZE);\n    int lane_id = threadIdx.x + (threadIdx.y % (WARP_SIZE / TILE_SIZE)) * TILE_SIZE;\n\n    int row = blockIdx.y * (WARPS_PER_BLOCK * TILE_SIZE) + warp_id * TILE_SIZE + threadIdx.x;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.y;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + lane_id) < K) {\n            As[warp_id][threadIdx.x][lane_id] = A[row * K + tiled_k + lane_id];\n        } else {\n            As[warp_id][threadIdx.x][lane_id] = 0.0f;\n        }\n\n        if ((tiled_k + lane_id) < K && col < N) {\n            Bs[warp_id][lane_id][threadIdx.y] = B[(tiled_k + lane_id) * N + col];\n        } else {\n            Bs[warp_id][lane_id][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[warp_id][threadIdx.x][k] * Bs[warp_id][k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, WARPS_PER_BLOCK * (WARP_SIZE / TILE_SIZE));\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + (WARPS_PER_BLOCK * TILE_SIZE) - 1) / (WARPS_PER_BLOCK * TILE_SIZE));\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the CUDA matmul kernel by using a warp-level tiling approach with register accumulation and double buffering to hide memory latency while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / THREADS_PER_WARP;\n    const int lane_id = threadIdx.x % THREADS_PER_WARP;\n    \n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    float accum[TILE_SIZE/WARPS_PER_BLOCK] = {0.0f};\n    int phase = 0;\n    \n    int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE/WARPS_PER_BLOCK);\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Double buffered loading\n        if (row + threadIdx.y < M && tiled_k + lane_id < K) {\n            As[phase][threadIdx.y][lane_id] = A[(row + threadIdx.y) * K + tiled_k + lane_id];\n        } else {\n            As[phase][threadIdx.y][lane_id] = 0.0f;\n        }\n        \n        if (tiled_k + threadIdx.y < K && col < N) {\n            Bs[phase][threadIdx.y][lane_id] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[phase][threadIdx.y][lane_id] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute while next tile is loading\n        phase ^= 1;\n        if (t > 0) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n                    accum[i] += As[phase^1][warp_id*(TILE_SIZE/WARPS_PER_BLOCK)+i][k] * Bs[phase^1][k][lane_id];\n                }\n            }\n        }\n        __syncthreads();\n    }\n    \n    // Final accumulation\n    for (int k = 0; k < TILE_SIZE; ++k) {\n        for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n            accum[i] += As[phase][warp_id*(TILE_SIZE/WARPS_PER_BLOCK)+i][k] * Bs[phase][k][lane_id];\n        }\n    }\n    \n    // Write results\n    for (int i = 0; i < TILE_SIZE/WARPS_PER_BLOCK; ++i) {\n        if (row + i < M && col < N) {\n            C[(row + i) * N + col] = accum[i];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(THREADS_PER_WARP * WARPS_PER_BLOCK);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the CUDA matmul kernel by transposing matrix B for better memory access patterns and using shared memory tiling with a larger tile size (32x32) to better utilize the RTX 4090's resources.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; t++) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[col * K + tiled_k];  // B is transposed\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; k++) {\n            value += As[threadIdx.y][k] * Bs[threadIdx.x][k];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n    auto B_t = B.t().contiguous();  // Transpose B for better memory access\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B_t.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -133.16301574707032
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing tiled matrix multiplication with shared memory and loop unrolling to further reduce memory latency and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1];  // +1 for bank conflict avoidance\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -61.89261474609375
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing tiled matrix multiplication with shared memory and loop unrolling to reduce memory accesses and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing tiled matrix multiplication with shared memory and loop unrolling to reduce instruction overhead and improve parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1]; // +1 to avoid bank conflicts\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory with double buffering to overlap memory transfers with computation, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    int phase = 0;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        // Load next tile while computing current tile\n        int load_phase = 1 - phase;\n        \n        if (row < M && (t * TILE_SIZE + threadIdx.x) < K) {\n            As[load_phase][threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n        }\n        \n        if (col < N && (t * TILE_SIZE + threadIdx.y) < K) {\n            Bs[load_phase][threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n        }\n        \n        __syncthreads();\n        \n        // Compute current tile\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[phase][threadIdx.y][i] * Bs[phase][i][threadIdx.x];\n        }\n        \n        phase = load_phase;\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory with larger tile size (32x32) and loop unrolling to improve memory bandwidth utilization and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -43.5389217376709
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing tiled matrix multiplication with increased tile size (32x32) and loop unrolling for better memory access patterns and reduced synchronization overhead.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -42.31730575561524
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling with a larger tile size (32x32) and loop unrolling to improve memory bandwidth utilization and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        if (row < M && (t * TILE_SIZE + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && (t * TILE_SIZE + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -43.810726165771484
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory with larger tile size (32x32) and loop unrolling to improve memory bandwidth utilization and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        if (row < M && (t * TILE_SIZE + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && (t * TILE_SIZE + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -44.303270721435545
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiplication with register tiling and shared memory to maximize memory efficiency and reduce bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    const int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    const int col = blockIdx.x * TILE_SIZE + lane_id;\n\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n\n    float reg_A[TILE_SIZE / WARPS_PER_BLOCK] = {0.0f};\n    float reg_B[TILE_SIZE / WARPS_PER_BLOCK] = {0.0f};\n    float reg_C[TILE_SIZE / WARPS_PER_BLOCK][TILE_SIZE / WARPS_PER_BLOCK] = {{0.0f}};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + lane_id;\n        int b_row = tiled_k + lane_id;\n\n        if (row < M && a_col < K) {\n            reg_A[0] = A[row * K + a_col];\n        }\n        if (b_row < K && col < N) {\n            reg_B[0] = B[b_row * N + col];\n        }\n\n        As[warp_id][lane_id][0] = reg_A[0];\n        Bs[warp_id][0][lane_id] = reg_B[0];\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            reg_C[0][0] += As[warp_id][lane_id][k] * Bs[warp_id][k][lane_id];\n        }\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = reg_C[0][0];\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARPS_PER_BLOCK * WARP_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiplication with register tiling and shared memory to maximize memory coalescing and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int row = blockIdx.y * TILE_SIZE * WARPS_PER_BLOCK + warp_id * TILE_SIZE;\n    int col = blockIdx.x * TILE_SIZE;\n\n    float reg_A[TILE_SIZE] = {0.0f};\n    float reg_B[TILE_SIZE] = {0.0f};\n    float reg_C[TILE_SIZE][TILE_SIZE] = {{0.0f}};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Cooperative loading of A and B tiles into shared memory\n        for (int i = 0; i < TILE_SIZE; i += WARP_SIZE) {\n            int load_row = row + threadIdx.y + i;\n            int load_col = tiled_k + lane_id;\n            if (load_row < M && load_col < K) {\n                As[warp_id][threadIdx.y + i][lane_id] = A[load_row * K + load_col];\n            } else {\n                As[warp_id][threadIdx.y + i][lane_id] = 0.0f;\n            }\n\n            load_row = tiled_k + threadIdx.y + i;\n            load_col = col + lane_id;\n            if (load_row < K && load_col < N) {\n                Bs[warp_id][threadIdx.y + i][lane_id] = B[load_row * N + load_col];\n            } else {\n                Bs[warp_id][threadIdx.y + i][lane_id] = 0.0f;\n            }\n        }\n\n        __syncthreads();\n\n        // Register tiling for warp-level matrix multiplication\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                reg_A[i] = As[warp_id][i][k];\n                reg_B[i] = Bs[warp_id][k][i];\n            }\n\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                for (int j = 0; j < TILE_SIZE; ++j) {\n                    reg_C[i][j] += reg_A[i] * reg_B[j];\n                }\n            }\n        }\n\n        __syncthreads();\n    }\n\n    // Store results to global memory\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        for (int j = 0; j < TILE_SIZE; j += WARP_SIZE) {\n            int store_row = row + i;\n            int store_col = col + lane_id + j;\n            if (store_row < M && store_col < N) {\n                C[store_row * N + store_col] = reg_C[i][lane_id + j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE * WARPS_PER_BLOCK, 1);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK));\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiplication with register tiling to maximize instruction-level parallelism and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define SUB_TILE_SIZE 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) / WARP_SIZE;\n    int warp_col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (warp_row >= M || warp_col >= N) return;\n\n    float accum[SUB_TILE_SIZE][SUB_TILE_SIZE] = {0.0f};\n    int lane_id = threadIdx.y % WARP_SIZE;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        float a_frag[SUB_TILE_SIZE];\n        float b_frag[SUB_TILE_SIZE];\n        \n        int tiled_k = t * TILE_SIZE;\n        for (int i = 0; i < SUB_TILE_SIZE; ++i) {\n            int a_row = warp_row * SUB_TILE_SIZE + i;\n            int a_col = tiled_k + lane_id;\n            a_frag[i] = (a_row < M && a_col < K) ? A[a_row * K + a_col] : 0.0f;\n            \n            int b_row = tiled_k + lane_id;\n            int b_col = warp_col * SUB_TILE_SIZE + i;\n            b_frag[i] = (b_row < K && b_col < N) ? B[b_row * N + b_col] : 0.0f;\n        }\n\n        for (int i = 0; i < SUB_TILE_SIZE; ++i) {\n            for (int j = 0; j < SUB_TILE_SIZE; ++j) {\n                accum[i][j] += a_frag[i] * b_frag[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < SUB_TILE_SIZE; ++i) {\n        int c_row = warp_row * SUB_TILE_SIZE + i;\n        if (c_row >= M) continue;\n        for (int j = 0; j < SUB_TILE_SIZE; ++j) {\n            int c_col = warp_col * SUB_TILE_SIZE + j;\n            if (c_col < N) {\n                C[c_row * N + c_col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(32, 32);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiplication with register tiling and shared memory to maximize memory efficiency and computational throughput.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int row = (blockIdx.y * WARPS_PER_BLOCK + warp_id) * TILE_SIZE + (lane_id / 4);\n    int col = blockIdx.x * TILE_SIZE + (lane_id % 4) * 8;\n\n    float reg_A[8] = {0.0f};\n    float reg_B[8] = {0.0f};\n    float reg_C[8][8] = {{0.0f}};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + lane_id % TILE_SIZE;\n        int b_row = tiled_k + lane_id / TILE_SIZE;\n\n        if (row < M && a_col < K) {\n            As[warp_id][lane_id / TILE_SIZE][lane_id % TILE_SIZE] = A[row * K + a_col];\n        } else {\n            As[warp_id][lane_id / TILE_SIZE][lane_id % TILE_SIZE] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[warp_id][lane_id / TILE_SIZE][lane_id % TILE_SIZE] = B[b_row * N + col];\n        } else {\n            Bs[warp_id][lane_id / TILE_SIZE][lane_id % TILE_SIZE] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < 8; ++i) {\n            reg_A[i] = As[warp_id][lane_id / 4][i * 4 + (lane_id % 4)];\n            reg_B[i] = Bs[warp_id][i][(lane_id % 4) * 8 + (lane_id / 4)];\n        }\n\n        for (int i = 0; i < 8; ++i) {\n            for (int j = 0; j < 8; ++j) {\n                reg_C[i][j] += reg_A[i] * reg_B[j];\n            }\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        for (int i = 0; i < 8; ++i) {\n            for (int j = 0; j < 8; ++j) {\n                if ((row + i) < M && (col + j) < N) {\n                    C[(row + i) * N + (col + j)] = reg_C[i][j];\n                }\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE * WARPS_PER_BLOCK);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK));\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling with a different thread mapping strategy and loop unrolling to improve parallelism and reduce synchronization overhead.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    int bx = blockIdx.x * TILE_SIZE;\n    int by = blockIdx.y * TILE_SIZE;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < K; t += TILE_SIZE) {\n        // Load tile from A\n        if (by + ty < M && t + tx < K) {\n            As[ty][tx] = A[(by + ty) * K + t + tx];\n        } else {\n            As[ty][tx] = 0.0f;\n        }\n\n        // Load tile from B\n        if (bx + tx < N && t + ty < K) {\n            Bs[ty][tx] = B[(t + ty) * N + bx + tx];\n        } else {\n            Bs[ty][tx] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute partial product with loop unrolling\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[ty][i] * Bs[i][tx];\n            value += As[ty][i+1] * Bs[i+1][tx];\n            value += As[ty][i+2] * Bs[i+2][tx];\n            value += As[ty][i+3] * Bs[i+3][tx];\n        }\n\n        __syncthreads();\n    }\n\n    // Write result to C\n    if (by + ty < M && bx + tx < N) {\n        C[(by + ty) * N + bx + tx] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -42.98975715637207
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling with 32x32 blocks and unrolling the inner loop for better performance on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE + threadIdx.x;\n        if (row < M && tiled_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        tiled_k = t * TILE_SIZE + threadIdx.y;\n        if (col < N && tiled_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -42.8486946105957
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing a warp-level tiled approach with 32x8 tile sizes and warp-level matrix multiplication to reduce shared memory usage and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_M 32\n#define TILE_N 8\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = threadIdx.y;\n    const int warp_col = threadIdx.x % 8;\n    const int warp_id = threadIdx.x / 8;\n\n    const int row = blockIdx.y * TILE_M + warp_row * 8;\n    const int col = blockIdx.x * TILE_N + warp_id * TILE_N + warp_col;\n\n    float accum[TILE_N] = {0.0f};\n\n    for (int t = 0; t < K; t += TILE_K) {\n        __shared__ float As[TILE_M][TILE_K];\n        __shared__ float Bs[TILE_K][TILE_N];\n\n        if (row + warp_row < M && t + warp_col < K) {\n            As[warp_row * 8 + warp_col][warp_col] = A[(row + warp_row) * K + (t + warp_col)];\n        }\n\n        if (t + warp_row < K && col + warp_col < N) {\n            Bs[warp_row][warp_id * TILE_N + warp_col] = B[(t + warp_row) * N + (col + warp_col)];\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_K; ++k) {\n            float a = As[warp_row * 8 + warp_col][k];\n            for (int n = 0; n < TILE_N; ++n) {\n                accum[n] += a * Bs[k][warp_id * TILE_N + n];\n            }\n        }\n\n        __syncthreads();\n    }\n\n    if (row + warp_row < M) {\n        for (int n = 0; n < TILE_N; ++n) {\n            if (col + n < N) {\n                C[(row + warp_row) * N + (col + n)] = accum[n];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(32, 4);\n    dim3 grid((N + TILE_N - 1) / TILE_N, (M + TILE_M - 1) / TILE_M);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing a warp-level tiled approach using 32x32 tiles with warp-level matrix multiply-accumulate operations to better utilize the Tensor Cores on RTX 4090 while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int warp_id = threadIdx.y / 8;\n    int lane_id = threadIdx.x + (threadIdx.y % 8) * 32;\n    int row = blockIdx.y * TILE_SIZE + warp_id * 8;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x * 4;\n\n    float4 c_val = {0.0f, 0.0f, 0.0f, 0.0f};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load A tile\n        int a_col = tiled_k + threadIdx.x;\n        if (row + threadIdx.y < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[(row + threadIdx.y) * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        // Load B tile\n        int b_row = tiled_k + threadIdx.y;\n        if (b_row < K && col + threadIdx.x * 4 < N) {\n            float4 b_val = *reinterpret_cast<const float4*>(&B[b_row * N + col]);\n            Bs[threadIdx.y][threadIdx.x * 4 + 0] = b_val.x;\n            Bs[threadIdx.y][threadIdx.x * 4 + 1] = b_val.y;\n            Bs[threadIdx.y][threadIdx.x * 4 + 2] = b_val.z;\n            Bs[threadIdx.y][threadIdx.x * 4 + 3] = b_val.w;\n        } else {\n            Bs[threadIdx.y][threadIdx.x * 4 + 0] = 0.0f;\n            Bs[threadIdx.y][threadIdx.x * 4 + 1] = 0.0f;\n            Bs[threadIdx.y][threadIdx.x * 4 + 2] = 0.0f;\n            Bs[threadIdx.y][threadIdx.x * 4 + 3] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Warp-level matrix multiply-accumulate\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            float a_val = As[threadIdx.y][k];\n            c_val.x += a_val * Bs[k][threadIdx.x * 4 + 0];\n            c_val.y += a_val * Bs[k][threadIdx.x * 4 + 1];\n            c_val.z += a_val * Bs[k][threadIdx.x * 4 + 2];\n            c_val.w += a_val * Bs[k][threadIdx.x * 4 + 3];\n        }\n\n        __syncthreads();\n    }\n\n    // Store results\n    if (row + threadIdx.y < M && col + threadIdx.x * 4 + 3 < N) {\n        *reinterpret_cast<float4*>(&C[(row + threadIdx.y) * N + col]) = c_val;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(8, 32);  // 8x32 = 256 threads (4 warps)\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory with double buffering to overlap memory transfers with computation and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    int loadA = 0, loadB = 0;\n    int computeA = 0, computeB = 0;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        if (row < M && (t * TILE_SIZE + threadIdx.x) < K) {\n            As[loadA][threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n        }\n        if (col < N && (t * TILE_SIZE + threadIdx.y) < K) {\n            Bs[loadB][threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n        }\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[computeA][threadIdx.y][i] * Bs[computeB][i][threadIdx.x];\n        }\n        __syncthreads();\n\n        loadA ^= 1;\n        loadB ^= 1;\n        computeA ^= 1;\n        computeB ^= 1;\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling and loop unrolling to further reduce memory accesses and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1];  // +1 for bank conflict avoidance\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load A tile\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        // Load B tile\n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Unrolled computation\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n            value += As[threadIdx.y][i+1] * Bs[i+1][threadIdx.x];\n            value += As[threadIdx.y][i+2] * Bs[i+2][threadIdx.x];\n            value += As[threadIdx.y][i+3] * Bs[i+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -58.813664245605466
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing tiled matrix multiplication with shared memory and loop unrolling to reduce global memory accesses and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1];  // +1 for bank conflict avoidance\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the CUDA matmul kernel using register tiling and loop unrolling to reduce shared memory bank conflicts and improve instruction-level parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value[UNROLL_FACTOR] = {0.0f};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / (TILE_SIZE * UNROLL_FACTOR); t++) {\n        #pragma unroll\n        for (int u = 0; u < UNROLL_FACTOR; u++) {\n            int tiled_k = (t * UNROLL_FACTOR + u) * TILE_SIZE + threadIdx.x;\n            if (row < M && tiled_k < K) {\n                As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            tiled_k = (t * UNROLL_FACTOR + u) * TILE_SIZE + threadIdx.y;\n            if (col < N && tiled_k < K) {\n                Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            __syncthreads();\n\n            #pragma unroll\n            for (int k = 0; k < TILE_SIZE; k++) {\n                value[u] += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n\n            __syncthreads();\n        }\n    }\n\n    float final_value = 0.0f;\n    #pragma unroll\n    for (int u = 0; u < UNROLL_FACTOR; u++) {\n        final_value += value[u];\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = final_value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing tiled matrix multiplication with shared memory using a larger tile size (32x32) to better utilize the RTX 4090's resources and increase thread block occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -43.44253768920898
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling with a larger tile size (32x32) to better utilize the RTX 4090's resources and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        if (row < M && (t * TILE_SIZE + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + t * TILE_SIZE + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && (t * TILE_SIZE + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            value += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": -43.88181381225586
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing tiled matrix multiplication with larger shared memory tiles (32x32) and loop unrolling to improve memory bandwidth utilization and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing tiled matrix multiplication with shared memory using a larger tile size (32x32) and adjusting the block and grid dimensions accordingly for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiplication with register tiling and cooperative fetching to maximize memory throughput and computational efficiency.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define THREADS_PER_BLOCK 256\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    const int warp_row = warp_id / (blockDim.x / WARP_SIZE);\n    const int warp_col = warp_id % (blockDim.x / WARP_SIZE);\n\n    const int row = blockIdx.y * TILE_SIZE + warp_row * 8;\n    const int col = blockIdx.x * TILE_SIZE + warp_col * 8;\n\n    float reg_A[8];\n    float reg_B[8];\n    float reg_C[8][8] = {0.0f};\n\n    for (int t = 0; t < K; t += TILE_SIZE) {\n        #pragma unroll\n        for (int i = 0; i < 8; ++i) {\n            int a_row = row + i;\n            int a_col = t + lane_id;\n            if (a_row < M && a_col < K) {\n                reg_A[i] = A[a_row * K + a_col];\n            } else {\n                reg_A[i] = 0.0f;\n            }\n\n            int b_row = t + lane_id;\n            int b_col = col + i;\n            if (b_row < K && b_col < N) {\n                reg_B[i] = B[b_row * N + b_col];\n            } else {\n                reg_B[i] = 0.0f;\n            }\n        }\n\n        #pragma unroll\n        for (int k = 0; k < 8; ++k) {\n            #pragma unroll\n            for (int i = 0; i < 8; ++i) {\n                #pragma unroll\n                for (int j = 0; j < 8; ++j) {\n                    reg_C[i][j] += reg_A[i] * reg_B[j];\n                }\n            }\n        }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < 8; ++i) {\n        #pragma unroll\n        for (int j = 0; j < 8; ++j) {\n            int c_row = row + i;\n            int c_col = col + j;\n            if (c_row < M && c_col < N) {\n                C[c_row * N + c_col] = reg_C[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(THREADS_PER_BLOCK);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the CUDA matmul kernel by using register tiling and warp-level matrix multiplication to maximize memory access efficiency and arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    \n    const int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    const int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float reg_A[TILE_SIZE / WARPS_PER_BLOCK] = {0.0f};\n    float reg_B = 0.0f;\n    float reg_C[TILE_SIZE / WARPS_PER_BLOCK][TILE_SIZE] = {{0.0f}};\n\n    for (int t = 0; t < K; t += TILE_SIZE) {\n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            if (row + i < M && t + lane_id < K) {\n                reg_A[i] = A[(row + i) * K + t + lane_id];\n            }\n        }\n        \n        for (int j = 0; j < TILE_SIZE; ++j) {\n            if (t + j < K && col < N) {\n                reg_B = B[(t + j) * N + col];\n            }\n            \n            for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n                reg_C[i][j] += reg_A[i] * reg_B;\n            }\n        }\n    }\n\n    for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n        if (row + i < M && col < N) {\n            C[(row + i) * N + col] = reg_C[i][lane_id];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE * WARPS_PER_BLOCK);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiplication with register tiling and cooperative fetching to maximize memory throughput and computational efficiency.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define THREADS_PER_BLOCK 256\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    const int warp_row = warp_id / (TILE_SIZE / WARP_SIZE);\n    const int warp_col = warp_id % (TILE_SIZE / WARP_SIZE);\n\n    const int row = blockIdx.y * TILE_SIZE + warp_row * WARP_SIZE + lane_id;\n    const int col = blockIdx.x * TILE_SIZE + warp_col * WARP_SIZE + lane_id;\n\n    float reg_a[4], reg_b[4];\n    float accum[TILE_SIZE / WARP_SIZE][TILE_SIZE / WARP_SIZE] = {0.0f};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        for (int i = 0; i < 4; ++i) {\n            int a_col = t * TILE_SIZE + lane_id + i * WARP_SIZE;\n            if (row < M && a_col < K) reg_a[i] = A[row * K + a_col];\n            else reg_a[i] = 0.0f;\n\n            int b_row = t * TILE_SIZE + lane_id + i * WARP_SIZE;\n            if (col < N && b_row < K) reg_b[i] = B[b_row * N + col];\n            else reg_b[i] = 0.0f;\n        }\n\n        for (int i = 0; i < 4; ++i) {\n            for (int j = 0; j < 4; ++j) {\n                accum[i][j] += reg_a[i] * reg_b[j];\n            }\n        }\n    }\n\n    for (int i = 0; i < TILE_SIZE / WARP_SIZE; ++i) {\n        for (int j = 0; j < TILE_SIZE / WARP_SIZE; ++j) {\n            int out_row = row + i * WARP_SIZE;\n            int out_col = col + j * WARP_SIZE;\n            if (out_row < M && out_col < N) {\n                C[out_row * N + out_col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(THREADS_PER_BLOCK);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimized matrix multiplication using warp-level tiling and register accumulation to maximize memory throughput and minimize shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    const int warp_row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    const int warp_col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float accum[TILE_SIZE/WARP_SIZE] = {0.0f};\n\n    for (int tile = 0; tile < K; tile += TILE_SIZE) {\n        float a_frag[TILE_SIZE/WARP_SIZE];\n        float b_frag[TILE_SIZE/WARP_SIZE];\n\n        for (int i = 0; i < TILE_SIZE/WARP_SIZE; ++i) {\n            if (warp_row < M && (tile + threadIdx.x + i * WARP_SIZE) < K) {\n                a_frag[i] = A[warp_row * K + tile + threadIdx.x + i * WARP_SIZE];\n            } else {\n                a_frag[i] = 0.0f;\n            }\n\n            if (warp_col < N && (tile + threadIdx.x + i * WARP_SIZE) < K) {\n                b_frag[i] = B[(tile + threadIdx.x + i * WARP_SIZE) * N + warp_col];\n            } else {\n                b_frag[i] = 0.0f;\n            }\n        }\n\n        for (int i = 0; i < TILE_SIZE/WARP_SIZE; ++i) {\n            for (int j = 0; j < TILE_SIZE/WARP_SIZE; ++j) {\n                accum[i] += a_frag[j] * b_frag[j];\n            }\n        }\n    }\n\n    if (warp_row < M && warp_col < N) {\n        C[warp_row * N + warp_col] = 0.0f;\n        for (int i = 0; i < TILE_SIZE/WARP_SIZE; ++i) {\n            C[warp_row * N + warp_col] += accum[i];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(WARP_SIZE, TILE_SIZE/WARP_SIZE);\n    dim3 grid((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the CUDA matmul kernel by implementing a warp-level tiling approach with 32x8 tile sizes and warp-level matrix multiply-accumulate operations to reduce shared memory bank conflicts and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_M 32\n#define TILE_N 8\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_M][TILE_K];\n    __shared__ float Bs[TILE_K][TILE_N];\n\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int row = blockIdx.y * TILE_M + warp_id * TILE_M / (blockDim.x / WARP_SIZE);\n    int col = blockIdx.x * TILE_N;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_K - 1) / TILE_K; ++t) {\n        int tiled_k = t * TILE_K;\n        \n        // Cooperative loading of tiles\n        if (row + threadIdx.y < M && tiled_k + threadIdx.x < K) {\n            As[threadIdx.y][threadIdx.x] = A[(row + threadIdx.y) * K + (tiled_k + threadIdx.x)];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tiled_k + threadIdx.y < K && col + threadIdx.x < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + (col + threadIdx.x)];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Warp-level matrix multiply-accumulate\n        for (int k = 0; k < TILE_K; ++k) {\n            float a = As[lane_id / TILE_N][k];\n            float b = Bs[k][lane_id % TILE_N];\n            sum += a * b;\n        }\n\n        __syncthreads();\n    }\n\n    if (row + lane_id / TILE_N < M && col + lane_id % TILE_N < N) {\n        C[(row + lane_id / TILE_N) * N + (col + lane_id % TILE_N)] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    int M = A.size(0);\n    int K = A.size(1);\n    int N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 block(256);  // 8 warps per block\n    dim3 grid((N + TILE_N - 1) / TILE_N, (M + TILE_M - 1) / TILE_M);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    cudaDeviceSynchronize();\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication (CUDA)\");\n}",
        "score": null
    }
]