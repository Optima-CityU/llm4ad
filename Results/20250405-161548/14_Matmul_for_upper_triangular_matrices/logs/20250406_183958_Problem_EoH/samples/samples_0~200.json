[
    {
        "sample_order": 1,
        "algorithm": "{Optimized algorithm computes only upper triangular elements by leveraging shared memory for tile-based matrix multiplication and loop unrolling to reduce memory accesses and improve parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = row; tile < N; tile += TILE_SIZE) {\n        int k = tile + threadIdx.x;\n        if (k < N && row < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        k = tile + threadIdx.y;\n        if (k < N && col < N && col >= row) {\n            Bs[threadIdx.y][threadIdx.x] = B[k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; i += UNROLL_FACTOR) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n            sum += As[threadIdx.y][i+1] * Bs[i+1][threadIdx.x];\n            sum += As[threadIdx.y][i+2] * Bs[i+2][threadIdx.x];\n            sum += As[threadIdx.y][i+3] * Bs[i+3][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular multiplication through loop tiling and shared memory utilization, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = row; tile < N; tile += TILE_SIZE) {\n        int k = tile + threadIdx.x;\n        if (row < N && k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        k = tile + threadIdx.y;\n        if (col < N && k < N && col >= row) {\n            Bs[threadIdx.y][threadIdx.x] = B[k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through loop tiling and shared memory utilization, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n            int tiled_k = tile * TILE_SIZE + threadIdx.x;\n            if (tiled_k < N && row < N) {\n                As[threadIdx.y][threadIdx.x] = A[row * N + tiled_k];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            tiled_k = tile * TILE_SIZE + threadIdx.y;\n            if (tiled_k < N && col < N) {\n                Bs[threadIdx.y][threadIdx.x] = B[tiled_k * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            __syncthreads();\n\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                int global_k = tile * TILE_SIZE + k;\n                if (global_k >= row && global_k < N) {\n                    sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n                }\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through shared memory tiling and loop unrolling, while maintaining correctness by only computing elements where row <= col.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_offset = tile * TILE_SIZE;\n\n        // Load A tile\n        int A_col = tile_offset + threadIdx.x;\n        if (row < N && A_col < N && A_col >= row) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        // Load B tile\n        int B_row = tile_offset + threadIdx.y;\n        if (B_row < N && col < N && col >= B_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll UNROLL_FACTOR\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through tiling and shared memory utilization while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = (row/TILE_SIZE)*TILE_SIZE; tile < N; tile += TILE_SIZE) {\n        int tile_row = tile + threadIdx.y;\n        int tile_col = tile + threadIdx.x;\n\n        if (row < N && tile_col < N && tile_col >= row) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N && col >= tile_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize by reducing redundant computations through tiling and shared memory utilization while maintaining the upper triangular property, and unrolling inner loops for better instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = row; tile < N; tile += TILE_SIZE) {\n        int k = tile + threadIdx.x;\n        if (row < N && k < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        k = tile + threadIdx.y;\n        if (col < N && k < N && col >= row) {\n            Bs[threadIdx.y][threadIdx.x] = B[k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through shared memory tiling and loop unrolling, while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = row / TILE_SIZE; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n            int k = tile * TILE_SIZE + threadIdx.x;\n            if (k < N && row < N) {\n                As[threadIdx.y][threadIdx.x] = A[row * N + k];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            k = tile * TILE_SIZE + threadIdx.y;\n            if (k < N && col < N) {\n                Bs[threadIdx.y][threadIdx.x] = B[k * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            __syncthreads();\n\n            #pragma unroll UNROLL_FACTOR\n            for (int kk = 0; kk < TILE_SIZE; ++kk) {\n                if (tile * TILE_SIZE + kk >= row && tile * TILE_SIZE + kk < N) {\n                    sum += As[threadIdx.y][kk] * Bs[kk][threadIdx.x];\n                }\n            }\n\n            __syncthreads();\n        }\n\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimized algorithm: Utilize shared memory to cache tiles of A and B, reduce redundant computations by leveraging upper triangular properties, and employ loop unrolling for better instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE + 1];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE + 1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = (row / TILE_SIZE) * TILE_SIZE; tile < N; tile += TILE_SIZE) {\n        // Load tiles into shared memory\n        int load_row = row;\n        int load_col = tile + threadIdx.x;\n        if (load_row < N && load_col < N && load_col >= load_row) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * N + load_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        load_row = tile + threadIdx.y;\n        load_col = col;\n        if (load_row < N && load_col < N && load_col >= load_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        __syncthreads();\n\n        // Compute partial sum with loop unrolling\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            sum += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            sum += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            sum += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular multiplication through loop unrolling, shared memory caching, and increased thread utilization while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N && tile_col >= row) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N && col >= tile_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (row < N && col < N && col >= row) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimized algorithm: Utilize shared memory to cache tiles of A and B, and compute only the upper triangular portion with improved memory access patterns and loop unrolling.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            sum += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            sum += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            sum += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize by using shared memory to cache tiles of A and B, and only compute upper triangular elements while minimizing redundant computations.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        sA[threadIdx.y][threadIdx.x] = (row < N && tile_col < N && tile_col >= row) ? A[row * N + tile_col] : 0.0f;\n        sB[threadIdx.y][threadIdx.x] = (tile_row < N && col < N && col >= tile_row) ? B[tile_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        if (row < N && col < N && col >= row) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += sA[threadIdx.y][k] * sB[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through shared memory tiling and exploiting the upper triangular structure to minimize memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N && tile_col >= row) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N && col >= tile_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (row < N && col < N && col >= row) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimized algorithm: Utilize shared memory to cache tiles of A and B, reduce redundant computations by leveraging the upper triangular property, and employ loop unrolling for better instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N && tile_col >= row) {\n            sA[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            sA[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && tile_row < N && col >= tile_row) {\n            sB[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            sB[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll UNROLL_FACTOR\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += sA[threadIdx.y][k] * sB[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize by leveraging shared memory for block-wise computation, reducing global memory accesses, and exploiting the upper triangular structure to skip unnecessary computations.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        // Load A tile (only upper triangular part)\n        if (row < N && tile_col < N && tile_col >= row) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        // Load B tile (only upper triangular part)\n        if (tile_row < N && col < N && col >= tile_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (row < N && col < N && col >= row) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular multiplication through loop unrolling, shared memory caching, and minimizing thread divergence.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = row / TILE_SIZE; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n            int k = tile * TILE_SIZE + threadIdx.x;\n            As[threadIdx.y][threadIdx.x] = (k < N && row <= k) ? A[row * N + k] : 0.0f;\n            \n            k = tile * TILE_SIZE + threadIdx.y;\n            Bs[threadIdx.y][threadIdx.x] = (k < N && k <= col) ? B[k * N + col] : 0.0f;\n            \n            __syncthreads();\n\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize by reducing redundant computations through loop unrolling, shared memory caching of matrix tiles, and leveraging the upper triangular property to skip unnecessary operations.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = (row / TILE_SIZE) * TILE_SIZE; tile < N; tile += TILE_SIZE) {\n        int tile_row = tile + threadIdx.y;\n        int tile_col = tile + threadIdx.x;\n\n        if (row < N && tile_col < N && tile_col >= row) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N && col >= tile_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through loop unrolling, shared memory caching, and minimizing thread divergence by focusing only on upper triangular elements.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = row; tile < N; tile += TILE_SIZE * UNROLL_FACTOR) {\n            for (int t = 0; t < UNROLL_FACTOR && (tile + t * TILE_SIZE) < N; ++t) {\n                int current_tile = tile + t * TILE_SIZE;\n                int k = current_tile + threadIdx.x;\n                \n                if (k < N) {\n                    As[threadIdx.y][threadIdx.x] = A[row * N + k];\n                    Bs[threadIdx.y][threadIdx.x] = B[k * N + col];\n                }\n                __syncthreads();\n\n                for (int i = 0; i < TILE_SIZE && (current_tile + i) < N; ++i) {\n                    sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n                }\n                __syncthreads();\n            }\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B, reducing global memory accesses, and only computing upper triangular elements while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel_optimized(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n            int tile_row = tile * TILE_SIZE + threadIdx.y;\n            int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n            if (row < N && tile_col < N) {\n                As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            if (tile_row < N && col < N) {\n                Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            __syncthreads();\n\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel_optimized<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through loop unrolling, shared memory caching, and minimizing thread divergence.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = row; tile < N; tile += TILE_SIZE) {\n        int k_end = min(tile + TILE_SIZE, N);\n        \n        for (int k = tile; k < k_end; k += UNROLL_FACTOR) {\n            #pragma unroll\n            for (int i = 0; i < UNROLL_FACTOR; ++i) {\n                if (k + i < k_end && col < N) {\n                    As[threadIdx.y][threadIdx.x + i] = (row < N && (k + i) >= row) ? A[row * N + (k + i)] : 0.0f;\n                    Bs[threadIdx.y + i][threadIdx.x] = (col >= (k + i) && (k + i) < N) ? B[(k + i) * N + col] : 0.0f;\n                }\n            }\n            __syncthreads();\n\n            #pragma unroll\n            for (int i = 0; i < UNROLL_FACTOR; ++i) {\n                if (k + i < k_end && col >= row) {\n                    sum += As[threadIdx.y][threadIdx.x + i] * Bs[threadIdx.y + i][threadIdx.x];\n                }\n            }\n            __syncthreads();\n        }\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE/UNROLL_FACTOR);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular matrix multiplication through loop unrolling, shared memory caching, and adjusting thread block configuration for better memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = row; tile < N; tile += TILE_SIZE * UNROLL_FACTOR) {\n            for (int unroll = 0; unroll < UNROLL_FACTOR && (tile + unroll * TILE_SIZE) < N; ++unroll) {\n                int current_tile = tile + unroll * TILE_SIZE;\n                \n                if (threadIdx.x < TILE_SIZE && (current_tile + threadIdx.x) < N) {\n                    As[threadIdx.y][threadIdx.x] = A[row * N + current_tile + threadIdx.x];\n                }\n                if (threadIdx.y < TILE_SIZE && (current_tile + threadIdx.y) < N) {\n                    Bs[threadIdx.y][threadIdx.x] = B[(current_tile + threadIdx.y) * N + col];\n                }\n                __syncthreads();\n\n                for (int k = 0; k < TILE_SIZE && (current_tile + k) < N; ++k) {\n                    sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n                }\n                __syncthreads();\n            }\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize by reducing redundant computations through loop unrolling, shared memory caching, and leveraging the upper triangular property to minimize memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = row; tile < N; tile += TILE_SIZE * UNROLL_FACTOR) {\n        for (int t = 0; t < UNROLL_FACTOR && (tile + t * TILE_SIZE) < N; ++t) {\n            int current_tile = tile + t * TILE_SIZE;\n\n            if (row < N && (current_tile + threadIdx.x) < N) {\n                As[threadIdx.y][threadIdx.x] = A[row * N + current_tile + threadIdx.x];\n            }\n            if ((current_tile + threadIdx.y) < N && col < N && col >= (current_tile + threadIdx.y)) {\n                Bs[threadIdx.y][threadIdx.x] = B[(current_tile + threadIdx.y) * N + col];\n            }\n            __syncthreads();\n\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                if (col >= row && (current_tile + k) < N && (current_tile + k) >= row) {\n                    sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n                }\n            }\n            __syncthreads();\n        }\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through loop unrolling, shared memory caching, and restricting thread execution to only necessary upper triangular elements.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = row; tile < N; tile += TILE_SIZE) {\n            int end_tile = min(tile + TILE_SIZE, N);\n            int steps = (end_tile - tile + UNROLL_FACTOR - 1) / UNROLL_FACTOR;\n\n            for (int s = 0; s < steps; ++s) {\n                int k = tile + s * UNROLL_FACTOR;\n                if (k < end_tile) As[threadIdx.y][s * UNROLL_FACTOR] = A[row * N + k];\n                if (col < N) Bs[s * UNROLL_FACTOR][threadIdx.x] = B[k * N + col];\n            }\n            __syncthreads();\n\n            for (int k = 0; k < min(TILE_SIZE, end_tile - tile); ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B, reducing global memory accesses, and only computing upper triangular elements with improved thread utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n            int tile_row = tile * TILE_SIZE + threadIdx.y;\n            int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n            if (tile_row < N && threadIdx.x < TILE_SIZE) {\n                As[threadIdx.y][threadIdx.x] = (tile_col < N && tile_col >= row) ? A[row * N + tile_col] : 0.0f;\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            if (tile_col < N && threadIdx.y < TILE_SIZE) {\n                Bs[threadIdx.y][threadIdx.x] = (tile_row < N && col >= tile_row) ? B[tile_row * N + col] : 0.0f;\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            __syncthreads();\n\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n            __syncthreads();\n        }\n\n        if (row < N && col < N && col >= row) {\n            C[row * N + col] = sum;\n        }\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimized algorithm computes upper triangular matrix multiplication by reducing redundant computations through shared memory tiling and exploiting the upper triangular structure to limit memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    if (row >= N || col < row) return;\n    \n    float sum = 0.0f;\n    \n    for (int tile = row / TILE_SIZE; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = row;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n        \n        if (tile_row < N && tile_col < N && tile_col >= tile_row) {\n            As[threadIdx.y][threadIdx.x] = A[tile_row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        tile_row = tile * TILE_SIZE + threadIdx.y;\n        tile_col = col;\n        \n        if (tile_row < N && tile_col < N && tile_col >= tile_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + tile_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimized the kernel by reducing redundant computations in the upper triangular multiplication through loop tiling and shared memory utilization, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n            int k = tile * TILE_SIZE + threadIdx.x;\n            if (k < N && row < N) {\n                As[threadIdx.y][threadIdx.x] = A[row * N + k];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            k = tile * TILE_SIZE + threadIdx.y;\n            if (k < N && col < N) {\n                Bs[threadIdx.y][threadIdx.x] = (k <= col) ? B[k * N + col] : 0.0f;\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            __syncthreads();\n\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize by leveraging shared memory to reduce global memory accesses and adjusting the loop bounds to only compute upper triangular elements while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row >= N || col < row) return;\n\n    float sum = 0.0f;\n    int k_start = row;\n    \n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int k = tile * TILE_SIZE + threadIdx.x;\n        if (k < N && k >= k_start) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        k = tile * TILE_SIZE + threadIdx.y;\n        if (k < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through thread coarsening and shared memory utilization while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float s_data[];\n    float* sA = s_data;\n    float* sB = s_data + blockDim.y * blockDim.x;\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N && col >= row) {\n        float sum = 0.0f;\n        for (int tile = row; tile < N; tile += blockDim.x) {\n            int k = tile + threadIdx.x;\n            if (k < N) {\n                sA[threadIdx.y * blockDim.x + threadIdx.x] = A[row * N + k];\n                sB[threadIdx.y * blockDim.x + threadIdx.x] = B[k * N + col];\n            }\n            __syncthreads();\n\n            for (int i = 0; i < blockDim.x && (tile + i) < N; ++i) {\n                sum += sA[threadIdx.y * blockDim.x + i] * sB[i * blockDim.x + threadIdx.x];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    const int block_size = 16;\n    dim3 threadsPerBlock(block_size, block_size);\n    dim3 numBlocks((N + block_size - 1) / block_size, (N + block_size - 1) / block_size);\n    size_t shared_mem_size = 2 * block_size * block_size * sizeof(float);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock, shared_mem_size>>>(\n        A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the kernel by reducing redundant computations in the upper triangular matrix multiplication through loop unrolling, shared memory caching, and increased thread utilization while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = row; tile < N; tile += TILE_SIZE) {\n            int k = tile + threadIdx.x;\n            if (k < N) {\n                As[threadIdx.y][threadIdx.x] = A[row * N + k];\n            }\n            k = tile + threadIdx.y;\n            if (k < N && col < N) {\n                Bs[threadIdx.y][threadIdx.x] = B[k * N + col];\n            }\n            __syncthreads();\n\n            int end = min(tile + TILE_SIZE, N);\n            for (int kk = 0; kk < end - tile; kk += UNROLL_FACTOR) {\n                #pragma unroll\n                for (int i = 0; i < UNROLL_FACTOR && (tile + kk + i) < end; ++i) {\n                    sum += As[threadIdx.y][kk + i] * Bs[kk + i][threadIdx.x];\n                }\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimized algorithm computes only upper triangular elements by leveraging shared memory for tile-based multiplication with loop unrolling and reduced bounds checking.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = (row / TILE_SIZE) * TILE_SIZE; tile < N; tile += TILE_SIZE) {\n        int load_row = row;\n        int load_col = tile + threadIdx.x;\n        if (load_row < N && load_col < N && load_col >= load_row) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * N + load_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        load_row = tile + threadIdx.y;\n        load_col = col;\n        if (load_row < N && load_col < N && load_col >= load_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            sum += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            sum += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            sum += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular multiplication through tiling and shared memory, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = (tile_col >= row) ? A[row * N + tile_col] : 0.0f;\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N && col >= tile_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (row < N && col < N && col >= row) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimized algorithm computes only upper triangular elements by leveraging shared memory for tile-based matrix multiplication and reducing redundant computations for zero elements in upper triangular matrices.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = row / TILE_SIZE; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n            int load_row = row;\n            int load_col = tile * TILE_SIZE + threadIdx.x;\n            if (load_col < N && load_row <= load_col) {\n                As[threadIdx.y][threadIdx.x] = A[load_row * N + load_col];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            load_row = tile * TILE_SIZE + threadIdx.y;\n            load_col = col;\n            if (load_row < N && load_col < N && load_row <= load_col) {\n                Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n            __syncthreads();\n\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize by computing only upper triangular elements, using shared memory for data reuse, and unrolling inner loops to reduce memory accesses and improve parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        sA[threadIdx.y][threadIdx.x] = (row < N && tile_col < N && tile_col >= row) ? A[row * N + tile_col] : 0.0f;\n        sB[threadIdx.y][threadIdx.x] = (tile_row < N && col < N && col >= tile_row) ? B[tile_row * N + col] : 0.0f;\n\n        __syncthreads();\n\n        if (row < N && col < N && col >= row) {\n            #pragma unroll\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += sA[threadIdx.y][k] * sB[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular multiplication through tiling and shared memory, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        // Load tiles into shared memory\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N && col >= tile_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute partial sum\n        if (row < N && col < N && col >= row) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular multiplication, leveraging shared memory for data reuse, and adjusting thread block configuration for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        if (row < N && tile_col < N && tile_col >= row) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (col < N && tile_row < N && col >= tile_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (row < N && col < N && col >= row) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through thread coarsening and loop unrolling, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    const int row = blockIdx.y * blockDim.y * 4 + threadIdx.y;\n    const int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (col >= row && col < N) {\n        float sum[4] = {0.0f, 0.0f, 0.0f, 0.0f};\n        for (int k = 0; k < N; ++k) {\n            float a0 = (row < N) ? A[row * N + k] : 0.0f;\n            float a1 = (row + blockDim.y < N) ? A[(row + blockDim.y) * N + k] : 0.0f;\n            float a2 = (row + 2 * blockDim.y < N) ? A[(row + 2 * blockDim.y) * N + k] : 0.0f;\n            float a3 = (row + 3 * blockDim.y < N) ? A[(row + 3 * blockDim.y) * N + k] : 0.0f;\n            float b = B[k * N + col];\n            \n            if (row < N && k >= row) sum[0] += a0 * b;\n            if (row + blockDim.y < N && k >= row + blockDim.y) sum[1] += a1 * b;\n            if (row + 2 * blockDim.y < N && k >= row + 2 * blockDim.y) sum[2] += a2 * b;\n            if (row + 3 * blockDim.y < N && k >= row + 3 * blockDim.y) sum[3] += a3 * b;\n        }\n        \n        if (row < N) C[row * N + col] = sum[0];\n        if (row + blockDim.y < N) C[(row + blockDim.y) * N + col] = sum[1];\n        if (row + 2 * blockDim.y < N) C[(row + 2 * blockDim.y) * N + col] = sum[2];\n        if (row + 3 * blockDim.y < N) C[(row + 3 * blockDim.y) * N + col] = sum[3];\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks((N + 15) / 16, (N + 15) / (16 * 4));\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular multiplication through tiling and shared memory, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = (row / TILE_SIZE) * TILE_SIZE; tile < N; tile += TILE_SIZE) {\n        // Load tiles into shared memory\n        int load_row = row;\n        int load_col = tile + threadIdx.x;\n        if (load_row < N && load_col < N && load_col >= load_row) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * N + load_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        load_row = tile + threadIdx.y;\n        load_col = col;\n        if (load_row < N && load_col < N && load_col >= load_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        __syncthreads();\n\n        // Compute partial sum\n        if (col >= row && row < N && col < N) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (col >= row && row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through thread coarsening and loop unrolling while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.y * blockDim.y * 4 + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (col >= row && col < N) {\n        float sum[4] = {0.0f, 0.0f, 0.0f, 0.0f};\n        int rows[4] = {row, row + blockDim.y, row + 2*blockDim.y, row + 3*blockDim.y};\n        bool valid[4] = {rows[0] < N, rows[1] < N, rows[2] < N, rows[3] < N};\n\n        for (int k = 0; k < N; ++k) {\n            #pragma unroll\n            for (int i = 0; i < 4; ++i) {\n                if (valid[i] && k >= rows[i] && col >= rows[i]) {\n                    sum[i] += A[rows[i] * N + k] * B[k * N + col];\n                }\n            }\n        }\n\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            if (valid[i]) {\n                C[rows[i] * N + col] = sum[i];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(16, 4);\n    dim3 numBlocks((N + 15) / 16, (N + 3) / 4);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular multiplication through block-wise tiling and shared memory utilization, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = (row / TILE_SIZE) * TILE_SIZE; tile < N; tile += TILE_SIZE) {\n        if (row < N && (tile + threadIdx.x) < N && col >= row) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + (tile + threadIdx.x)];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if ((tile + threadIdx.y) < N && col < N && col >= (tile + threadIdx.y)) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tile + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (col >= row && row < N && col < N) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize by reducing redundant computations in upper triangular matrix multiplication through loop unrolling, shared memory caching, and minimizing thread divergence.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = (row / TILE_SIZE) * TILE_SIZE; tile < N; tile += TILE_SIZE) {\n        int load_row = row;\n        int load_col = tile + threadIdx.x;\n\n        if (load_row < N && load_col < N && load_col >= load_row) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * N + load_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        load_row = tile + threadIdx.y;\n        load_col = col;\n\n        if (load_row < N && load_col < N && load_col >= load_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll UNROLL_FACTOR\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular matrix multiplication through tiling and shared memory utilization, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = (row / TILE_SIZE) * TILE_SIZE; tile < N; tile += TILE_SIZE) {\n        int tile_row = tile + threadIdx.y;\n        int tile_col = tile + threadIdx.x;\n\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = (threadIdx.x >= threadIdx.y) ? A[row * N + tile_col] : 0.0f;\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N && col >= row) {\n            Bs[threadIdx.y][threadIdx.x] = (col >= tile_row) ? B[tile_row * N + col] : 0.0f;\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (col >= row && row < N && col < N) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (col >= row && row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular multiplication through tiling with shared memory and loop unrolling, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE + 1]; // +1 for bank conflict avoidance\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE + 1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = row; tile < N; tile += TILE_SIZE) {\n            int tile_end = min(tile + TILE_SIZE, N);\n            \n            // Load A tile\n            if (threadIdx.x == 0 && row < N && (tile + threadIdx.y) < N) {\n                As[threadIdx.y][0] = A[row * N + (tile + threadIdx.y)];\n            }\n            \n            // Load B tile\n            if (threadIdx.y == 0 && (tile + threadIdx.x) < N && col < N) {\n                Bs[threadIdx.x][0] = B[(tile + threadIdx.x) * N + col];\n            }\n            \n            __syncthreads();\n\n            // Compute partial sum with loop unrolling\n            int k = 0;\n            for (; k + UNROLL_FACTOR <= tile_end - tile; k += UNROLL_FACTOR) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n                sum += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n                sum += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n                sum += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n            }\n            for (; k < tile_end - tile; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n            \n            __syncthreads();\n        }\n        \n        if (row < N && col >= row && col < N) {\n            C[row * N + col] = sum;\n        }\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimized algorithm computes only upper triangular elements by leveraging shared memory for tile-based matrix multiplication with reduced computation for zero elements in upper triangular matrices.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = (row / TILE_SIZE) * TILE_SIZE; tile < N; tile += TILE_SIZE) {\n        int tile_row = tile + threadIdx.y;\n        int tile_col = tile + threadIdx.x;\n\n        if (row < N && tile_col < N && tile_col >= row) {\n            As[threadIdx.y][threadIdx.x] = A[row * N + tile_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N && col >= tile_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[tile_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (col >= row && row < N && col < N) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize by computing only upper triangular elements, using shared memory for tile-based matrix multiplication, and reducing redundant computations by leveraging the upper triangular property.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int tile_row = tile * TILE_SIZE + threadIdx.y;\n        int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n        // Load tiles into shared memory\n        if (row < N && tile_col < N) {\n            As[threadIdx.y][threadIdx.x] = (tile_col >= row) ? A[row * N + tile_col] : 0.0f;\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (tile_row < N && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = (col >= tile_row) ? B[tile_row * N + col] : 0.0f;\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        // Compute partial sum\n        if (row < N && col < N && col >= row) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimized algorithm computes only upper triangular elements using thread coarsening, loop unrolling, and shared memory to reduce redundant computations and memory accesses.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float s_data[];\n    float* sA = s_data;\n    float* sB = s_data + blockDim.y * blockDim.x;\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row >= N || col < row) return;\n\n    float sum = 0.0f;\n    const int tile_size = 32;\n    const int num_tiles = (N + tile_size - 1) / tile_size;\n\n    for (int t = 0; t < num_tiles; ++t) {\n        int tiled_row = row;\n        int tiled_col = t * tile_size + threadIdx.x;\n        if (tiled_col < N) {\n            sA[threadIdx.y * tile_size + threadIdx.x] = A[tiled_row * N + tiled_col];\n        }\n\n        tiled_row = t * tile_size + threadIdx.y;\n        tiled_col = col;\n        if (tiled_row < N) {\n            sB[threadIdx.y * tile_size + threadIdx.x] = B[tiled_row * N + tiled_col];\n        }\n        __syncthreads();\n\n        for (int k = 0; k < tile_size; ++k) {\n            int global_k = t * tile_size + k;\n            if (global_k >= N) break;\n            if (global_k >= row) {\n                sum += sA[threadIdx.y * tile_size + k] * sB[k * tile_size + threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    const int tile_size = 32;\n    dim3 threadsPerBlock(tile_size, tile_size);\n    dim3 numBlocks((N + tile_size - 1) / tile_size, (N + tile_size - 1) / tile_size);\n\n    size_t shared_mem_size = 2 * tile_size * tile_size * sizeof(float);\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock, shared_mem_size>>>(\n        A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimized algorithm computes only upper triangular elements by leveraging shared memory for block-level matrix multiplication and reducing redundant computations through loop unrolling and thread coarsening.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE + 1];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE + 1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = row; tile < N; tile += TILE_SIZE) {\n        int tile_end = min(tile + TILE_SIZE, N);\n        \n        for (int k = tile + threadIdx.x; k < tile_end; k += blockDim.x) {\n            if (row < N && k < N && k >= row) {\n                sA[threadIdx.y][k - tile] = A[row * N + k];\n            }\n        }\n        \n        for (int k = tile + threadIdx.y; k < tile_end; k += blockDim.y) {\n            if (col < N && k < N && col >= k) {\n                sB[k - tile][threadIdx.x] = B[k * N + col];\n            }\n        }\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE && (tile + k) < N; k += UNROLL_FACTOR) {\n            if (col >= row + tile + k) {\n                sum += sA[threadIdx.y][k] * sB[k][threadIdx.x];\n                if (k + 1 < TILE_SIZE && (tile + k + 1) < N) sum += sA[threadIdx.y][k+1] * sB[k+1][threadIdx.x];\n                if (k + 2 < TILE_SIZE && (tile + k + 2) < N) sum += sA[threadIdx.y][k+2] * sB[k+2][threadIdx.x];\n                if (k + 3 < TILE_SIZE && (tile + k + 3) < N) sum += sA[threadIdx.y][k+3] * sB[k+3][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize by leveraging shared memory to cache tiles of A and B, and only computing upper triangular elements while reducing redundant computations through loop tiling and proper thread indexing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float sA[TILE_SIZE][TILE_SIZE];\n    __shared__ float sB[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tRow = t * TILE_SIZE + threadIdx.y;\n        int tCol = t * TILE_SIZE + threadIdx.x;\n\n        sA[threadIdx.y][threadIdx.x] = (row < N && tCol < N) ? A[row * N + tCol] : 0.0f;\n        sB[threadIdx.y][threadIdx.x] = (tRow < N && col < N && col >= tRow) ? B[tRow * N + col] : 0.0f;\n\n        __syncthreads();\n\n        if (col >= row && row < N && col < N) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += sA[threadIdx.y][k] * sB[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (col >= row && row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimized algorithm computes only upper triangular elements with coalesced memory access and loop unrolling for better memory bandwidth utilization and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N && col >= row) {\n        float sum = 0.0f;\n        int k = row;\n        int end_k = N - (N - row) % UNROLL_FACTOR;\n        \n        for (; k < end_k; k += UNROLL_FACTOR) {\n            sum += A[row * N + k] * B[k * N + col];\n            sum += A[row * N + k+1] * B[(k+1) * N + col];\n            sum += A[row * N + k+2] * B[(k+2) * N + col];\n            sum += A[row * N + k+3] * B[(k+3) * N + col];\n        }\n        \n        for (; k < N; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        \n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(32, 32);\n    dim3 numBlocks((N + 31) / 32, (N + 31) / 32);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimized algorithm computes upper triangular matrix multiplication by leveraging shared memory for data reuse and reducing redundant computations by only processing elements where col >= row, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = 0; tile < (N + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n            int tile_row = tile * TILE_SIZE + threadIdx.y;\n            int tile_col = tile * TILE_SIZE + threadIdx.x;\n\n            if (tile_row < N && threadIdx.x + tile * TILE_SIZE < N && row <= threadIdx.x + tile * TILE_SIZE) {\n                As[threadIdx.y][threadIdx.x] = A[row * N + (threadIdx.x + tile * TILE_SIZE)];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            if (tile_col < N && threadIdx.y + tile * TILE_SIZE < N && col >= threadIdx.y + tile * TILE_SIZE) {\n                Bs[threadIdx.y][threadIdx.x] = B[(threadIdx.y + tile * TILE_SIZE) * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            __syncthreads();\n\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n            __syncthreads();\n        }\n\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular multiplication through thread coarsening and loop unrolling while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col_start = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (row >= N) return;\n    \n    for (int col = col_start; col < N && col >= row; col += blockDim.x * gridDim.x) {\n        float sum = 0.0f;\n        #pragma unroll 4\n        for (int k = row; k < N; ++k) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(32, 8);\n    dim3 numBlocks((N + 31) / 32, (N + 7) / 8);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the kernel by reducing redundant computations in the upper triangular matrix multiplication through loop unrolling, shared memory caching, and minimizing thread divergence while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = row; tile < N; tile += TILE_SIZE) {\n            int k = tile + threadIdx.x;\n            if (k < N) {\n                As[threadIdx.y][threadIdx.x] = A[row * N + k];\n            }\n            k = tile + threadIdx.y;\n            if (k < N && (col < N)) {\n                Bs[threadIdx.y][threadIdx.x] = B[k * N + col];\n            }\n            __syncthreads();\n\n            int max_k = min(tile + TILE_SIZE, N);\n            for (int kk = 0; kk < max_k - tile; kk += UNROLL_FACTOR) {\n                #pragma unroll\n                for (int i = 0; i < UNROLL_FACTOR; ++i) {\n                    if (kk + i < max_k - tile && (tile + kk + i) >= row) {\n                        sum += As[threadIdx.y][kk + i] * Bs[kk + i][threadIdx.x];\n                    }\n                }\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimized algorithm computes only upper triangular elements by leveraging shared memory for block-level matrix multiplication and reducing the loop range for each thread based on its position in the upper triangle.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    extern __shared__ float shared_mem[];\n    float* sA = shared_mem;\n    float* sB = shared_mem + blockDim.y * blockDim.x;\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (col < N && row < N && col >= row) {\n        float sum = 0.0f;\n        for (int k = 0; k < ((N + blockDim.x - 1) / blockDim.x); ++k) {\n            int tile_k = k * blockDim.x;\n            int load_row = row;\n            int load_col = tile_k + threadIdx.x;\n            \n            if (load_col < N && load_col >= load_row) {\n                sA[threadIdx.y * blockDim.x + threadIdx.x] = A[load_row * N + load_col];\n            } else {\n                sA[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n            }\n            \n            load_row = tile_k + threadIdx.y;\n            load_col = col;\n            \n            if (load_col < N && load_row < N && load_col >= load_row) {\n                sB[threadIdx.y * blockDim.x + threadIdx.x] = B[load_row * N + load_col];\n            } else {\n                sB[threadIdx.y * blockDim.x + threadIdx.x] = 0.0f;\n            }\n            \n            __syncthreads();\n            \n            for (int i = 0; i < blockDim.x; ++i) {\n                sum += sA[threadIdx.y * blockDim.x + i] * sB[i * blockDim.x + threadIdx.x];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(16, 16);\n    dim3 numBlocks((N + 15) / 16, (N + 15) / 16);\n    size_t shared_mem_size = 2 * threadsPerBlock.x * threadsPerBlock.y * sizeof(float);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock, shared_mem_size>>>(\n        A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimized algorithm computes only upper triangular elements by leveraging shared memory for tile-based matrix multiplication and reducing redundant computations for lower triangular elements.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int tile = (row / TILE_SIZE) * TILE_SIZE; tile < N; tile += TILE_SIZE) {\n        int local_row = row;\n        int local_col = tile + threadIdx.x;\n        if (local_row < N && local_col < N && local_col >= local_row) {\n            As[threadIdx.y][threadIdx.x] = A[local_row * N + local_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        local_row = tile + threadIdx.y;\n        local_col = col;\n        if (local_row < N && local_col < N && local_col >= local_row) {\n            Bs[threadIdx.y][threadIdx.x] = B[local_row * N + local_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        if (row < N && col < N && col >= row) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (row < N && col < N && col >= row) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular matrix multiplication through loop unrolling, shared memory caching, and minimizing thread divergence by focusing only on upper triangular elements.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_upper_triangular_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    if (row < N && col >= row && col < N) {\n        for (int tile = row; tile < N; tile += TILE_SIZE) {\n            int k = tile + threadIdx.x;\n            if (k < N) {\n                As[threadIdx.y][threadIdx.x] = A[row * N + k];\n            } else {\n                As[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n\n            k = tile + threadIdx.y;\n            if (k < N && (col < N)) {\n                Bs[threadIdx.y][threadIdx.x] = B[k * N + col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x] = 0.0f;\n            }\n            __syncthreads();\n\n            #pragma unroll\n            for (int i = 0; i < TILE_SIZE; ++i) {\n                sum += As[threadIdx.y][i] * Bs[i][threadIdx.x];\n            }\n            __syncthreads();\n        }\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 numBlocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize by reducing redundant computations in the upper triangular multiplication through thread coarsening and loop unrolling, while maintaining correctness by only computing elements where col >= row.}",
        "function": "#include <torch/extension.h>\n\n__global__ void matmul_upper_triangular_kernel(const float* A, const float* B, float* C, int N) {\n    int row = blockIdx.y * blockDim.y * 4 + threadIdx.y;\n    int col = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            int current_row = row + i * blockDim.y;\n            int current_col = col + j * blockDim.x;\n            \n            if (current_row < N && current_col < N && current_col >= current_row) {\n                float sum = 0.0f;\n                #pragma unroll(4)\n                for (int k = current_row; k < N; ++k) {\n                    sum += A[current_row * N + k] * B[k * N + current_col];\n                }\n                C[current_row * N + current_col] = sum;\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_upper_triangular(torch::Tensor A, torch::Tensor B) {\n    int N = A.size(0);\n    torch::Tensor C = torch::zeros_like(A);\n\n    dim3 threadsPerBlock(8, 8);\n    dim3 numBlocks((N + 31) / 32, (N + 31) / 32);\n\n    matmul_upper_triangular_kernel<<<numBlocks, threadsPerBlock>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N);\n\n    cudaDeviceSynchronize();\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_upper_triangular, \"Upper triangular matrix multiplication\");\n}",
        "score": null
    }
]