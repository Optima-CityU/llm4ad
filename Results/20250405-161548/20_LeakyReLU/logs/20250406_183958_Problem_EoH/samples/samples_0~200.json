[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by using vectorized memory access and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                  scalar_t* __restrict__ output,\n                                  float negative_slope,\n                                  size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx < num_elements) {\n        float4 in_val;\n        in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        \n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&in_val)[i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = in_val;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(float);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives for reduced branching, and increased occupancy with proper block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * static_cast<scalar_t>(negative_slope);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int numSMs;\n    cudaDeviceGetAttribute(&numSMs, cudaDevAttrMultiProcessorCount, device);\n    \n    const int threads = 256;\n    const int blocks = 4 * numSMs;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by using vectorized memory access, loop unrolling, and warp-level operations to maximize memory throughput and minimize branch divergence.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                  scalar_t* __restrict__ output,\n                                  float negative_slope,\n                                  size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_val;\n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&out_val)[i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n        *reinterpret_cast<float4*>(&output[idx]) = out_val;\n    } else {\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t val = input[idx + i];\n            output[idx + i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(float);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives, and minimizing branch divergence while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_val;\n        \n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&out_val)[i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = out_val;\n    } else {\n        // Handle remaining elements\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t val = input[idx + i];\n            output[idx + i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    const int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives, and minimizing branch divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size <= num_elements) {\n        float4 in_vec = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_vec;\n        \n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t in_val = reinterpret_cast<scalar_t*>(&in_vec)[i];\n            reinterpret_cast<scalar_t*>(&out_vec)[i] = \n                in_val > static_cast<scalar_t>(0) ? in_val : in_val * negative_slope;\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = out_vec;\n    } else {\n        // Handle remaining elements\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = in_val > static_cast<scalar_t>(0) ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(float);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by using vectorized memory access, loop unrolling, and minimizing branch divergence while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void leaky_relu_kernel_vec(const scalar_t* __restrict__ input,\n                                     scalar_t* __restrict__ output,\n                                     float negative_slope,\n                                     size_t num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    \n    if (idx + VEC_SIZE <= num_elements) {\n        scalar_t in_vals[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            in_vals[i] = input[idx + i];\n        }\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            output[idx + i] = (in_vals[i] > static_cast<scalar_t>(0)) ? in_vals[i] : in_vals[i] * negative_slope;\n        }\n    } else {\n        for (int i = 0; i < VEC_SIZE && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int vec_size = 4;\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel_vec<scalar_t, vec_size><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, warp-level primitives for efficient branching, and maximizing occupancy through optimal block and thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n#include <cub/cub.cuh>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int element_idx = idx + i * blockDim.x;\n        if (element_idx < num_elements) {\n            scalar_t in_val = input[element_idx];\n            output[element_idx] = (in_val > static_cast<scalar_t>(0)) ? \n                                in_val : in_val * static_cast<scalar_t>(negative_slope);\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, \n        input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the LeakyReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = in_val > static_cast<scalar_t>(0) ? in_val : static_cast<scalar_t>(in_val * negative_slope);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    if (idx < num_elements) {\n        scalar_t in_val1 = input[idx];\n        scalar_t in_val2 = idx + blockDim.x < num_elements ? input[idx + blockDim.x] : static_cast<scalar_t>(0);\n        scalar_t in_val3 = idx + 2 * blockDim.x < num_elements ? input[idx + 2 * blockDim.x] : static_cast<scalar_t>(0);\n        scalar_t in_val4 = idx + 3 * blockDim.x < num_elements ? input[idx + 3 * blockDim.x] : static_cast<scalar_t>(0);\n        \n        output[idx] = (in_val1 > static_cast<scalar_t>(0)) ? in_val1 : in_val1 * negative_slope;\n        if (idx + blockDim.x < num_elements) {\n            output[idx + blockDim.x] = (in_val2 > static_cast<scalar_t>(0)) ? in_val2 : in_val2 * negative_slope;\n        }\n        if (idx + 2 * blockDim.x < num_elements) {\n            output[idx + 2 * blockDim.x] = (in_val3 > static_cast<scalar_t>(0)) ? in_val3 : in_val3 * negative_slope;\n        }\n        if (idx + 3 * blockDim.x < num_elements) {\n            output[idx + 3 * blockDim.x] = (in_val4 > static_cast<scalar_t>(0)) ? in_val4 : in_val4 * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;  // Reduced for better occupancy\n    const int blocks = (num_elements + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, maximizing occupancy with optimal block size, and minimizing branch divergence through warp-level operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void leaky_relu_kernel_vectorized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    float negative_slope,\n    size_t num_elements) {\n    \n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    if (idx + VEC_SIZE - 1 < num_elements) {\n        scalar_t in_val[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            in_val[i] = input[idx + i];\n        }\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            output[idx + i] = (in_val[i] > static_cast<scalar_t>(0)) ? in_val[i] : in_val[i] * negative_slope;\n        }\n    } else {\n        for (int i = 0; i < VEC_SIZE && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int VEC_SIZE = 4;\n    const int threads = 256;\n    const int blocks = (num_elements + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel_vectorized<scalar_t, VEC_SIZE><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by using vectorized memory access, warp-level primitives for reduced branching, and maximizing memory throughput while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                  scalar_t* __restrict__ output,\n                                  float negative_slope,\n                                  size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx < num_elements) {\n        float4 in_val;\n        in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        \n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&in_val)[i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = in_val;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int threads = 256;\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by using vectorized memory access, maximizing memory coalescing, and minimizing warp divergence through improved branch handling.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_val;\n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&out_val)[i] = val > static_cast<scalar_t>(0) ? val : val * negative_slope;\n        }\n        *reinterpret_cast<float4*>(&output[idx]) = out_val;\n    } else {\n        // Handle remaining elements\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = in_val > static_cast<scalar_t>(0) ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <c10/util/Half.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = (in_val > static_cast<scalar_t>(0)) ? in_val : static_cast<scalar_t>(in_val * negative_slope);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimized the LeakyReLU kernel by using vectorized memory accesses, warp-level primitives for reduced branching, and increased occupancy through optimal block and grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = (in_val > static_cast<scalar_t>(0)) ? in_val : static_cast<scalar_t>(negative_slope) * in_val;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int multiprocessor_count;\n    cudaDeviceGetAttribute(&multiprocessor_count, cudaDevAttrMultiProcessorCount, device);\n    \n    const int threads = 256;\n    const int blocks = 4 * multiprocessor_count;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives, and minimizing branch divergence while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <ATen/cuda/CUDAContext.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int stride = blockDim.x * gridDim.x;\n    const scalar_t slope = static_cast<scalar_t>(negative_slope);\n    \n    for (int idx = blockIdx.x * blockDim.x + threadIdx.x; \n         idx < num_elements; \n         idx += stride) {\n        scalar_t in_val = input[idx];\n        output[idx] = in_val > static_cast<scalar_t>(0) ? in_val : in_val * slope;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int numSMs;\n    cudaDeviceGetAttribute(&numSMs, cudaDevAttrMultiProcessorCount, device);\n    const int threads = 256;\n    const int blocks = 4 * numSMs;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by using vectorized memory accesses, warp-level primitives for reduced branching, and maximizing memory throughput while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_val;\n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&out_val)[i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n        *reinterpret_cast<float4*>(&output[idx]) = out_val;\n    } else {\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(float);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimized the LeakyReLU kernel by using vectorized memory access, warp-level primitives for reduced branching, and maximizing memory throughput while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = fmaxf(in_val, in_val * negative_slope);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int threads = 256;\n    int blocks = (num_elements + threads - 1) / threads;\n    blocks = min(blocks, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, warp-level primitives for reduced branching, and maximizing memory throughput while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_vec = reinterpret_cast<const float4*>(input)[idx / vec_size];\n        float4 out_vec;\n        \n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_vec)[i];\n            reinterpret_cast<scalar_t*>(&out_vec)[i] = \n                val > static_cast<scalar_t>(0) ? val : val * negative_slope;\n        }\n        \n        reinterpret_cast<float4*>(output)[idx / vec_size] = out_vec;\n    } else {\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t val = input[idx + i];\n            output[idx + i] = val > static_cast<scalar_t>(0) ? val : val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(float);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the LeakyReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = in_val * (in_val > static_cast<scalar_t>(0) ? static_cast<scalar_t>(1) : static_cast<scalar_t>(negative_slope));\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimized the LeakyReLU kernel by using vectorized memory access, warp-level primitives for reduced branching, and increased occupancy through optimal block/thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        const int element_idx = idx + i * blockDim.x;\n        if (element_idx < num_elements) {\n            const scalar_t in_val = input[element_idx];\n            output[element_idx] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives for reduced branching, and maximizing memory throughput while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_val;\n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&out_val)[i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n        *reinterpret_cast<float4*>(&output[idx]) = out_val;\n    } else {\n        // Handle remaining elements\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t val = input[idx + i];\n            output[idx + i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimized the LeakyReLU kernel by using vectorized memory access, warp-level primitives, and increased occupancy with proper block/grid sizing while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * static_cast<scalar_t>(negative_slope);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int multiprocessor_count;\n    cudaDeviceGetAttribute(&multiprocessor_count, cudaDevAttrMultiProcessorCount, device);\n    \n    const int threads = 256;\n    const int blocks = 4 * multiprocessor_count;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, warp-level primitives for reduced branching, and maximizing memory coalescing while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_vec = reinterpret_cast<const float4*>(input)[idx / vec_size];\n        float4 out_vec;\n        \n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_vec)[i];\n            reinterpret_cast<scalar_t*>(&out_vec)[i] = val > static_cast<scalar_t>(0) ? val : val * negative_slope;\n        }\n        \n        reinterpret_cast<float4*>(output)[idx / vec_size] = out_vec;\n    } else {\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t val = input[idx + i];\n            output[idx + i] = val > static_cast<scalar_t>(0) ? val : val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives for efficiency, and ensuring proper type conversion for the negative slope.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const scalar_t slope = static_cast<scalar_t>(negative_slope);\n    const int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x * 4;\n    \n    if (idx + 3 < num_elements) {\n        scalar_t in_val[4];\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            in_val[i] = input[idx + i];\n            output[idx + i] = (in_val[i] > static_cast<scalar_t>(0)) ? in_val[i] : in_val[i] * slope;\n        }\n    } else {\n        for (int i = 0; i < 4 && (idx + i) < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, warp-level primitives, and minimizing branch divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                  scalar_t* __restrict__ output,\n                                  float negative_slope,\n                                  size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = fmaxf(in_val, in_val * negative_slope);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int multiprocessor_count;\n    cudaDeviceGetAttribute(&multiprocessor_count, cudaDevAttrMultiProcessorCount, device);\n    \n    const int threads = 256;\n    const int blocks = 4 * multiprocessor_count;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the LeakyReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = fmaxf(in_val, in_val * negative_slope);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int numSMs;\n    cudaDeviceGetAttribute(&numSMs, cudaDevAttrMultiProcessorCount, device);\n    \n    const int threads = 256;\n    const int blocks = 4 * numSMs;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by using vectorized memory accesses, maximizing occupancy with optimal block size, and minimizing branch divergence with a fused multiply-add operation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n    if (idx + 3 < num_elements) {\n        scalar_t in_val0 = input[idx];\n        scalar_t in_val1 = input[idx+1];\n        scalar_t in_val2 = input[idx+2];\n        scalar_t in_val3 = input[idx+3];\n        \n        output[idx]   = in_val0 * (in_val0 > 0 ? 1.0f : negative_slope);\n        output[idx+1] = in_val1 * (in_val1 > 0 ? 1.0f : negative_slope);\n        output[idx+2] = in_val2 * (in_val2 > 0 ? 1.0f : negative_slope);\n        output[idx+3] = in_val3 * (in_val3 > 0 ? 1.0f : negative_slope);\n    } else {\n        for (int i = 0; i < 4 && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = in_val * (in_val > 0 ? 1.0f : negative_slope);\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + 4 * threads - 1) / (4 * threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives, and minimizing branch divergence while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx < num_elements) {\n        float4 in_val;\n        in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        \n        scalar_t out_val[vec_size];\n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_val)[i];\n            out_val[i] = val > static_cast<scalar_t>(0) ? val : val * negative_slope;\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = *reinterpret_cast<float4*>(out_val);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, warp-level primitives for reduced branching, and increased occupancy through optimal block and grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void leaky_relu_kernel_vec(const scalar_t* __restrict__ input,\n                                     scalar_t* __restrict__ output,\n                                     float negative_slope,\n                                     size_t num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    if (idx < num_elements) {\n        scalar_t in_vals[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (idx + i < num_elements) {\n                in_vals[i] = input[idx + i];\n            }\n        }\n\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (idx + i < num_elements) {\n                output[idx + i] = (in_vals[i] > static_cast<scalar_t>(0)) ? in_vals[i] : in_vals[i] * negative_slope;\n            }\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int vec_size = 4;\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel_vec<scalar_t, vec_size><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by using vectorized memory access, warp-level primitives, and minimizing branch divergence while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_val;\n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<const scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&out_val)[i] = val > static_cast<scalar_t>(0) ? val : val * negative_slope;\n        }\n        *reinterpret_cast<float4*>(&output[idx]) = out_val;\n    } else {\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t val = input[idx + i];\n            output[idx + i] = val > static_cast<scalar_t>(0) ? val : val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, warp-level primitives for reduced branching, and maximizing memory throughput while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_vec = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_vec;\n        \n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t in_val = reinterpret_cast<scalar_t*>(&in_vec)[i];\n            reinterpret_cast<scalar_t*>(&out_vec)[i] = \n                (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = out_vec;\n    } else {\n        // Handle remaining elements\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives, and minimizing branch divergence while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = in_val > scalar_t(0) ? in_val : scalar_t(in_val * negative_slope);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int numSMs;\n    cudaDeviceGetAttribute(&numSMs, cudaDevAttrMultiProcessorCount, device);\n    \n    const int threads = 256;\n    const int blocks = 4 * numSMs;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by using vectorized memory accesses, warp-level primitives for efficient branching, and maximizing memory throughput while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x * 4;\n    if (idx + 3 < num_elements) {\n        scalar_t in_val[4];\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            in_val[i] = input[idx + i];\n        }\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            output[idx + i] = (in_val[i] > static_cast<scalar_t>(0)) ? in_val[i] : in_val[i] * static_cast<scalar_t>(negative_slope);\n        }\n    } else {\n        for (int i = 0; i < 4 && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * static_cast<scalar_t>(negative_slope);\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimized the LeakyReLU kernel by using vectorized memory accesses, increased occupancy with optimal block size, and minimizing branch divergence with bitwise operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx < num_elements) {\n        float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_val;\n        \n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<const scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&out_val)[i] = \n                val > static_cast<scalar_t>(0) ? val : val * negative_slope;\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = out_val;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, warp-level primitives for reduced branching, and maximizing memory throughput while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void leaky_relu_kernel_vec(const scalar_t* __restrict__ input,\n                                     scalar_t* __restrict__ output,\n                                     float negative_slope,\n                                     size_t num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    if (idx < num_elements) {\n        scalar_t in_vals[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (idx + i < num_elements) {\n                in_vals[i] = input[idx + i];\n                output[idx + i] = (in_vals[i] > static_cast<scalar_t>(0)) ? in_vals[i] : in_vals[i] * negative_slope;\n            }\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int VEC_SIZE = 4;\n    const int threads = 256;\n    const int blocks = (num_elements + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel_vec<scalar_t, VEC_SIZE><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by increasing thread utilization through block size tuning, using vectorized memory access, and minimizing branching with predicated execution.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n    if (idx + 3 < num_elements) {\n        scalar_t in_val0 = input[idx];\n        scalar_t in_val1 = input[idx+1];\n        scalar_t in_val2 = input[idx+2];\n        scalar_t in_val3 = input[idx+3];\n        \n        output[idx]   = in_val0 > 0 ? in_val0 : in_val0 * negative_slope;\n        output[idx+1] = in_val1 > 0 ? in_val1 : in_val1 * negative_slope;\n        output[idx+2] = in_val2 > 0 ? in_val2 : in_val2 * negative_slope;\n        output[idx+3] = in_val3 > 0 ? in_val3 : in_val3 * negative_slope;\n    } else {\n        for (int i = 0; i < 4 && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = in_val > 0 ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;  // Reduced for better occupancy\n    const int blocks = (num_elements + 4 * threads - 1) / (4 * threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, maximizing memory coalescing, and minimizing warp divergence through branchless operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                  scalar_t* __restrict__ output,\n                                  float negative_slope,\n                                  size_t num_elements) {\n    const int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n    if (idx + 3 < num_elements) {\n        scalar_t in_val0 = input[idx];\n        scalar_t in_val1 = input[idx+1];\n        scalar_t in_val2 = input[idx+2];\n        scalar_t in_val3 = input[idx+3];\n        \n        output[idx] = fmaxf(in_val0, in_val0 * negative_slope);\n        output[idx+1] = fmaxf(in_val1, in_val1 * negative_slope);\n        output[idx+2] = fmaxf(in_val2, in_val2 * negative_slope);\n        output[idx+3] = fmaxf(in_val3, in_val3 * negative_slope);\n    } else {\n        for (int i = 0; i < 4 && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = fmaxf(in_val, in_val * negative_slope);\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + 4 * threads - 1) / (4 * threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, warp-level primitives for reduced branching, and maximizing memory coalescing while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <c10/util/Half.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = (in_val > static_cast<scalar_t>(0)) ? in_val : static_cast<scalar_t>(in_val * negative_slope);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = std::min((num_elements + threads - 1) / threads, 2048);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives for reduced branching, and maximizing memory coalescing while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_val;\n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&out_val)[i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n        *reinterpret_cast<float4*>(&output[idx]) = out_val;\n    } else {\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t val = input[idx + i];\n            output[idx + i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the LeakyReLU kernel by using warp-level primitives for coalesced memory access and reduced branch divergence, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    float negative_slope,\n    size_t num_elements) {\n    \n    const int stride = blockDim.x * gridDim.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    while (idx < num_elements) {\n        scalar_t in_val = input[idx];\n        output[idx] = (in_val > static_cast<scalar_t>(0)) ? in_val : static_cast<scalar_t>(negative_slope) * in_val;\n        idx += stride;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device);\n    \n    int threads = prop.maxThreadsPerBlock;\n    int blocks = prop.multiProcessorCount * 4;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access and warp-level primitives to reduce memory transactions and improve instruction throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    if (idx < num_elements) {\n        scalar_t in_val1 = input[idx];\n        scalar_t in_val2 = (idx + blockDim.x < num_elements) ? input[idx + blockDim.x] : static_cast<scalar_t>(0);\n        scalar_t in_val3 = (idx + 2 * blockDim.x < num_elements) ? input[idx + 2 * blockDim.x] : static_cast<scalar_t>(0);\n        scalar_t in_val4 = (idx + 3 * blockDim.x < num_elements) ? input[idx + 3 * blockDim.x] : static_cast<scalar_t>(0);\n        \n        output[idx] = (in_val1 > static_cast<scalar_t>(0)) ? in_val1 : in_val1 * negative_slope;\n        if (idx + blockDim.x < num_elements) {\n            output[idx + blockDim.x] = (in_val2 > static_cast<scalar_t>(0)) ? in_val2 : in_val2 * negative_slope;\n        }\n        if (idx + 2 * blockDim.x < num_elements) {\n            output[idx + 2 * blockDim.x] = (in_val3 > static_cast<scalar_t>(0)) ? in_val3 : in_val3 * negative_slope;\n        }\n        if (idx + 3 * blockDim.x < num_elements) {\n            output[idx + 3 * blockDim.x] = (in_val4 > static_cast<scalar_t>(0)) ? in_val4 : in_val4 * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the LeakyReLU CUDA kernel by using vectorized memory access, warp-level primitives, and increased occupancy while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_val = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_val;\n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t val = reinterpret_cast<const scalar_t*>(&in_val)[i];\n            reinterpret_cast<scalar_t*>(&out_val)[i] = (val > static_cast<scalar_t>(0)) ? val : val * negative_slope;\n        }\n        *reinterpret_cast<float4*>(&output[idx]) = out_val;\n    } else {\n        // Handle remaining elements\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives for reduced branching, and increased thread utilization with proper grid-stride loops.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int stride = blockDim.x * gridDim.x;\n    for (int idx = blockIdx.x * blockDim.x + threadIdx.x; \n         idx < num_elements; \n         idx += stride) {\n        scalar_t in_val = input[idx];\n        output[idx] = (in_val > static_cast<scalar_t>(0)) ? in_val : static_cast<scalar_t>(negative_slope) * in_val;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int multiprocessor_count;\n    cudaDeviceGetAttribute(&multiprocessor_count, cudaDevAttrMultiProcessorCount, device);\n    \n    const int threads = 256;\n    const int blocks = 4 * multiprocessor_count;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the LeakyReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    cudaDeviceProp props;\n    cudaGetDeviceProperties(&props, device);\n    \n    int threads = props.maxThreadsPerBlock;\n    int blocks = (num_elements + threads - 1) / threads;\n    blocks = std::min(blocks, props.multiProcessorCount * 32);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives for efficient branching, and maximizing memory throughput while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    if (idx < num_elements) {\n        scalar_t in_val1 = input[idx];\n        scalar_t in_val2 = (idx + blockDim.x < num_elements) ? input[idx + blockDim.x] : 0;\n        scalar_t in_val3 = (idx + 2 * blockDim.x < num_elements) ? input[idx + 2 * blockDim.x] : 0;\n        scalar_t in_val4 = (idx + 3 * blockDim.x < num_elements) ? input[idx + 3 * blockDim.x] : 0;\n        \n        output[idx] = (in_val1 > 0) ? in_val1 : in_val1 * negative_slope;\n        if (idx + blockDim.x < num_elements)\n            output[idx + blockDim.x] = (in_val2 > 0) ? in_val2 : in_val2 * negative_slope;\n        if (idx + 2 * blockDim.x < num_elements)\n            output[idx + 2 * blockDim.x] = (in_val3 > 0) ? in_val3 : in_val3 * negative_slope;\n        if (idx + 3 * blockDim.x < num_elements)\n            output[idx + 3 * blockDim.x] = (in_val4 > 0) ? in_val4 : in_val4 * negative_slope;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level operations, and minimizing branch divergence while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x * 4;\n    if (idx < num_elements) {\n        const scalar_t in_val0 = input[idx];\n        const scalar_t in_val1 = input[idx + 1];\n        const scalar_t in_val2 = input[idx + 2];\n        const scalar_t in_val3 = input[idx + 3];\n        \n        output[idx] = in_val0 > static_cast<scalar_t>(0) ? in_val0 : in_val0 * negative_slope;\n        output[idx + 1] = in_val1 > static_cast<scalar_t>(0) ? in_val1 : in_val1 * negative_slope;\n        output[idx + 2] = in_val2 > static_cast<scalar_t>(0) ? in_val2 : in_val2 * negative_slope;\n        output[idx + 3] = in_val3 > static_cast<scalar_t>(0) ? in_val3 : in_val3 * negative_slope;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives for reduced branching, and maximizing memory throughput while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <c10/util/Half.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = in_val > scalar_t(0) ? in_val : static_cast<scalar_t>(in_val * negative_slope);\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, maximizing occupancy through optimal block/thread configuration, and minimizing warp divergence with branchless operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n    if (idx + 3 < num_elements) {\n        scalar_t in_val0 = input[idx];\n        scalar_t in_val1 = input[idx+1];\n        scalar_t in_val2 = input[idx+2];\n        scalar_t in_val3 = input[idx+3];\n        \n        output[idx] = fmaxf(in_val0, in_val0 * negative_slope);\n        output[idx+1] = fmaxf(in_val1, in_val1 * negative_slope);\n        output[idx+2] = fmaxf(in_val2, in_val2 * negative_slope);\n        output[idx+3] = fmaxf(in_val3, in_val3 * negative_slope);\n    } else {\n        for (int i = 0; i < 4 && idx + i < num_elements; i++) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = fmaxf(in_val, in_val * negative_slope);\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + 4 * threads - 1) / (4 * threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access with 128-bit loads/stores and unrolling the loop to reduce instruction overhead while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_vec = *reinterpret_cast<const float4*>(&input[idx]);\n        scalar_t* in_vals = reinterpret_cast<scalar_t*>(&in_vec);\n        scalar_t out_vals[vec_size];\n        \n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            out_vals[i] = (in_vals[i] > static_cast<scalar_t>(0)) ? in_vals[i] : in_vals[i] * negative_slope;\n        }\n        \n        *reinterpret_cast<float4*>(&output[idx]) = *reinterpret_cast<float4*>(out_vals);\n    } else {\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory accesses, warp-level primitives, and maximizing memory coalescing while maintaining numerical correctness.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    if (idx < num_elements) {\n        scalar_t in_val1 = input[idx];\n        scalar_t in_val2 = (idx + blockDim.x < num_elements) ? input[idx + blockDim.x] : static_cast<scalar_t>(0);\n        scalar_t in_val3 = (idx + 2 * blockDim.x < num_elements) ? input[idx + 2 * blockDim.x] : static_cast<scalar_t>(0);\n        scalar_t in_val4 = (idx + 3 * blockDim.x < num_elements) ? input[idx + 3 * blockDim.x] : static_cast<scalar_t>(0);\n\n        output[idx] = (in_val1 > static_cast<scalar_t>(0)) ? in_val1 : in_val1 * negative_slope;\n        if (idx + blockDim.x < num_elements) output[idx + blockDim.x] = (in_val2 > static_cast<scalar_t>(0)) ? in_val2 : in_val2 * negative_slope;\n        if (idx + 2 * blockDim.x < num_elements) output[idx + 2 * blockDim.x] = (in_val3 > static_cast<scalar_t>(0)) ? in_val3 : in_val3 * negative_slope;\n        if (idx + 3 * blockDim.x < num_elements) output[idx + 3 * blockDim.x] = (in_val4 > static_cast<scalar_t>(0)) ? in_val4 : in_val4 * negative_slope;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (num_elements + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize the LeakyReLU kernel by using vectorized memory access, warp-level primitives for reduced branching, and maximizing memory throughput while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    \n    if (idx + vec_size - 1 < num_elements) {\n        float4 in_vec = *reinterpret_cast<const float4*>(&input[idx]);\n        float4 out_vec;\n        \n        #pragma unroll\n        for (int i = 0; i < vec_size; ++i) {\n            scalar_t in_val = reinterpret_cast<scalar_t*>(&in_vec)[i];\n            reinterpret_cast<scalar_t*>(&out_vec)[i] = \n                (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n        }\n        *reinterpret_cast<float4*>(&output[idx]) = out_vec;\n    } else {\n        for (int i = 0; i < vec_size && idx + i < num_elements; ++i) {\n            scalar_t in_val = input[idx + i];\n            output[idx + i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    constexpr int threads = 256;\n    constexpr int vec_size = sizeof(float4) / sizeof(float);\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimized the kernel by using vectorized memory access, warp-level primitives for reduced branching, and increased occupancy with proper block/grid sizing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel(const scalar_t* __restrict__ input,\n                                 scalar_t* __restrict__ output,\n                                 float negative_slope,\n                                 size_t num_elements) {\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = (in_val > static_cast<scalar_t>(0)) ? in_val : in_val * negative_slope;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    int multiprocessor_count;\n    cudaDeviceGetAttribute(&multiprocessor_count, cudaDevAttrMultiProcessorCount, device);\n    \n    const int threads = 256;\n    const int blocks = 4 * multiprocessor_count;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimize the LeakyReLU kernel by increasing thread utilization through block size tuning and using vectorized memory access for improved memory throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t, int VEC_SIZE>\n__global__ void leaky_relu_kernel_vec(const scalar_t* __restrict__ input,\n                                     scalar_t* __restrict__ output,\n                                     float negative_slope,\n                                     size_t num_elements) {\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    if (idx < num_elements) {\n        scalar_t in_vals[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (idx + i < num_elements) {\n                in_vals[i] = input[idx + i];\n                output[idx + i] = (in_vals[i] > static_cast<scalar_t>(0)) ? in_vals[i] : in_vals[i] * negative_slope;\n            }\n        }\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    const int vec_size = 4;\n    const int threads = 256;\n    const int blocks = (num_elements + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel_vec<scalar_t, vec_size><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n\n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize the LeakyReLU kernel by using warp-level primitives for coalesced memory access and reduced branching, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\ntemplate <typename scalar_t>\n__global__ void leaky_relu_kernel_optimized(\n    const scalar_t* __restrict__ input,\n    scalar_t* __restrict__ output,\n    float negative_slope,\n    size_t num_elements) {\n    \n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < num_elements; i += stride) {\n        scalar_t in_val = input[i];\n        output[i] = in_val > static_cast<scalar_t>(0) ? in_val : static_cast<scalar_t>(negative_slope) * in_val;\n    }\n}\n\ntorch::Tensor leaky_relu_forward(torch::Tensor input, float negative_slope) {\n    auto output = torch::empty_like(input);\n    size_t num_elements = input.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    cudaDeviceProp props;\n    cudaGetDeviceProperties(&props, device);\n    \n    int threads = props.maxThreadsPerBlock;\n    int blocks = props.multiProcessorCount * (props.maxThreadsPerMultiProcessor / threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"leaky_relu_forward\", ([&] {\n        leaky_relu_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            negative_slope,\n            num_elements);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &leaky_relu_forward, \"LeakyReLU forward (CUDA)\");\n}",
        "score": null
    }
]