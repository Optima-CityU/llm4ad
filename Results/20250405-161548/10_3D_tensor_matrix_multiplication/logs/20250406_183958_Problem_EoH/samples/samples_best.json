[
    {
        "sample_order": 2,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for matrix B, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        if (threadIdx.y == 0 && l < L) {\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                Bs[k][threadIdx.x] = B[(kk + k) * L + l];\n            }\n        }\n        __syncthreads();\n\n        if (n < N && m < M) {\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += A[n * M * K + m * K + (kk + k)] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": -129.11346435546875
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for matrix B, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += BLOCK_SIZE) {\n        if (threadIdx.x < BLOCK_SIZE && (kk + threadIdx.x) < K && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        }\n        __syncthreads();\n\n        for (int k = kk; k < min(kk + BLOCK_SIZE, K); k++) {\n            if (n < N && m < M) {\n                value += A[n * M * K + m * K + k] * Bs[k - kk][threadIdx.y];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": -98.39430084228516
    }
]