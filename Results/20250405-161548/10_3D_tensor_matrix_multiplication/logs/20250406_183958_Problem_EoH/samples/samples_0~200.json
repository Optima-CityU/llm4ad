[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the CUDA kernel by utilizing shared memory for matrix B to reduce global memory accesses and increase memory bandwidth utilization, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_SIZE) {\n        int k = kk + threadIdx.y;\n        if (k < K && threadIdx.x < TILE_SIZE && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[k * L + l];\n        }\n        __syncthreads();\n\n        if (n < N && m < M && l < L) {\n            for (int k = kk; k < min(kk + TILE_SIZE, K); ++k) {\n                value += A[n * M * K + m * K + k] * Bs[k - kk][threadIdx.y];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for matrix B, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        if (threadIdx.y == 0 && l < L) {\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                Bs[k][threadIdx.x] = B[(kk + k) * L + l];\n            }\n        }\n        __syncthreads();\n\n        if (n < N && m < M) {\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += A[n * M * K + m * K + (kk + k)] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": -129.11346435546875
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimized algorithm using shared memory for matrix B and loop unrolling to reduce global memory accesses and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        for (int k_base = 0; k_base < K; k_base += TILE_SIZE) {\n            int k = k_base + threadIdx.z;\n            if (k < K && threadIdx.x < TILE_SIZE && threadIdx.y == 0) {\n                Bs[threadIdx.x][threadIdx.z] = B[k * L + l];\n            }\n            __syncthreads();\n\n            for (int k_inner = 0; k_inner < TILE_SIZE && (k_base + k_inner) < K; k_inner += UNROLL_FACTOR) {\n                value += A[n * M * K + m * K + k_base + k_inner] * Bs[k_inner][threadIdx.z];\n                if (k_inner + 1 < TILE_SIZE && (k_base + k_inner + 1) < K) \n                    value += A[n * M * K + m * K + k_base + k_inner + 1] * Bs[k_inner + 1][threadIdx.z];\n                if (k_inner + 2 < TILE_SIZE && (k_base + k_inner + 2) < K) \n                    value += A[n * M * K + m * K + k_base + k_inner + 2] * Bs[k_inner + 2][threadIdx.z];\n                if (k_inner + 3 < TILE_SIZE && (k_base + k_inner + 3) < K) \n                    value += A[n * M * K + m * K + k_base + k_inner + 3] * Bs[k_inner + 3][threadIdx.z];\n            }\n            __syncthreads();\n        }\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the CUDA kernel by increasing parallelism through block tiling and shared memory utilization for better memory access patterns while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n    \n    for (int kk = 0; kk < K; kk += TILE_SIZE) {\n        if (n < N && m < M && (kk + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[n * M * K + m * K + (kk + threadIdx.x)];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((kk + threadIdx.y) < K && l < L) {\n            Bs[threadIdx.y][threadIdx.x] = B[(kk + threadIdx.y) * L + l];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE && (kk + k) < K; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the CUDA kernel by using shared memory to cache tiles of matrices A and B, and reorganize the thread blocks to better utilize memory coalescing and reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_SIZE) {\n        if (n < N && m < M && (kk + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[n * M * K + m * K + (kk + threadIdx.x)];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if ((kk + threadIdx.y) < K && l < L) {\n            Bs[threadIdx.y][threadIdx.x] = B[(kk + threadIdx.y) * L + l];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, L);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the CUDA kernel by using shared memory to cache tiles of matrices A and B, reducing global memory accesses, and adjusting block/grid dimensions for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int k = t * TILE_SIZE + threadIdx.z;\n        if (n < N && m < M && k < K) {\n            As[threadIdx.x][threadIdx.z] = A[n * M * K + m * K + k];\n        } else {\n            As[threadIdx.x][threadIdx.z] = 0.0f;\n        }\n\n        if (k < K && l < L) {\n            Bs[threadIdx.z][threadIdx.y] = B[k * L + l];\n        } else {\n            Bs[threadIdx.z][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k_tile = 0; k_tile < TILE_SIZE && (t * TILE_SIZE + k_tile) < K; ++k_tile) {\n            value += As[threadIdx.x][k_tile] * Bs[k_tile][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the CUDA kernel by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_SIZE) {\n        if (n < N && m < M && (kk + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[n * M * K + m * K + (kk + threadIdx.x)];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if ((kk + threadIdx.y) < K && l < L) {\n            Bs[threadIdx.y][threadIdx.x] = B[(kk + threadIdx.y) * L + l];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE && (kk + k) < K; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for matrix B, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += BLOCK_SIZE) {\n        if (threadIdx.x < BLOCK_SIZE && (kk + threadIdx.x) < K && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        }\n        __syncthreads();\n\n        for (int k = kk; k < min(kk + BLOCK_SIZE, K); k++) {\n            if (n < N && m < M) {\n                value += A[n * M * K + m * K + k] * Bs[k - kk][threadIdx.y];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": -98.39430084228516
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for matrix B to reduce global memory accesses, increasing thread block utilization, and unrolling the inner loop for better instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 16\n#define TILE_K 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int k = 0; k < K; k += TILE_K) {\n        if (threadIdx.y < TILE_K && (k + threadIdx.y) < K && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[(k + threadIdx.y) * L + l];\n        }\n        __syncthreads();\n\n        if (n < N && m < M) {\n            #pragma unroll\n            for (int tk = 0; tk < TILE_K && (k + tk) < K; ++tk) {\n                value += A[n * M * K + m * K + (k + tk)] * Bs[threadIdx.x][tk];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": -533.3207153320312
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for matrix B to reduce global memory accesses and increase thread block utilization with better grid and block dimensions.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int k_tile = 0; k_tile < K; k_tile += TILE_SIZE) {\n        int k = k_tile + threadIdx.y;\n        if (k < K && threadIdx.x < TILE_SIZE && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[k * L + l];\n        }\n        __syncthreads();\n\n        if (n < N && m < M) {\n            for (int kk = 0; kk < TILE_SIZE && (k_tile + kk) < K; ++kk) {\n                value += A[n * M * K + m * K + (k_tile + kk)] * Bs[kk][threadIdx.y];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the CUDA kernel by utilizing shared memory for matrix B, increasing thread block size, and unrolling the inner loop for better memory access patterns and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float B_tile[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        for (int k_base = 0; k_base < K; k_base += TILE_SIZE) {\n            int k_tile = k_base + threadIdx.x;\n            if (k_tile < K && threadIdx.y < TILE_SIZE) {\n                B_tile[threadIdx.y][threadIdx.x] = B[k_tile * L + l];\n            }\n            __syncthreads();\n\n            int k_end = min(k_base + TILE_SIZE, K);\n            for (int k = k_base; k < k_end; k += UNROLL_FACTOR) {\n                float a_val[UNROLL_FACTOR];\n                for (int i = 0; i < UNROLL_FACTOR && (k + i) < k_end; i++) {\n                    a_val[i] = A[n * M * K + m * K + (k + i)];\n                    value += a_val[i] * B_tile[threadIdx.y][(k + i - k_base)];\n                }\n            }\n            __syncthreads();\n        }\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the CUDA kernel by utilizing shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    extern __shared__ float shared_mem[];\n    float* A_shared = shared_mem;\n    float* B_shared = shared_mem + TILE_SIZE * TILE_SIZE;\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int k_tile = 0; k_tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++k_tile) {\n        int k = k_tile * TILE_SIZE + threadIdx.x;\n        if (n < N && m < M && k < K) {\n            A_shared[threadIdx.y * TILE_SIZE + threadIdx.x] = A[n * M * K + m * K + k];\n        } else {\n            A_shared[threadIdx.y * TILE_SIZE + threadIdx.x] = 0.0f;\n        }\n\n        k = k_tile * TILE_SIZE + threadIdx.y;\n        if (k < K && l < L) {\n            B_shared[threadIdx.y * TILE_SIZE + threadIdx.z] = B[k * L + l];\n        } else {\n            B_shared[threadIdx.y * TILE_SIZE + threadIdx.z] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += A_shared[threadIdx.y * TILE_SIZE + k] * B_shared[k * TILE_SIZE + threadIdx.z];\n        }\n\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    size_t shared_mem_size = 2 * TILE_SIZE * TILE_SIZE * sizeof(float);\n    matmul_kernel<<<grid, block, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for matrix B, increasing thread block size, and unrolling the inner loop for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        \n        for (int kk = 0; kk < K; kk += TILE_K) {\n            if (threadIdx.z == 0 && (kk + threadIdx.y) < K && l < L) {\n                Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.y) * L + l];\n            }\n            __syncthreads();\n\n            for (int k = kk; k < min(kk + TILE_K, K); ++k) {\n                value += A[n * M * K + m * K + k] * Bs[k - kk][threadIdx.y];\n            }\n            __syncthreads();\n        }\n        \n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the CUDA kernel by utilizing shared memory for matrix B to reduce global memory accesses and increase memory bandwidth utilization, while maintaining correctness through proper synchronization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int k_tile = 0; k_tile < K; k_tile += TILE_SIZE) {\n        int k = k_tile + threadIdx.y;\n        if (k < K && threadIdx.x < TILE_SIZE && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[k * L + l];\n        }\n        __syncthreads();\n\n        if (n < N && m < M && l < L) {\n            for (int kk = 0; kk < TILE_SIZE && (k_tile + kk) < K; ++kk) {\n                value += A[n * M * K + m * K + (k_tile + kk)] * Bs[threadIdx.y][kk];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": -202.07709655761718
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimized implementation using shared memory for matrix B to reduce global memory accesses and improved thread block configuration for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    extern __shared__ float shared_B[];\n    \n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n    \n    for (int tile = 0; tile < (K + TILE_SIZE - 1) / TILE_SIZE; ++tile) {\n        int k_tile = tile * TILE_SIZE;\n        int k = k_tile + threadIdx.z;\n        \n        if (k < K && threadIdx.x < TILE_SIZE && l < L) {\n            shared_B[threadIdx.x * TILE_SIZE + threadIdx.z] = B[k * L + l];\n        }\n        __syncthreads();\n        \n        if (n < N && m < M && l < L) {\n            for (int k_inner = 0; k_inner < TILE_SIZE && (k_tile + k_inner) < K; ++k_inner) {\n                value += A[n * M * K + m * K + k_tile + k_inner] * shared_B[k_inner * TILE_SIZE + threadIdx.z];\n            }\n        }\n        __syncthreads();\n    }\n    \n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, 1, TILE_SIZE);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    size_t shared_mem_size = TILE_SIZE * TILE_SIZE * sizeof(float);\n    matmul_kernel<<<grid, block, shared_mem_size>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for tiling matrix B, increasing thread block utilization, and unrolling the inner loop for better memory access patterns and reduced global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float B_tile[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n >= N || m >= M || l >= L) return;\n\n    float value = 0.0f;\n    for (int k_base = 0; k_base < K; k_base += TILE_SIZE) {\n        int k = k_base + threadIdx.z;\n        if (k < K && threadIdx.x < TILE_SIZE && l < L) {\n            B_tile[threadIdx.x][threadIdx.y] = B[k * L + l];\n        }\n        __syncthreads();\n\n        for (int k_tile = 0; k_tile < TILE_SIZE && (k_base + k_tile) < K; ++k_tile) {\n            value += A[n * M * K + m * K + (k_base + k_tile)] * B_tile[k_tile][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the CUDA kernel by using shared memory to cache tiles of matrices A and B, reducing global memory accesses and improving memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_SIZE) {\n        if (n < N && m < M && (kk + threadIdx.z) < K) {\n            As[threadIdx.y][threadIdx.z] = A[n * M * K + m * K + (kk + threadIdx.z)];\n        } else {\n            As[threadIdx.y][threadIdx.z] = 0.0f;\n        }\n\n        if ((kk + threadIdx.y) < K && l < L) {\n            Bs[threadIdx.y][threadIdx.z] = B[(kk + threadIdx.y) * L + l];\n        } else {\n            Bs[threadIdx.y][threadIdx.z] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.z];\n        }\n\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the CUDA kernel by using shared memory to cache tiles of matrices A and B, and increase thread block utilization with better grid and block dimensions.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_SIZE) {\n        if (n < N && m < M && (kk + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[n * M * K + m * K + (kk + threadIdx.x)];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if ((kk + threadIdx.y) < K && l < L) {\n            Bs[threadIdx.y][threadIdx.x] = B[(kk + threadIdx.y) * L + l];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE && (kk + k) < K; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, L);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the CUDA kernel by utilizing shared memory for matrix B to reduce global memory accesses, increasing thread block size for better occupancy, and unrolling the inner loop for improved instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        if (threadIdx.z == 0 && m < M && (kk + threadIdx.x) < K && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        }\n        __syncthreads();\n\n        if (n < N && m < M && l < L) {\n            #pragma unroll\n            for (int k = kk; k < min(kk + TILE_K, K); ++k) {\n                value += A[n * M * K + m * K + k] * Bs[k - kk][threadIdx.y];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": -138.04224853515626
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the CUDA kernel by using shared memory to cache tiles of matrices A and B, reducing global memory accesses, and adjusting block/grid dimensions for better GPU utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_SIZE) {\n        if (n < N && m < M && (kk + threadIdx.z) < K) {\n            As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.z)];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        if ((kk + threadIdx.x) < K && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for matrix B, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n    \n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        \n        for (int kk = 0; kk < K; kk += TILE_K) {\n            if (threadIdx.y < TILE_K && (kk + threadIdx.y) < K && l < L) {\n                Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.y) * L + l];\n            }\n            __syncthreads();\n            \n            #pragma unroll\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += A[n * M * K + m * K + (kk + k)] * Bs[threadIdx.x][k];\n            }\n            __syncthreads();\n        }\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": -129.18228454589843
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for matrix B, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        \n        for (int kk = 0; kk < K; kk += TILE_K) {\n            if (threadIdx.x < TILE_K && (kk + threadIdx.x) < K && l < L) {\n                Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n            }\n            __syncthreads();\n\n            for (int k = kk; k < kk + TILE_K && k < K; ++k) {\n                value += A[n * M * K + m * K + k] * Bs[k - kk][threadIdx.y];\n            }\n            __syncthreads();\n        }\n        \n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": -135.16038513183594
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the CUDA kernel by using a warp-level approach with cooperative loading of matrix B into registers, reducing shared memory usage and improving memory access patterns through vectorized loads.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 4\n#define WARPS_PER_BLOCK 2\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    int warp_id = threadIdx.y / 32;\n    int lane_id = threadIdx.y % 32;\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n    float B_reg[TILE_K];\n\n    for (int kk = 0; kk < K; kk += TILE_K * WARPS_PER_BLOCK) {\n        if (n < N && m < M && l < L) {\n            #pragma unroll\n            for (int k = 0; k < TILE_K; ++k) {\n                int k_idx = kk + warp_id * TILE_K + k;\n                if (k_idx < K) {\n                    B_reg[k] = B[k_idx * L + l];\n                }\n            }\n\n            #pragma unroll\n            for (int k = 0; k < TILE_K; ++k) {\n                int k_idx = kk + warp_id * TILE_K + k;\n                if (k_idx < K) {\n                    value += A[n * M * K + m * K + k_idx] * B_reg[k];\n                }\n            }\n        }\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the CUDA kernel by using a warp-level approach with register tiling for matrix B and vectorized memory access to maximize memory throughput and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n#define TILE_K 4\n#define VEC_SIZE 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int n = blockIdx.x * blockDim.x / WARP_SIZE + warp_id;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float4 B_reg[TILE_K];\n    float accum = 0.0f;\n\n    if (n < N && m < M && l < L) {\n        for (int k_base = 0; k_base < K; k_base += WARP_SIZE * TILE_K) {\n            int k = k_base + lane_id * TILE_K;\n            #pragma unroll\n            for (int tk = 0; tk < TILE_K; ++tk) {\n                if (k + tk < K) {\n                    B_reg[tk] = *reinterpret_cast<const float4*>(&B[(k + tk) * L + l]);\n                }\n            }\n\n            #pragma unroll\n            for (int tk = 0; tk < TILE_K; ++tk) {\n                if (k + tk < K) {\n                    accum += A[n * M * K + m * K + (k + tk)] * B_reg[tk].x;\n                }\n            }\n        }\n        C[n * M * L + m * L + l] = accum;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(WARP_SIZE * WARPS_PER_BLOCK, 1, 1);\n    dim3 grid((N * WARPS_PER_BLOCK + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the CUDA kernel by using a warp-level approach with cooperative loading of both A and B matrices into shared memory, leveraging tensor cores for mixed-precision computation while maintaining final accumulation in full precision for accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ __half As[BLOCK_SIZE][TILE_K];\n    __shared__ __half Bs[TILE_K][BLOCK_SIZE];\n\n    int warp_id = threadIdx.x / 32;\n    int lane_id = threadIdx.x % 32;\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        // Cooperative loading of A and B tiles\n        if (n < N && m < M && (kk + lane_id) < K) {\n            As[threadIdx.y][lane_id] = __float2half(A[n * M * K + m * K + kk + lane_id]);\n        }\n        if ((kk + warp_id) < K && l < L && threadIdx.y < BLOCK_SIZE) {\n            Bs[warp_id][threadIdx.y] = __float2half(B[(kk + warp_id) * L + l]);\n        }\n        __syncthreads();\n\n        if (n < N && m < M && l < L) {\n            __half2 acc = __float2half2_rn(0.0f);\n            for (int k = 0; k < TILE_K; k += 2) {\n                __half2 a = *reinterpret_cast<__half2*>(&As[threadIdx.y][k]);\n                __half2 b = *reinterpret_cast<__half2*>(&Bs[k][threadIdx.x % BLOCK_SIZE]);\n                acc = __hfma2(a, b, acc);\n            }\n            value += __half2float(__hadd(__low2half(acc), __high2half(acc)));\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, implementing a tiled matrix multiplication approach with warp-level operations, and leveraging register-level parallelism for better performance.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int warp_id = threadIdx.y;\n    int lane_id = threadIdx.x;\n    int n = blockIdx.x * TILE_SIZE + threadIdx.y;\n    int m = blockIdx.y * TILE_SIZE + threadIdx.x;\n    int l = blockIdx.z;\n\n    float accum = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_SIZE) {\n        if (n < N && (kk + lane_id) < K) {\n            As[warp_id][lane_id] = A[n * M * K + m * K + (kk + lane_id)];\n        } else {\n            As[warp_id][lane_id] = 0.0f;\n        }\n\n        if ((kk + warp_id) < K && l < L) {\n            Bs[warp_id][lane_id] = B[(kk + warp_id) * L + l];\n        } else {\n            Bs[warp_id][lane_id] = 0.0f;\n        }\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            accum += As[warp_id][k] * Bs[k][lane_id];\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = accum;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, L);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, increasing thread block utilization, and implementing a more efficient tiling strategy with loop unrolling.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 16\n#define TILE_K 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        if (n < N && m < M && (kk + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n        }\n        if ((kk + threadIdx.x) < K && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        }\n        __syncthreads();\n\n        if (n < N && m < M) {\n            #pragma unroll\n            for (int k = 0; k < TILE_K; ++k) {\n                value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the CUDA kernel by transposing matrix B for better memory coalescing, using shared memory for both A and B tiles, and increasing thread block size while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n    \n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        \n        for (int kk = 0; kk < K; kk += TILE_K) {\n            // Load A tile into shared memory\n            if (threadIdx.y < TILE_K && (kk + threadIdx.y) < K) {\n                As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n            }\n            \n            // Load B tile into shared memory (transposed)\n            if (threadIdx.x < TILE_K && (kk + threadIdx.x) < K) {\n                Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n            }\n            __syncthreads();\n            \n            #pragma unroll\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n            }\n            __syncthreads();\n        }\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the CUDA kernel by transposing matrix B for better memory coalescing, using shared memory for both A and B tiles, and implementing a 2D block-stride loop for increased parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        if (n < N && m < M && (kk + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n        }\n        if (l < L && (kk + threadIdx.x) < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        }\n        __syncthreads();\n\n        if (n < N && m < M && l < L) {\n            #pragma unroll\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, implementing a tiled matrix multiplication approach with loop unrolling, and increasing thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += BLOCK_SIZE) {\n        if (n < N && m < M && (kk + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n        }\n        if ((kk + threadIdx.x) < K && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        }\n        __syncthreads();\n\n        for (int k = 0; k < BLOCK_SIZE; k++) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, increasing thread block size, and employing loop unrolling with register tiling to maximize memory throughput and reduce bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        \n        for (int kk = 0; kk < K; kk += TILE_K) {\n            if (threadIdx.y < TILE_K && (kk + threadIdx.y) < K) {\n                As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n            }\n            if (threadIdx.x < TILE_K && (kk + threadIdx.x) < K) {\n                Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n            }\n            __syncthreads();\n\n            #pragma unroll\n            for (int k = 0; k < TILE_K; ++k) {\n                value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n            }\n            __syncthreads();\n        }\n        \n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, increasing thread block size, and implementing loop unrolling to improve memory access patterns and reduce bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        \n        for (int kk = 0; kk < K; kk += TILE_K) {\n            if (threadIdx.y < TILE_K && (kk + threadIdx.y) < K) {\n                As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n            }\n            if (threadIdx.x < TILE_K && (kk + threadIdx.x) < K) {\n                Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n            }\n            __syncthreads();\n\n            #pragma unroll\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n            }\n            __syncthreads();\n        }\n        \n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, increasing thread block size, and employing loop unrolling with register tiling to maximize memory efficiency and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        \n        for (int kk = 0; kk < K; kk += TILE_K) {\n            if (threadIdx.y < TILE_K && (kk + threadIdx.y) < K && n < N && m < M) {\n                As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n            }\n            if (threadIdx.x < TILE_K && (kk + threadIdx.x) < K && l < L) {\n                Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n            }\n            __syncthreads();\n\n            #pragma unroll\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n            }\n            __syncthreads();\n        }\n        \n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, increasing thread block size, and implementing loop unrolling with thread coarsening to improve memory access patterns and reduce global memory bandwidth.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += BLOCK_SIZE) {\n        if (n < N && m < M && (kk + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n        }\n        if ((kk + threadIdx.x) < K && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        }\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < BLOCK_SIZE && (kk + k) < K; k++) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, increasing thread block size to 64, and using loop unrolling with a tile size of 16 to improve memory access patterns and arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 64\n#define TILE_K 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        if (n < N && m < M) {\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                As[threadIdx.y][k] = A[n * M * K + m * K + (kk + k)];\n            }\n        }\n        \n        if (threadIdx.y < TILE_K && l < L) {\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                Bs[k][threadIdx.x] = B[(kk + k) * L + l];\n            }\n        }\n        __syncthreads();\n\n        if (n < N && m < M) {\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the CUDA kernel by increasing thread block size to 64x64 for better GPU utilization, using shared memory for both A and B matrices to reduce global memory accesses, and implementing loop unrolling with a tile size of 16 for improved instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 64\n#define TILE_K 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        if (n < N && m < M && (kk + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n        }\n        if ((kk + threadIdx.x) < K && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        }\n        __syncthreads();\n\n        if (n < N && m < M && l < L) {\n            #pragma unroll\n            for (int k = 0; k < TILE_K; ++k) {\n                value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, increasing thread block size to 64x64, and unrolling the inner loop with a tile size of 16 to maximize memory bandwidth utilization and reduce global memory accesses.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 64\n#define TILE_K 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n    \n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        \n        for (int kk = 0; kk < K; kk += TILE_K) {\n            if (threadIdx.x < TILE_K && (kk + threadIdx.x) < K) {\n                As[threadIdx.y][threadIdx.x] = A[n * M * K + m * K + (kk + threadIdx.x)];\n            }\n            if (threadIdx.y < TILE_K && (kk + threadIdx.y) < K) {\n                Bs[threadIdx.y][threadIdx.z] = B[(kk + threadIdx.y) * L + l];\n            }\n            __syncthreads();\n            \n            #pragma unroll\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += As[threadIdx.y][k] * Bs[k][threadIdx.z];\n            }\n            __syncthreads();\n        }\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, increasing thread block size to 64, and using loop unrolling with a tile size of 16 to improve memory access patterns and reduce bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 64\n#define TILE_K 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n    \n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    if (n < N && m < M && l < L) {\n        float value = 0.0f;\n        \n        for (int kk = 0; kk < K; kk += TILE_K) {\n            if (threadIdx.x < TILE_K && (kk + threadIdx.x) < K) {\n                As[threadIdx.y][threadIdx.x] = A[n * M * K + m * K + (kk + threadIdx.x)];\n            }\n            if (threadIdx.y < TILE_K && (kk + threadIdx.y) < K) {\n                Bs[threadIdx.y][threadIdx.z] = B[(kk + threadIdx.y) * L + l];\n            }\n            __syncthreads();\n            \n            #pragma unroll\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += As[threadIdx.y][k] * Bs[k][threadIdx.z];\n            }\n            __syncthreads();\n        }\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both input matrices A and B, tiling the computation across all three dimensions (N, M, L), and utilizing register accumulation with loop unrolling for better performance.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_N 16\n#define TILE_M 16\n#define TILE_L 16\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[TILE_N][TILE_K];\n    __shared__ float Bs[TILE_K][TILE_L];\n\n    int tn = threadIdx.x;\n    int tm = threadIdx.y;\n    int tl = threadIdx.z;\n\n    int n_base = blockIdx.x * TILE_N;\n    int m_base = blockIdx.y * TILE_M;\n    int l_base = blockIdx.z * TILE_L;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        // Load A tile\n        if (n_base + tn < N && m_base + tm < M && kk + tl < K) {\n            As[tn][tl] = A[(n_base + tn) * M * K + (m_base + tm) * K + (kk + tl)];\n        } else {\n            As[tn][tl] = 0.0f;\n        }\n\n        // Load B tile\n        if (kk + tn < K && l_base + tm < L) {\n            Bs[tn][tm] = B[(kk + tn) * L + (l_base + tm)];\n        } else {\n            Bs[tn][tm] = 0.0f;\n        }\n        __syncthreads();\n\n        // Compute partial result\n        for (int k = 0; k < TILE_K; k++) {\n            value += As[tn][k] * Bs[k][tl];\n        }\n        __syncthreads();\n    }\n\n    // Store result\n    if (n_base + tn < N && m_base + tm < M && l_base + tl < L) {\n        C[(n_base + tn) * M * L + (m_base + tm) * L + (l_base + tl)] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_N, TILE_M, TILE_L);\n    dim3 grid((N + TILE_N - 1) / TILE_N, \n              (M + TILE_M - 1) / TILE_M, \n              (L + TILE_L - 1) / TILE_L);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both input matrices A and B, implementing a tiled matrix multiplication approach with register accumulation, and carefully selecting thread block dimensions to maximize occupancy and memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 16\n#define TILE_K 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int bz = blockIdx.z;\n    \n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    \n    int n = bx * blockDim.x + tx;\n    int m = by * blockDim.y + ty;\n    int l = bz * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_SIZE) {\n        if (n < N && m < M && (kk + ty) < K) {\n            As[tx][ty] = A[n * M * K + m * K + (kk + ty)];\n        } else {\n            As[tx][ty] = 0.0f;\n        }\n\n        if ((kk + tx) < K && l < L) {\n            Bs[tx][ty] = B[(kk + tx) * L + l];\n        } else {\n            Bs[tx][ty] = 0.0f;\n        }\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += As[tx][k] * Bs[k][ty];\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(TILE_SIZE, TILE_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the CUDA kernel by using a warp-level approach with cooperative fetching of matrix B into shared memory, vectorized memory accesses, and loop unrolling for improved memory efficiency and instruction throughput.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 4\n#define VEC_SIZE 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int warp_id = threadIdx.x / 32;\n    int lane_id = threadIdx.x % 32;\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n    float4 a_vec, b_vec;\n\n    for (int kk = 0; kk < K; kk += BLOCK_SIZE) {\n        if (lane_id < BLOCK_SIZE && (kk + lane_id) < K && l < L) {\n            Bs[lane_id][threadIdx.y] = B[(kk + lane_id) * L + l];\n        }\n        __syncwarp();\n\n        for (int k = kk; k < min(kk + BLOCK_SIZE, K); k += TILE_K) {\n            if (n < N && m < M) {\n                #pragma unroll\n                for (int t = 0; t < TILE_K; t++) {\n                    if (k + t < K) {\n                        value += A[n * M * K + m * K + k + t] * Bs[k + t - kk][threadIdx.y];\n                    }\n                }\n            }\n        }\n        __syncwarp();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the CUDA kernel by tiling both A and B matrices in shared memory, using warp-level matrix operations, and employing register blocking to maximize memory efficiency and arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        if (n < N && m < M && (kk + lane_id) < K) {\n            As[threadIdx.y][lane_id] = A[n * M * K + m * K + (kk + lane_id)];\n        }\n        if ((kk + threadIdx.y) < K && l < L) {\n            Bs[threadIdx.y][threadIdx.x] = B[(kk + threadIdx.y) * L + l];\n        }\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_K; ++k) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, processing multiple elements per thread, and leveraging warp-level operations for better utilization of the RTX 4090's architecture.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 4\n#define TILE_N 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int warp_id = threadIdx.x / 32;\n    int lane_id = threadIdx.x % 32;\n    int n = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n    int m = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    int l = blockIdx.z * BLOCK_SIZE + warp_id;\n\n    float accum[TILE_N] = {0.0f};\n\n    for (int kk = 0; kk < K; kk += BLOCK_SIZE) {\n        for (int t = 0; t < TILE_N; t++) {\n            int n_t = n + t * BLOCK_SIZE;\n            if (n_t < N && m < M && (kk + lane_id) < K) {\n                As[threadIdx.y][lane_id + t * BLOCK_SIZE] = A[n_t * M * K + m * K + (kk + lane_id)];\n            }\n        }\n        if (l < L && (kk + threadIdx.y) < K) {\n            Bs[threadIdx.y][warp_id] = B[(kk + threadIdx.y) * L + l];\n        }\n        __syncthreads();\n\n        for (int k = 0; k < BLOCK_SIZE && (kk + k) < K; k++) {\n            for (int t = 0; t < TILE_N; t++) {\n                if (n + t * BLOCK_SIZE < N && m < M) {\n                    accum[t] += As[threadIdx.y][k + t * BLOCK_SIZE] * Bs[k][warp_id];\n                }\n            }\n        }\n        __syncthreads();\n    }\n\n    for (int t = 0; t < TILE_N; t++) {\n        int n_t = n + t * BLOCK_SIZE;\n        if (n_t < N && m < M && l < L) {\n            C[n_t * M * L + m * L + l] = accum[t];\n        }\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x * TILE_N - 1) / (block.x * TILE_N), \n              (M + block.y - 1) / block.y, \n              (L + 8 - 1) / 8);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both matrices A and B, processing in 2D tiles, and leveraging register accumulation with loop unrolling for better memory efficiency and computation throughput.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n\n    int n = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n    int m = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n    int l = blockIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        // Load tile of A into shared memory\n        if (n < N && m < M && (kk + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n        } else {\n            As[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        // Load tile of B into shared memory\n        if ((kk + threadIdx.x) < K && l < L && threadIdx.y < BLOCK_SIZE) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_K; ++k) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, L);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the CUDA kernel by transposing matrix B for better memory coalescing, using shared memory for both A and B tiles, and increasing thread block size for better occupancy while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][TILE_K];\n    __shared__ float Bs[TILE_K][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += TILE_K) {\n        if (n < N && m < M) {\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                As[threadIdx.y][k] = A[n * M * K + m * K + (kk + k)];\n            }\n        }\n\n        if (threadIdx.y == 0 && l < L) {\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                Bs[k][threadIdx.x] = B[(kk + k) * L + l];\n            }\n        }\n        __syncthreads();\n\n        if (n < N && m < M && l < L) {\n            for (int k = 0; k < TILE_K && (kk + k) < K; ++k) {\n                value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            }\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for both input matrices A and B, processing tiles of both matrices, and using warp-level optimizations to reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int N, int M, int K, int L) {\n    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n    int n = blockIdx.x * blockDim.x + threadIdx.x;\n    int m = blockIdx.y * blockDim.y + threadIdx.y;\n    int l = blockIdx.z * blockDim.z + threadIdx.z;\n\n    float value = 0.0f;\n\n    for (int kk = 0; kk < K; kk += BLOCK_SIZE) {\n        if (n < N && m < M && (kk + threadIdx.y) < K) {\n            As[threadIdx.x][threadIdx.y] = A[n * M * K + m * K + (kk + threadIdx.y)];\n        }\n        if ((kk + threadIdx.x) < K && l < L) {\n            Bs[threadIdx.x][threadIdx.y] = B[(kk + threadIdx.x) * L + l];\n        }\n        __syncthreads();\n\n        for (int k = 0; k < BLOCK_SIZE && (kk + k) < K; k++) {\n            value += As[threadIdx.x][k] * Bs[k][threadIdx.y];\n        }\n        __syncthreads();\n    }\n\n    if (n < N && m < M && l < L) {\n        C[n * M * L + m * L + l] = value;\n    }\n}\n\nat::Tensor matmul_cuda(at::Tensor A, at::Tensor B) {\n    int N = A.size(0);\n    int M = A.size(1);\n    int K = A.size(2);\n    int L = B.size(1);\n\n    auto C = torch::zeros({N, M, L}, A.options());\n\n    dim3 block(BLOCK_SIZE, BLOCK_SIZE, 1);\n    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y, (L + block.z - 1) / block.z);\n\n    matmul_kernel<<<grid, block>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), N, M, K, L);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"3D tensor-matrix multiplication (CUDA)\");\n}",
        "score": null
    }
]