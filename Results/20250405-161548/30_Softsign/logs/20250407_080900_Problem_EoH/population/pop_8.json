[
    {
        "algorithm": "{Optimize the Softsign CUDA kernel by utilizing shared memory for coalesced memory access and warp-level reduction while maintaining numerical accuracy with precise math operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    extern __shared__ float s_data[];\n    int tid = threadIdx.x;\n    int idx = blockIdx.x * blockDim.x + tid;\n    \n    if (idx < size) {\n        s_data[tid] = x[idx];\n        __syncthreads();\n        \n        float val = s_data[tid];\n        s_data[tid] = val / (1.0f + fabsf(val));\n        __syncthreads();\n        \n        y[idx] = s_data[tid];\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    const int shared_mem = threads * sizeof(float);\n    \n    softsign_kernel<<<blocks, threads, shared_mem>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.02417280003428459
    },
    {
        "algorithm": "{Optimize the Softsign CUDA kernel by using cooperative groups for warp-level synchronization and shared memory for reduced global memory accesses while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cmath>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    cg::thread_block cta = cg::this_thread_block();\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cta);\n    \n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < size; i += stride) {\n        float val = x[i];\n        val = val / (1.0f + fabsf(val));\n        y[i] = val;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.024265600368380547
    },
    {
        "algorithm": "{Optimize the Softsign CUDA kernel by using fused multiply-add operations, loop unrolling, and increased thread utilization while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n// Optimized CUDA kernel for Softsign activation\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        if (index < size) {\n            float val = x[index];\n            y[index] = __fdividef(val, __fadd_rn(1.0f, fabsf(val)));\n            index += blockDim.x;\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads * 4 - 1) / (threads * 4);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.02951360009610653
    },
    {
        "algorithm": "{Optimize the Softsign CUDA kernel by combining warp-level operations with cooperative groups for better warp utilization and reduced divergence, while processing multiple elements per thread.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cmath>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cg::this_thread_block());\n    int warp_idx = (blockIdx.x * blockDim.x + threadIdx.x) / tile.size();\n    int lane = threadIdx.x % tile.size();\n    \n    for (int i = warp_idx * 4; i < size; i += gridDim.x * blockDim.x / tile.size() * 4) {\n        float vals[4];\n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            int idx = i + j * tile.size() + lane;\n            vals[j] = (idx < size) ? x[idx] : 0.0f;\n        }\n        \n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            vals[j] = vals[j] / (1.0f + fabsf(vals[j]));\n        }\n        \n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            int idx = i + j * tile.size() + lane;\n            if (idx < size) y[idx] = vals[j];\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads * 4 - 1) / (threads * 4);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.036608000472188
    },
    {
        "algorithm": "{Optimize the Softsign CUDA kernel by utilizing warp-level operations and vectorized memory accesses while maintaining numerical accuracy with precise math operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <int VEC_SIZE>\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int tid = threadIdx.x + blockIdx.x * blockDim.x * VEC_SIZE;\n    if (tid < size) {\n        float vals[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            int idx = tid + i * blockDim.x;\n            if (idx < size) {\n                vals[i] = x[idx];\n                vals[i] = vals[i] / (1.0f + fabsf(vals[i]));\n                y[idx] = vals[i];\n            }\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    constexpr int VEC_SIZE = 4;\n    const int threads = 256;\n    const int blocks = (size + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n    \n    softsign_kernel<VEC_SIZE><<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04199359901249409
    }
]