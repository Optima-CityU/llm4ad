[
    {
        "algorithm": "{Optimize the Softsign CUDA kernel by using fused multiply-add operations, loop unrolling, and increased thread utilization while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n// Optimized CUDA kernel for Softsign activation\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        if (index < size) {\n            float val = x[index];\n            y[index] = __fdividef(val, __fadd_rn(1.0f, fabsf(val)));\n            index += blockDim.x;\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads * 4 - 1) / (threads * 4);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.02951360009610653
    },
    {
        "algorithm": "{Optimize the Softsign CUDA kernel by using fast math operations, increasing occupancy with optimal block size, and minimizing branch divergence with aligned memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < size) {\n        const float val = x[index];\n        y[index] = __fdividef(val, 1.0f + __fsqrt_rn(val * val));\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04308159984648228
    },
    {
        "algorithm": "{Optimize the Softsign activation kernel by using fast math operations while maintaining numerical correctness, and ensure proper memory coalescing with optimal block and thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < size) {\n        float val = x[index];\n        float abs_val = val < 0.0f ? -val : val;  // Branchless absolute value\n        y[index] = __fdividef(val, 1.0f + abs_val);  // Fast division\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;  // Better occupancy for Ampere\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads, 0, cudaStreamPerThread>>>(\n        x.data_ptr<float>(), y.data_ptr<float>(), size);\n    \n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04403839968144894
    },
    {
        "algorithm": "{Optimize the Softsign activation kernel by using vectorized memory accesses (float4) and increasing occupancy with proper block/grid sizing while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n    if (index + 3 < size) {\n        float4 val = reinterpret_cast<const float4*>(x)[index/4];\n        float4 result;\n        result.x = val.x / (1.0f + fabsf(val.x));\n        result.y = val.y / (1.0f + fabsf(val.y));\n        result.z = val.z / (1.0f + fabsf(val.z));\n        result.w = val.w / (1.0f + fabsf(val.w));\n        reinterpret_cast<float4*>(y)[index/4] = result;\n    } else {\n        for (int i = 0; i < 4 && index + i < size; i++) {\n            float val = x[index + i];\n            y[index + i] = val / (1.0f + fabsf(val));\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + 4 * threads - 1) / (4 * threads);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.048342400789260866
    },
    {
        "algorithm": "{Optimize the Softsign activation kernel by using vectorized memory access and loop unrolling to increase memory throughput while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n// Optimized CUDA kernel for Softsign activation\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int index = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n    if (index + 3 < size) {\n        float4 val = reinterpret_cast<const float4*>(x)[index/4];\n        float4 result;\n        result.x = val.x / (1.0f + fabsf(val.x));\n        result.y = val.y / (1.0f + fabsf(val.y));\n        result.z = val.z / (1.0f + fabsf(val.z));\n        result.w = val.w / (1.0f + fabsf(val.w));\n        reinterpret_cast<float4*>(y)[index/4] = result;\n    } else {\n        // Handle remaining elements\n        for (int i = 0; i < 4 && index + i < size; i++) {\n            float val = x[index + i];\n            y[index + i] = val / (1.0f + fabsf(val));\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;  // Reduced due to vectorization\n    const int blocks = (size + 4 * threads - 1) / (4 * threads);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.051120000332593916
    }
]