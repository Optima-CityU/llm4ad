[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the Softsign activation kernel by using fused multiply-add operations and minimizing memory accesses while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < size) {\n        float val = x[index];\n        float abs_val = fabsf(val);\n        y[index] = __fdividef(val, 1.0f + abs_val);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads, 0, cudaStreamTailLaunch>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    \n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the Softsign activation kernel by using fused multiply-add operations and improved memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < size) {\n        float val = x[index];\n        float abs_val = fabsf(val);\n        y[index] = __fdividef(val, 1.0f + abs_val);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads, 0, c10::cuda::getCurrentCUDAStream()>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    \n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the Softsign CUDA kernel by using fused multiply-add operations, loop unrolling, and increased thread utilization while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n// Optimized CUDA kernel for Softsign activation\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        if (index < size) {\n            float val = x[index];\n            y[index] = __fdividef(val, __fadd_rn(1.0f, fabsf(val)));\n            index += blockDim.x;\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads * 4 - 1) / (threads * 4);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.02951360009610653
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the Softsign CUDA kernel by using fast math operations, loop unrolling, and increased parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n// Optimized CUDA kernel for Softsign activation\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        if (index < size) {\n            float val = x[index];\n            y[index] = __fdividef(val, 1.0f + __fsqrt_rn(val * val));\n            index += blockDim.x;\n        }\n    }\n}\n\n// C++ interface function to launch the CUDA kernel\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads * 4 - 1) / (threads * 4);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    \n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.05299199931323528
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the Softsign activation kernel by using vectorized memory accesses (float4) and increasing occupancy with proper block/grid sizing while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n    if (index + 3 < size) {\n        float4 val = reinterpret_cast<const float4*>(x)[index/4];\n        float4 result;\n        result.x = val.x / (1.0f + fabsf(val.x));\n        result.y = val.y / (1.0f + fabsf(val.y));\n        result.z = val.z / (1.0f + fabsf(val.z));\n        result.w = val.w / (1.0f + fabsf(val.w));\n        reinterpret_cast<float4*>(y)[index/4] = result;\n    } else {\n        for (int i = 0; i < 4 && index + i < size; i++) {\n            float val = x[index + i];\n            y[index + i] = val / (1.0f + fabsf(val));\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + 4 * threads - 1) / (4 * threads);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.048342400789260866
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the Softsign activation kernel by using vectorized memory access and loop unrolling to increase memory throughput while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n// Optimized CUDA kernel for Softsign activation\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int index = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n    if (index + 3 < size) {\n        float4 val = reinterpret_cast<const float4*>(x)[index/4];\n        float4 result;\n        result.x = val.x / (1.0f + fabsf(val.x));\n        result.y = val.y / (1.0f + fabsf(val.y));\n        result.z = val.z / (1.0f + fabsf(val.z));\n        result.w = val.w / (1.0f + fabsf(val.w));\n        reinterpret_cast<float4*>(y)[index/4] = result;\n    } else {\n        // Handle remaining elements\n        for (int i = 0; i < 4 && index + i < size; i++) {\n            float val = x[index + i];\n            y[index + i] = val / (1.0f + fabsf(val));\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;  // Reduced due to vectorization\n    const int blocks = (size + 4 * threads - 1) / (4 * threads);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.051120000332593916
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the Softsign CUDA kernel by using fast math operations, increasing occupancy with optimal block size, and minimizing branch divergence with aligned memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < size) {\n        const float val = x[index];\n        y[index] = __fdividef(val, 1.0f + __fsqrt_rn(val * val));\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04308159984648228
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the Softsign kernel by using fast math operations, loop unrolling, and increased thread utilization while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n#define UNROLL_FACTOR 4\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x * UNROLL_FACTOR + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < UNROLL_FACTOR; i++) {\n        int current_idx = index + i * blockDim.x;\n        if (current_idx < size) {\n            float val = x[current_idx];\n            y[current_idx] = __fdividef(val, 1.0f + fabsf(val));\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads * UNROLL_FACTOR - 1) / (threads * UNROLL_FACTOR);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    \n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.0512192003428936
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the Softsign activation kernel by using fast math operations while maintaining numerical correctness, and ensure proper memory coalescing with optimal block and thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < size) {\n        float val = x[index];\n        float abs_val = val < 0.0f ? -val : val;  // Branchless absolute value\n        y[index] = __fdividef(val, 1.0f + abs_val);  // Fast division\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;  // Better occupancy for Ampere\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads, 0, cudaStreamPerThread>>>(\n        x.data_ptr<float>(), y.data_ptr<float>(), size);\n    \n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04403839968144894
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the Softsign kernel by using faster hardware intrinsics for absolute value and division, and ensure proper memory coalescing with aligned memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <mma.h>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < size) {\n        float val = __ldg(&x[index]);\n        y[index] = __fdividef(val, __fadd_rn(1.0f, __fabsf(val)));\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x, torch::MemoryFormat::Contiguous);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    \n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the Softsign activation kernel by using warp-level primitives and shared memory to reduce memory transactions while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    extern __shared__ float smem[];\n    const int tid = threadIdx.x;\n    const int warp_id = tid / 32;\n    const int lane_id = tid % 32;\n    const int warp_offset = warp_id * 32;\n    \n    for (int idx = blockIdx.x * blockDim.x + tid; idx < size; idx += blockDim.x * gridDim.x) {\n        float val = x[idx];\n        float res = val / (1.0f + fabsf(val));\n        y[idx] = res;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (size + threads - 1) / threads);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the Softsign kernel by using warp-level primitives for cooperative computation and shared memory to reduce global memory accesses while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int tid = threadIdx.x;\n    const int idx = blockIdx.x * blockDim.x + tid;\n    \n    __shared__ float smem[32];\n    const int warp_id = tid / 32;\n    const int lane_id = tid % 32;\n    \n    if (idx < size) {\n        float val = x[idx];\n        float res = val / (1.0f + fabsf(val));\n        \n        if (lane_id == 0) {\n            smem[warp_id] = res;\n        }\n        __syncthreads();\n        \n        if (warp_id == 0) {\n            y[idx] = smem[lane_id];\n        } else {\n            y[idx] = res;\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the Softsign activation kernel by using warp-level primitives and cooperative groups to reduce memory transactions while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cmath>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cg::this_thread_block());\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int warp_idx = idx / 32;\n    int lane = threadIdx.x % 32;\n    \n    for (int i = warp_idx * 32; i < size; i += blockDim.x * gridDim.x / 32 * 32) {\n        int pos = i + lane;\n        if (pos < size) {\n            float val = x[pos];\n            y[pos] = val / (1.0f + fabsf(val));\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the Softsign CUDA kernel using warp-level primitives for reduced divergence, cooperative groups for synchronization, and vectorized memory access for improved bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cuda_fp16.h>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    cg::thread_block tb = cg::this_thread_block();\n    int index = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    \n    float4 vals = *reinterpret_cast<const float4*>(&x[index]);\n    float4 results;\n    \n    results.x = vals.x / (1.0f + fabsf(vals.x));\n    results.y = vals.y / (1.0f + fabsf(vals.y));\n    results.z = vals.z / (1.0f + fabsf(vals.z));\n    results.w = vals.w / (1.0f + fabsf(vals.w));\n    \n    *reinterpret_cast<float4*>(&y[index]) = results;\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int elements_per_thread = 4;\n    const int blocks = (size + threads * elements_per_thread - 1) / (threads * elements_per_thread);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the Softsign CUDA kernel by using warp-level parallelism, shared memory for coalesced reads, and maintaining numerical accuracy with precise math operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    extern __shared__ float shared_x[];\n    const int tid = threadIdx.x;\n    const int warp_id = tid / 32;\n    const int lane_id = tid % 32;\n    const int global_idx = blockIdx.x * blockDim.x + tid;\n    \n    if (global_idx < size) {\n        shared_x[tid] = x[global_idx];\n    }\n    __syncthreads();\n    \n    if (global_idx < size) {\n        float val = shared_x[tid];\n        y[global_idx] = val / (1.0f + fabsf(val));\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    size_t shared_mem = threads * sizeof(float);\n    \n    softsign_kernel<<<blocks, threads, shared_mem>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.043110399320721624
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the Softsign CUDA kernel by combining warp-level operations with cooperative groups for better warp utilization and reduced divergence, while processing multiple elements per thread.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cmath>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cg::this_thread_block());\n    int warp_idx = (blockIdx.x * blockDim.x + threadIdx.x) / tile.size();\n    int lane = threadIdx.x % tile.size();\n    \n    for (int i = warp_idx * 4; i < size; i += gridDim.x * blockDim.x / tile.size() * 4) {\n        float vals[4];\n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            int idx = i + j * tile.size() + lane;\n            vals[j] = (idx < size) ? x[idx] : 0.0f;\n        }\n        \n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            vals[j] = vals[j] / (1.0f + fabsf(vals[j]));\n        }\n        \n        #pragma unroll\n        for (int j = 0; j < 4; ++j) {\n            int idx = i + j * tile.size() + lane;\n            if (idx < size) y[idx] = vals[j];\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads * 4 - 1) / (threads * 4);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.036608000472188
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the Softsign kernel by combining warp-level operations and shared memory to reduce redundant computations and improve memory access efficiency.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n#define WARP_SIZE 32\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int tid = threadIdx.x;\n    const int idx = blockIdx.x * blockDim.x + tid;\n    \n    __shared__ float smem[WARP_SIZE];\n    \n    if (idx < size) {\n        float val = x[idx];\n        float abs_val = fabsf(val);\n        smem[tid] = abs_val;\n        __syncthreads();\n        \n        // Warp-level reduction for coalesced memory access\n        for (int i = 0; i < WARP_SIZE; i++) {\n            float other_val = smem[i];\n            y[idx] = val / (1.0f + other_val);\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the Softsign activation kernel by using vectorized memory access and warp-level primitives for improved memory throughput, while maintaining numerical correctness with branchless operations and fast math.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <int VEC_SIZE>\n__global__ void softsign_kernel_vec(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    if (index < size) {\n        float vals[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (index + i < size) {\n                float val = x[index + i];\n                float abs_val = val < 0.0f ? -val : val;\n                vals[i] = __fdividef(val, 1.0f + abs_val);\n            }\n        }\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            if (index + i < size) {\n                y[index + i] = vals[i];\n            }\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int vec_size = 4;\n    const int threads = 256;\n    const int blocks = (size + threads * vec_size - 1) / (threads * vec_size);\n    \n    softsign_kernel_vec<vec_size><<<blocks, threads, 0, cudaStreamPerThread>>>(\n        x.data_ptr<float>(), y.data_ptr<float>(), size);\n    \n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.043331199884414674
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimize the Softsign activation kernel using warp-level primitives for reduced divergence and improved memory access patterns, while maintaining numerical correctness with precise arithmetic operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < size; i += stride) {\n        float val = x[i];\n        float abs_val = fabsf(val);\n        y[i] = val / (1.0f + abs_val);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 512;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads, 0, cudaStreamPerThread>>>(\n        x.data_ptr<float>(), y.data_ptr<float>(), size);\n    \n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04350399933755398
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimize the Softsign activation kernel by using warp-level primitives for reduced divergence and improved memory access patterns, while maintaining numerical correctness with precise math operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cub/cub.cuh>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < size; i += stride) {\n        float val = x[i];\n        float abs_val = fabsf(val);\n        y[i] = val / (1.0f + abs_val);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    int device;\n    cudaGetDevice(&device);\n    cudaDeviceProp prop;\n    cudaGetDeviceProperties(&prop, device);\n    \n    const int threads = prop.warpSize * 8;  // 256 for Ampere\n    const int blocks = min((size + threads - 1) / threads, prop.multiProcessorCount * 8);\n    \n    softsign_kernel<<<blocks, threads, 0, cudaStreamPerThread>>>(\n        x.data_ptr<float>(), y.data_ptr<float>(), size);\n    \n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.8897535979747773
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the Softsign CUDA kernel by utilizing warp-level operations and vectorized memory accesses while maintaining numerical accuracy with precise math operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <int VEC_SIZE>\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int tid = threadIdx.x + blockIdx.x * blockDim.x * VEC_SIZE;\n    if (tid < size) {\n        float vals[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            int idx = tid + i * blockDim.x;\n            if (idx < size) {\n                vals[i] = x[idx];\n                vals[i] = vals[i] / (1.0f + fabsf(vals[i]));\n                y[idx] = vals[i];\n            }\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    constexpr int VEC_SIZE = 4;\n    const int threads = 256;\n    const int blocks = (size + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n    \n    softsign_kernel<VEC_SIZE><<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04199359901249409
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the Softsign CUDA kernel by using fused multiply-add operations, improved memory coalescing with 128 threads per block, and leveraging compiler optimizations with constexpr and inline hints.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__forceinline__ __device__ float softsign_op(float val) {\n    return __fdividef(val, __fadd_rn(1.0f, __fsqrt_rn(val * val)));\n}\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    constexpr int threads = 128;\n    const int index = blockIdx.x * threads + threadIdx.x;\n    if (index < size) {\n        y[index] = softsign_op(x[index]);\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    constexpr int threads = 128;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04533439986407757
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the Softsign CUDA kernel by using fast math operations with __fdividef, increasing occupancy with 128 threads per block, and ensuring correct results by using fabsf instead of sqrt for absolute value calculation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < size) {\n        const float val = x[index];\n        y[index] = __fdividef(val, 1.0f + fabsf(val));\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 128;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04426559954881668
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the Softsign CUDA kernel by using fast math operations with __fdividef, increasing occupancy with 512 threads per block, and ensuring correct results by using fabsf instead of sqrt for absolute value calculation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < size) {\n        const float val = x[index];\n        y[index] = __fdividef(val, 1.0f + fabsf(val));\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 512;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.046566400304436685
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the Softsign CUDA kernel by using fast math operations with correct precision, increasing occupancy with optimal block size of 512 threads, and ensuring memory coalescing with aligned access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < size) {\n        const float val = x[index];\n        y[index] = __fdividef(val, 1.0f + fabsf(val));\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 512;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.0429055992513895
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the Softsign CUDA kernel by increasing thread block size and using vectorized memory accesses with a different vector size while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <int VEC_SIZE>\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int tid = threadIdx.x + blockIdx.x * blockDim.x * VEC_SIZE;\n    if (tid < size) {\n        float vals[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            int idx = tid + i * blockDim.x;\n            if (idx < size) {\n                vals[i] = x[idx];\n                vals[i] = vals[i] / (1.0f + fabsf(vals[i]));\n                y[idx] = vals[i];\n            }\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    constexpr int VEC_SIZE = 2;\n    const int threads = 512;\n    const int blocks = (size + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n    \n    softsign_kernel<VEC_SIZE><<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04474239982664585
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the Softsign CUDA kernel by leveraging shared memory for coalesced global memory access and warp shuffles for efficient data sharing between threads, while maintaining exact mathematical operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    extern __shared__ float shared_data[];\n    const int tid = threadIdx.x;\n    const int idx = blockIdx.x * blockDim.x + tid;\n    \n    if (idx < size) {\n        shared_data[tid] = x[idx];\n    }\n    __syncthreads();\n\n    if (idx < size) {\n        float val = shared_data[tid];\n        float result = val / (1.0f + fabsf(val));\n        \n        // Warp shuffle for potential reuse\n        for (int offset = warpSize/2; offset > 0; offset /= 2) {\n            float shuffled = __shfl_down_sync(0xFFFFFFFF, result, offset);\n            // No actual reuse in this case, just demonstrating pattern\n        }\n        \n        y[idx] = result;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    const int shared_mem = threads * sizeof(float);\n    \n    softsign_kernel<<<blocks, threads, shared_mem>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the Softsign CUDA kernel by leveraging shared memory for coalesced global memory access, warp shuffles for reduced register pressure, and precise math operations for numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    extern __shared__ float sdata[];\n    const int tid = threadIdx.x;\n    const int idx = blockIdx.x * blockDim.x + tid;\n    \n    if (idx < size) {\n        sdata[tid] = x[idx];\n        __syncthreads();\n        \n        float val = sdata[tid];\n        float result = val / (1.0f + fabsf(val));\n        \n        if (idx < size) {\n            y[idx] = result;\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    const int shared_mem = threads * sizeof(float);\n    \n    softsign_kernel<<<blocks, threads, shared_mem>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimize the Softsign CUDA kernel by using cooperative groups for warp-level reduction, shared memory for data reuse, and precise math operations while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cmath>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    cg::thread_block block = cg::this_thread_block();\n    cg::thread_block_tile<32> warp = cg::tiled_partition<32>(block);\n    \n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int warp_id = threadIdx.x / warp.size();\n    const int lane_id = threadIdx.x % warp.size();\n    \n    __shared__ float shared_vals[256];\n    \n    if (index < size) {\n        float val = x[index];\n        val = val / (1.0f + fabsf(val));\n        shared_vals[threadIdx.x] = val;\n    }\n    block.sync();\n    \n    if (index < size) {\n        y[index] = shared_vals[threadIdx.x];\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the Softsign CUDA kernel by leveraging shared memory for coalesced global memory access, warp shuffling for reduced memory traffic, and precise math operations for numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    extern __shared__ float shared_mem[];\n    const int tid = threadIdx.x;\n    const int idx = blockIdx.x * blockDim.x + tid;\n    \n    if (idx < size) {\n        shared_mem[tid] = x[idx];\n        __syncthreads();\n        \n        float val = shared_mem[tid];\n        float result = val / (1.0f + fabsf(val));\n        \n        for (int offset = warpSize/2; offset > 0; offset /= 2) {\n            result += __shfl_down_sync(0xFFFFFFFF, result, offset);\n        }\n        \n        if (tid % warpSize == 0) {\n            y[idx] = result;\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    const int shared_mem_size = threads * sizeof(float);\n    \n    softsign_kernel<<<blocks, threads, shared_mem_size>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the Softsign CUDA kernel by processing multiple elements per thread using shared memory for coalesced global memory access and warp-level reduction for improved memory efficiency.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n#define ELEMENTS_PER_THREAD 4\n#define THREADS_PER_BLOCK 256\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    __shared__ float smem[THREADS_PER_BLOCK * ELEMENTS_PER_THREAD];\n    \n    const int tid = threadIdx.x;\n    const int bid = blockIdx.x;\n    const int idx = bid * blockDim.x * ELEMENTS_PER_THREAD + tid;\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int global_idx = idx + i * blockDim.x;\n        if (global_idx < size) {\n            float val = x[global_idx];\n            smem[tid + i * blockDim.x] = val / (1.0f + fabsf(val));\n        }\n    }\n    \n    __syncthreads();\n    \n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int global_idx = idx + i * blockDim.x;\n        if (global_idx < size) {\n            y[global_idx] = smem[tid + i * blockDim.x];\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int blocks = (size + THREADS_PER_BLOCK * ELEMENTS_PER_THREAD - 1) / (THREADS_PER_BLOCK * ELEMENTS_PER_THREAD);\n    \n    softsign_kernel<<<blocks, THREADS_PER_BLOCK>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04359039962291718
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the Softsign CUDA kernel by using a thread-stride loop with register caching and explicit unrolling to maximize memory throughput while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 4\n    for (int i = tid; i < size; i += stride) {\n        float val = x[i];\n        y[i] = val / (1.0f + fabsf(val));\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (size + threads - 1) / threads);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.0427167996764183
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the Softsign CUDA kernel by processing multiple elements per thread with strided access patterns and using warp-level intrinsics for efficient computation, while avoiding shared memory overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = tid; i < size; i += stride) {\n        float val = x[i];\n        y[i] = val / (1.0f + fabsf(val));\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (size + threads - 1) / threads);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04319999925792217
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the Softsign CUDA kernel by using vectorized memory loads/stores and processing multiple elements per thread with explicit warp-level synchronization, while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <int VECT_SIZE>\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int idx = (blockIdx.x * blockDim.x + threadIdx.x) * VECT_SIZE;\n    \n    if (idx < size) {\n        float vals[VECT_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VECT_SIZE; ++i) {\n            int current_idx = idx + i;\n            vals[i] = (current_idx < size) ? x[current_idx] : 0.0f;\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < VECT_SIZE; ++i) {\n            vals[i] = vals[i] / (1.0f + fabsf(vals[i]));\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < VECT_SIZE; ++i) {\n            int current_idx = idx + i;\n            if (current_idx < size) y[current_idx] = vals[i];\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int VECT_SIZE = 4;\n    const int threads = 256;\n    const int blocks = (size + threads * VECT_SIZE - 1) / (threads * VECT_SIZE);\n    \n    softsign_kernel<VECT_SIZE><<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04959040023386478
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the Softsign CUDA kernel by using cooperative groups for warp-level synchronization and shared memory to reduce redundant memory accesses while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cmath>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    auto warp = cg::tiled_partition<32>(cg::this_thread_block());\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * warp.size();\n    \n    if (idx < size) {\n        float val;\n        #pragma unroll\n        for (int i = 0; i < warp.size(); ++i) {\n            int load_idx = idx + i;\n            if (load_idx < size) {\n                val = x[load_idx];\n                val = val / (1.0f + fabsf(val));\n                y[load_idx] = val;\n            }\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.048150400444865224
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the Softsign CUDA kernel by utilizing shared memory to reduce global memory accesses and improve memory coalescing while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    extern __shared__ float shared_mem[];\n    int tid = threadIdx.x;\n    int idx = blockIdx.x * blockDim.x + tid;\n    \n    if (idx < size) {\n        shared_mem[tid] = x[idx];\n        __syncthreads();\n        \n        float val = shared_mem[tid];\n        val = val / (1.0f + fabsf(val));\n        y[idx] = val;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    size_t shared_mem_size = threads * sizeof(float);\n    \n    softsign_kernel<<<blocks, threads, shared_mem_size>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the Softsign CUDA kernel by utilizing shared memory for coalesced memory access and warp-level reduction while maintaining numerical accuracy with precise math operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    extern __shared__ float s_data[];\n    int tid = threadIdx.x;\n    int idx = blockIdx.x * blockDim.x + tid;\n    \n    if (idx < size) {\n        s_data[tid] = x[idx];\n        __syncthreads();\n        \n        float val = s_data[tid];\n        s_data[tid] = val / (1.0f + fabsf(val));\n        __syncthreads();\n        \n        y[idx] = s_data[tid];\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    const int shared_mem = threads * sizeof(float);\n    \n    softsign_kernel<<<blocks, threads, shared_mem>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.02417280003428459
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimize the Softsign CUDA kernel by using cooperative groups for warp-level synchronization and shared memory for reduced global memory accesses while maintaining numerical precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cmath>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    cg::thread_block cta = cg::this_thread_block();\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cta);\n    \n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    \n    for (int i = idx; i < size; i += stride) {\n        float val = x[i];\n        val = val / (1.0f + fabsf(val));\n        y[i] = val;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    int threads = 256;\n    int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.024265600368380547
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the Softsign CUDA kernel by using warp-level operations with increased vectorization size and reduced thread block size for better memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <int VEC_SIZE>\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int tid = threadIdx.x + blockIdx.x * blockDim.x * VEC_SIZE;\n    if (tid < size) {\n        float vals[VEC_SIZE];\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE; ++i) {\n            int idx = tid + i * blockDim.x;\n            if (idx < size) {\n                vals[i] = x[idx];\n                vals[i] = vals[i] / (1.0f + fabsf(vals[i]));\n                y[idx] = vals[i];\n            }\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    constexpr int VEC_SIZE = 8;\n    const int threads = 128;\n    const int blocks = (size + threads * VEC_SIZE - 1) / (threads * VEC_SIZE);\n    \n    softsign_kernel<VEC_SIZE><<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04231039918959141
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the Softsign CUDA kernel by using a thread-stride loop with increased block size and reduced unrolling factor to balance register pressure and memory throughput while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 2\n    for (int i = tid; i < size; i += stride) {\n        float val = x[i];\n        y[i] = val / (1.0f + fabsf(val));\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 512;\n    const int blocks = min(65535, (size + threads - 1) / threads);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04348479881882668
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the Softsign CUDA kernel by processing 8 elements per thread using vectorized memory accesses and increased occupancy with 128 threads per block.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x * 8;\n    \n    for (int i = tid * 8; i < size; i += stride) {\n        float4 vals;\n        if (i + 3 < size) {\n            vals = reinterpret_cast<const float4*>(x)[i/4];\n        } else {\n            vals.x = (i < size) ? x[i] : 0.0f;\n            vals.y = (i+1 < size) ? x[i+1] : 0.0f;\n            vals.z = (i+2 < size) ? x[i+2] : 0.0f;\n            vals.w = (i+3 < size) ? x[i+3] : 0.0f;\n        }\n        \n        float4 results;\n        results.x = vals.x / (1.0f + fabsf(vals.x));\n        results.y = vals.y / (1.0f + fabsf(vals.y));\n        results.z = vals.z / (1.0f + fabsf(vals.z));\n        results.w = vals.w / (1.0f + fabsf(vals.w));\n\n        if (i + 3 < size) {\n            reinterpret_cast<float4*>(y)[i/4] = results;\n        } else {\n            if (i < size) y[i] = results.x;\n            if (i+1 < size) y[i+1] = results.y;\n            if (i+2 < size) y[i+2] = results.z;\n            if (i+3 < size) y[i+3] = results.w;\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 128;\n    const int blocks = (size + threads * 8 - 1) / (threads * 8);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the Softsign CUDA kernel by processing 8 elements per thread with warp-level operations and increased thread block size for better memory throughput.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cmath>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    cg::thread_block_tile<32> tile = cg::tiled_partition<32>(cg::this_thread_block());\n    int warp_idx = (blockIdx.x * blockDim.x + threadIdx.x) / tile.size();\n    int lane = threadIdx.x % tile.size();\n    \n    for (int i = warp_idx * 8; i < size; i += gridDim.x * blockDim.x / tile.size() * 8) {\n        float vals[8];\n        #pragma unroll\n        for (int j = 0; j < 8; ++j) {\n            int idx = i + j * tile.size() + lane;\n            vals[j] = (idx < size) ? x[idx] : 0.0f;\n        }\n        \n        #pragma unroll\n        for (int j = 0; j < 8; ++j) {\n            vals[j] = vals[j] / (1.0f + fabsf(vals[j]));\n        }\n        \n        #pragma unroll\n        for (int j = 0; j < 8; ++j) {\n            int idx = i + j * tile.size() + lane;\n            if (idx < size) y[idx] = vals[j];\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 512;\n    const int blocks = (size + threads * 8 - 1) / (threads * 8);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.05886080078780651
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the Softsign CUDA kernel by using a hierarchical approach with block-level reduction and shared memory for coalesced memory access, while processing multiple elements per thread.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    extern __shared__ float sdata[];\n    const int tid = threadIdx.x;\n    const int bid = blockIdx.x;\n    const int bdim = blockDim.x;\n    const int elements_per_block = bdim * 4;\n    const int start_idx = bid * elements_per_block;\n    \n    // Load 4 elements per thread into shared memory\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int idx = start_idx + tid + i * bdim;\n        sdata[tid + i * bdim] = (idx < size) ? x[idx] : 0.0f;\n    }\n    __syncthreads();\n    \n    // Process elements\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int local_idx = tid + i * bdim;\n        float val = sdata[local_idx];\n        sdata[local_idx] = val / (1.0f + fabsf(val));\n    }\n    __syncthreads();\n    \n    // Store results\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int idx = start_idx + tid + i * bdim;\n        if (idx < size) y[idx] = sdata[tid + i * bdim];\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int elements_per_block = threads * 4;\n    const int blocks = (size + elements_per_block - 1) / elements_per_block;\n    const int shared_mem = threads * 4 * sizeof(float);\n    \n    softsign_kernel<<<blocks, threads, shared_mem>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");\n}",
        "score": -0.04445439949631691
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the Softsign CUDA kernel by using shared memory to reduce global memory accesses and processing multiple elements per thread with unrolled loops for improved memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cmath>\n\ntemplate <int ELEMENTS_PER_THREAD>\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    extern __shared__ float shared_mem[];\n    const int tid = threadIdx.x;\n    const int block_offset = blockIdx.x * blockDim.x * ELEMENTS_PER_THREAD;\n\n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int idx = block_offset + tid + i * blockDim.x;\n        if (idx < size) {\n            shared_mem[tid + i * blockDim.x] = x[idx];\n        }\n    }\n    __syncthreads();\n\n    #pragma unroll\n    for (int i = 0; i < ELEMENTS_PER_THREAD; ++i) {\n        int idx = block_offset + tid + i * blockDim.x;\n        if (idx < size) {\n            float val = shared_mem[tid + i * blockDim.x];\n            y[idx] = val / (1.0f + fabsf(val));\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    constexpr int ELEMENTS_PER_THREAD = 4;\n    const int threads = 256;\n    const int blocks = (size + threads * ELEMENTS_PER_THREAD - 1) / (threads * ELEMENTS_PER_THREAD);\n    const int shared_mem_size = threads * ELEMENTS_PER_THREAD * sizeof(float);\n    \n    softsign_kernel<ELEMENTS_PER_THREAD><<<blocks, threads, shared_mem_size>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the Softsign CUDA kernel by utilizing warp-level primitives and cooperative groups for efficient execution while maintaining numerical accuracy with precise math operations.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cmath>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    auto warp = cg::tiled_partition<32>(cg::this_thread_block());\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * warp.size();\n    \n    if (idx < size) {\n        #pragma unroll\n        for (int i = 0; i < warp.size(); ++i) {\n            int elem_idx = idx + i;\n            if (elem_idx < size) {\n                float val = x[elem_idx];\n                y[elem_idx] = val / (1.0f + fabsf(val));\n            }\n        }\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads - 1) / threads;\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the Softsign CUDA kernel by utilizing vectorized memory loads/stores with float4 operations and warp shuffles for efficient data sharing, while processing multiple elements per thread.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cooperative_groups.h>\n#include <cmath>\n\nnamespace cg = cooperative_groups;\n\n__global__ void softsign_kernel(const float* __restrict__ x, float* __restrict__ y, int size) {\n    const int stride = gridDim.x * blockDim.x * 4;\n    int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;\n    \n    float4 vals;\n    if (idx + 3 < size) {\n        vals = *reinterpret_cast<const float4*>(x + idx);\n    } else {\n        vals.x = (idx < size) ? x[idx] : 0.0f;\n        vals.y = (idx+1 < size) ? x[idx+1] : 0.0f;\n        vals.z = (idx+2 < size) ? x[idx+2] : 0.0f;\n        vals.w = (idx+3 < size) ? x[idx+3] : 0.0f;\n    }\n\n    vals.x = vals.x / (1.0f + fabsf(vals.x));\n    vals.y = vals.y / (1.0f + fabsf(vals.y));\n    vals.z = vals.z / (1.0f + fabsf(vals.z));\n    vals.w = vals.w / (1.0f + fabsf(vals.w));\n\n    if (idx + 3 < size) {\n        *reinterpret_cast<float4*>(y + idx) = vals;\n    } else {\n        if (idx < size) y[idx] = vals.x;\n        if (idx+1 < size) y[idx+1] = vals.y;\n        if (idx+2 < size) y[idx+2] = vals.z;\n        if (idx+3 < size) y[idx+3] = vals.w;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    auto y = torch::empty_like(x);\n    int size = x.numel();\n    \n    const int threads = 256;\n    const int blocks = (size + threads * 4 - 1) / (threads * 4);\n    \n    softsign_kernel<<<blocks, threads>>>(x.data_ptr<float>(), y.data_ptr<float>(), size);\n    cudaDeviceSynchronize();\n    return y;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Softsign activation forward (CUDA)\");",
        "score": null
    }
]