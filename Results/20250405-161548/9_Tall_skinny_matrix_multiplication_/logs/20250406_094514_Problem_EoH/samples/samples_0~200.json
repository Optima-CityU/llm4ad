[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling, loop unrolling, and increasing thread block size for better memory access patterns and occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int load_k = tiled_k + threadIdx.x;\n\n        if (row < M && load_k < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + load_k];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        load_k = tiled_k + threadIdx.y;\n        if (col < N && load_k < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_k * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.041644763946533
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling, loop unrolling, and increasing thread block size to improve memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int a_col = tiled_k + threadIdx.x;\n        int b_row = tiled_k + threadIdx.y;\n\n        if (row < M && a_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + a_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (b_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[b_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.0301408767700195
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling, loop unrolling, and increased thread block size to improve memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tiles into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product with loop unrolling\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.0080255508422855
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimize the matmul kernel by using shared memory for tiling, loop unrolling, and increasing thread block size for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int A_col = tiledK + threadIdx.x;\n        int B_row = tiledK + threadIdx.y;\n\n        if (row < M && A_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            sum += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            sum += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            sum += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.027712011337281
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimized the CUDA kernel by using shared memory for tiling, loop unrolling, and increasing thread block size for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tile from A and B into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product with loop unrolling\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.011702251434326
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimized the kernel by using shared memory for tiling, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tile from A and B into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product with loop unrolling\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.011651229858399
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling, loop unrolling, and increasing thread block size for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    float value = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int load_row = row;\n        int load_col = tiled_k + threadIdx.x;\n        if (load_row < M && load_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[load_row * K + load_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        load_row = tiled_k + threadIdx.y;\n        load_col = col;\n        if (load_row < K && load_col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[load_row * N + load_col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.045795202255249
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the CUDA kernel by using shared memory for tiling, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tiles into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product with loop unrolling\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.010006332397461
    },
    {
        "sample_order": 9,
        "algorithm": "{Implement a warp-level matrix multiplication approach using register tiling and warp shuffles to reduce shared memory usage and improve memory access efficiency.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_K 8\n#define TILE_M 8\n#define TILE_N 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    \n    const int row = blockIdx.y * TILE_M * blockDim.y + warp_id * TILE_M;\n    const int col = blockIdx.x * TILE_N * WARP_SIZE + lane_id;\n    \n    float accum[TILE_M][TILE_N] = {0.0f};\n    float frag_a[TILE_M][TILE_K];\n    float frag_b[TILE_K][TILE_N];\n    \n    for (int k = 0; k < K; k += TILE_K) {\n        #pragma unroll\n        for (int ki = 0; ki < TILE_K; ++ki) {\n            #pragma unroll\n            for (int mi = 0; mi < TILE_M; ++mi) {\n                int a_row = row + mi;\n                int a_col = k + ki;\n                frag_a[mi][ki] = (a_row < M && a_col < K) ? A[a_row * K + a_col] : 0.0f;\n            }\n            \n            #pragma unroll\n            for (int ni = 0; ni < TILE_N; ++ni) {\n                int b_row = k + ki;\n                int b_col = col + ni * WARP_SIZE;\n                frag_b[ki][ni] = (b_row < K && b_col < N) ? B[b_row * N + b_col] : 0.0f;\n            }\n        }\n        \n        #pragma unroll\n        for (int ki = 0; ki < TILE_K; ++ki) {\n            #pragma unroll\n            for (int mi = 0; mi < TILE_M; ++mi) {\n                #pragma unroll\n                for (int ni = 0; ni < TILE_N; ++ni) {\n                    accum[mi][ni] += frag_a[mi][ki] * frag_b[ki][ni];\n                }\n            }\n        }\n    }\n    \n    #pragma unroll\n    for (int mi = 0; mi < TILE_M; ++mi) {\n        #pragma unroll\n        for (int ni = 0; ni < TILE_N; ++ni) {\n            int c_row = row + mi;\n            int c_col = col + ni * WARP_SIZE;\n            if (c_row < M && c_col < N) {\n                C[c_row * N + c_col] = accum[mi][ni];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n    \n    auto C = torch::zeros({M, N}, A.options());\n    \n    dim3 threads(256);\n    dim3 blocks((N + WARP_SIZE * TILE_N - 1) / (WARP_SIZE * TILE_N), \n                (M + TILE_M * (threads.x / WARP_SIZE) - 1) / (TILE_M * (threads.x / WARP_SIZE)));\n    \n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n    \n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the matmul kernel by using warp-level matrix operations and register tiling to reduce shared memory usage and improve instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_K 8\n#define TILE_M 8\n#define TILE_N 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    const int warp_row = blockIdx.y * blockDim.y + threadIdx.y;\n    const int warp_col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (warp_row >= M || warp_col >= N) return;\n\n    float accum[TILE_M][TILE_N] = {0.0f};\n    float a_frag[TILE_M][TILE_K];\n    float b_frag[TILE_K][TILE_N];\n\n    for (int k_base = 0; k_base < K; k_base += WARP_SIZE * TILE_K) {\n        #pragma unroll\n        for (int k = 0; k < TILE_K; ++k) {\n            int k_idx = k_base + threadIdx.x * TILE_K + k;\n            if (k_idx < K) {\n                a_frag[0][k] = A[warp_row * K + k_idx];\n                b_frag[k][0] = B[k_idx * N + warp_col];\n            } else {\n                a_frag[0][k] = 0.0f;\n                b_frag[k][0] = 0.0f;\n            }\n        }\n\n        #pragma unroll\n        for (int k = 0; k < TILE_K; ++k) {\n            #pragma unroll\n            for (int m = 0; m < TILE_M; ++m) {\n                #pragma unroll\n                for (int n = 0; n < TILE_N; ++n) {\n                    accum[m][n] += a_frag[m][k] * b_frag[k][n];\n                }\n            }\n        }\n    }\n\n    C[warp_row * N + warp_col] = accum[0][0];\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARP_SIZE, 1);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the matmul kernel by using warp-level matrix multiplication with register tiling and cooperative loading from shared memory to maximize memory throughput and minimize bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * THREADS_PER_WARP)\n#define REG_TILE_SIZE 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE+1];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE+1];\n\n    int warpId = threadIdx.x / THREADS_PER_WARP;\n    int laneId = threadIdx.x % THREADS_PER_WARP;\n\n    int row = (blockIdx.y * WARPS_PER_BLOCK + warpId) * REG_TILE_SIZE;\n    int col = blockIdx.x * TILE_SIZE + laneId;\n\n    float regA[REG_TILE_SIZE];\n    float regB[REG_TILE_SIZE];\n    float regC[REG_TILE_SIZE][REG_TILE_SIZE] = {0.0f};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        \n        #pragma unroll\n        for (int i = 0; i < REG_TILE_SIZE; ++i) {\n            int loadRow = row + i;\n            int loadCol = tiledK + laneId;\n            if (loadRow < M && loadCol < K) {\n                regA[i] = A[loadRow * K + loadCol];\n            } else {\n                regA[i] = 0.0f;\n            }\n        }\n\n        #pragma unroll\n        for (int i = 0; i < REG_TILE_SIZE; ++i) {\n            int loadRow = tiledK + laneId;\n            int loadCol = col + i;\n            if (loadRow < K && loadCol < N) {\n                regB[i] = B[loadRow * N + loadCol];\n            } else {\n                regB[i] = 0.0f;\n            }\n        }\n\n        for (int i = 0; i < REG_TILE_SIZE; ++i) {\n            As[laneId][warpId * REG_TILE_SIZE + i] = regA[i];\n        }\n\n        for (int i = 0; i < REG_TILE_SIZE; ++i) {\n            Bs[laneId][warpId * REG_TILE_SIZE + i] = regB[i];\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            float a = As[k][warpId * REG_TILE_SIZE + threadIdx.y];\n            #pragma unroll\n            for (int i = 0; i < REG_TILE_SIZE; ++i) {\n                regC[threadIdx.y][i] += a * Bs[k][laneId * REG_TILE_SIZE + i];\n            }\n        }\n\n        __syncthreads();\n    }\n\n    #pragma unroll\n    for (int i = 0; i < REG_TILE_SIZE; ++i) {\n        #pragma unroll\n        for (int j = 0; j < REG_TILE_SIZE; ++j) {\n            int writeRow = row + i;\n            int writeCol = col + j;\n            if (writeRow < M && writeCol < N) {\n                C[writeRow * N + writeCol] = regC[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(THREADS_PER_BLOCK);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + (WARPS_PER_BLOCK * REG_TILE_SIZE) - 1) / (WARPS_PER_BLOCK * REG_TILE_SIZE));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiply-accumulate operations with tensor cores for maximum throughput on RTX 4090.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\nconstexpr int WARP_SIZE = 32;\nconstexpr int MMA_M = 16;\nconstexpr int MMA_N = 16;\nconstexpr int MMA_K = 16;\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    // Warp and lane identification\n    const unsigned int warpId = threadIdx.x / WARP_SIZE;\n    const unsigned int laneId = threadIdx.x % WARP_SIZE;\n    \n    // Tile indices\n    const unsigned int m = (blockIdx.y * MMA_M) + (warpId / 2) * MMA_M;\n    const unsigned int n = (blockIdx.x * MMA_N) + (warpId % 2) * MMA_N;\n    \n    if (m >= M || n >= N) return;\n    \n    // Declare fragments for A, B and C\n    half frag_a[MMA_M][MMA_K];\n    half frag_b[MMA_N][MMA_K];\n    float frag_c[MMA_M][MMA_N] = {0.0f};\n    \n    // Main computation loop\n    for (unsigned int k = 0; k < K; k += MMA_K) {\n        // Load A tile\n        #pragma unroll\n        for (unsigned int mi = 0; mi < MMA_M; ++mi) {\n            #pragma unroll\n            for (unsigned int ki = 0; ki < MMA_K; ++ki) {\n                int row = m + mi;\n                int col = k + ki;\n                if (row < M && col < K) {\n                    frag_a[mi][ki] = __float2half(A[row * K + col]);\n                } else {\n                    frag_a[mi][ki] = __float2half(0.0f);\n                }\n            }\n        }\n        \n        // Load B tile\n        #pragma unroll\n        for (unsigned int ni = 0; ni < MMA_N; ++ni) {\n            #pragma unroll\n            for (unsigned int ki = 0; ki < MMA_K; ++ki) {\n                int row = k + ki;\n                int col = n + ni;\n                if (row < K && col < N) {\n                    frag_b[ni][ki] = __float2half(B[row * N + col]);\n                } else {\n                    frag_b[ni][ki] = __float2half(0.0f);\n                }\n            }\n        }\n        \n        // MMA operation\n        #pragma unroll\n        for (unsigned int mi = 0; mi < MMA_M; ++mi) {\n            #pragma unroll\n            for (unsigned int ni = 0; ni < MMA_N; ++ni) {\n                #pragma unroll\n                for (unsigned int ki = 0; ki < MMA_K; ++ki) {\n                    frag_c[mi][ni] += __half2float(frag_a[mi][ki]) * __half2float(frag_b[ni][ki]);\n                }\n            }\n        }\n    }\n    \n    // Store results\n    #pragma unroll\n    for (unsigned int mi = 0; mi < MMA_M; ++mi) {\n        #pragma unroll\n        for (unsigned int ni = 0; ni < MMA_N; ++ni) {\n            int row = m + mi;\n            int col = n + ni;\n            if (row < M && col < N) {\n                C[row * N + col] = frag_c[mi][ni];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(128); // 4 warps per block\n    dim3 blocks((N + MMA_N - 1) / MMA_N, (M + MMA_M - 1) / MMA_M);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the CUDA matmul kernel by using a warp-level approach with register tiling and cooperative loading to reduce shared memory bank conflicts while maintaining high occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    int warp_row = (blockIdx.y * blockDim.y + threadIdx.y) / WARP_SIZE;\n    int warp_col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (warp_row >= M || warp_col >= N) return;\n    \n    float reg_A[UNROLL_FACTOR];\n    float reg_B[UNROLL_FACTOR];\n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        int lane_id = threadIdx.y % WARP_SIZE;\n        \n        #pragma unroll\n        for (int i = 0; i < UNROLL_FACTOR; ++i) {\n            int k = tiled_k + lane_id + i * WARP_SIZE;\n            if (k < K) {\n                reg_A[i] = A[warp_row * K + k];\n                reg_B[i] = B[k * N + warp_col];\n            } else {\n                reg_A[i] = 0.0f;\n                reg_B[i] = 0.0f;\n            }\n        }\n        \n        #pragma unroll\n        for (int i = 0; i < UNROLL_FACTOR; ++i) {\n            value += reg_A[i] * reg_B[i];\n        }\n    }\n    \n    if (warp_row < M && warp_col < N) {\n        C[warp_row * N + warp_col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(32, 32);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M * WARP_SIZE + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the kernel by using a larger tile size (64x64) with double buffering in shared memory to overlap memory transfers with computation, while maintaining the same loop unrolling strategy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    int stage = 0;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Preload next tile while computing current tile\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[1-stage][threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[1-stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[1-stage][threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[1-stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute current tile\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[stage][threadIdx.y][k] * Bs[stage][k][threadIdx.x];\n            value += As[stage][threadIdx.y][k+1] * Bs[stage][k+1][threadIdx.x];\n            value += As[stage][threadIdx.y][k+2] * Bs[stage][k+2][threadIdx.x];\n            value += As[stage][threadIdx.y][k+3] * Bs[stage][k+3][threadIdx.x];\n        }\n        \n        stage = 1 - stage;\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the matmul kernel by using register tiling in addition to shared memory tiling, with each thread computing multiple output elements to increase arithmetic intensity and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n#define REG_TILE 2\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE * REG_TILE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE * REG_TILE + threadIdx.x;\n\n    float sum[REG_TILE][REG_TILE] = {0.0f};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n\n        for (int regRow = 0; regRow < REG_TILE; ++regRow) {\n            int A_row = row + regRow * TILE_SIZE;\n            int A_col = tiledK + threadIdx.x;\n            if (A_row < M && A_col < K) {\n                As[threadIdx.y + regRow * TILE_SIZE][threadIdx.x] = A[A_row * K + A_col];\n            } else {\n                As[threadIdx.y + regRow * TILE_SIZE][threadIdx.x] = 0.0f;\n            }\n        }\n\n        for (int regCol = 0; regCol < REG_TILE; ++regCol) {\n            int B_row = tiledK + threadIdx.y;\n            int B_col = col + regCol * TILE_SIZE;\n            if (B_row < K && B_col < N) {\n                Bs[threadIdx.y][threadIdx.x + regCol * TILE_SIZE] = B[B_row * N + B_col];\n            } else {\n                Bs[threadIdx.y][threadIdx.x + regCol * TILE_SIZE] = 0.0f;\n            }\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            for (int regRow = 0; regRow < REG_TILE; ++regRow) {\n                for (int regCol = 0; regCol < REG_TILE; ++regCol) {\n                    sum[regRow][regCol] += As[threadIdx.y + regRow * TILE_SIZE][k] * Bs[k][threadIdx.x + regCol * TILE_SIZE];\n                    sum[regRow][regCol] += As[threadIdx.y + regRow * TILE_SIZE][k+1] * Bs[k+1][threadIdx.x + regCol * TILE_SIZE];\n                    sum[regRow][regCol] += As[threadIdx.y + regRow * TILE_SIZE][k+2] * Bs[k+2][threadIdx.x + regCol * TILE_SIZE];\n                    sum[regRow][regCol] += As[threadIdx.y + regRow * TILE_SIZE][k+3] * Bs[k+3][threadIdx.x + regCol * TILE_SIZE];\n                }\n            }\n        }\n\n        __syncthreads();\n    }\n\n    for (int regRow = 0; regRow < REG_TILE; ++regRow) {\n        for (int regCol = 0; regCol < REG_TILE; ++regCol) {\n            int out_row = row + regRow * TILE_SIZE;\n            int out_col = col + regCol * TILE_SIZE;\n            if (out_row < M && out_col < N) {\n                C[out_row * N + out_col] = sum[regRow][regCol];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE * REG_TILE - 1) / (TILE_SIZE * REG_TILE), \n                (M + TILE_SIZE * REG_TILE - 1) / (TILE_SIZE * REG_TILE));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the CUDA matmul kernel by using a warp-level approach with register tiling, reducing shared memory usage while maintaining coalesced memory access and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    const int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    float regA[TILE_SIZE];\n    float regB[TILE_SIZE];\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        const int tiled_k = t * TILE_SIZE;\n        \n        // Load tiles into registers\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            regA[threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            regA[threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            regB[threadIdx.y] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            regB[threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value += regA[k] * regB[k];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARP_SIZE, 1);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimized the CUDA kernel by using shared memory for tiling with increased tile size, register blocking, and prefetching to reduce shared memory bank conflicts and improve memory bandwidth utilization.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define BLOCK_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float regC[BLOCK_SIZE][BLOCK_SIZE] = {{0.0f}};\n    float regA[BLOCK_SIZE];\n    float regB[BLOCK_SIZE];\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Prefetch next tile while computing current tile\n        int next_t = t + 1;\n        int next_tiled_k = next_t * TILE_SIZE;\n        int load_idx = t % 2;\n        int compute_idx = (t + 1) % 2;\n        \n        // Load tile from A and B into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[load_idx][threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        }\n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[load_idx][threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        }\n        \n        __syncthreads();\n        \n        // Compute with register blocking\n        for (int k = 0; k < TILE_SIZE; k += BLOCK_SIZE) {\n            for (int i = 0; i < BLOCK_SIZE; ++i) {\n                regA[i] = As[compute_idx][threadIdx.y][k + i];\n                regB[i] = Bs[compute_idx][k + i][threadIdx.x];\n            }\n            \n            for (int i = 0; i < BLOCK_SIZE; ++i) {\n                for (int j = 0; j < BLOCK_SIZE; ++j) {\n                    regC[i][j] += regA[i] * regB[j];\n                }\n            }\n        }\n        \n        __syncthreads();\n    }\n    \n    // Store results\n    if (row < M && col < N) {\n        C[row * N + col] = regC[0][0];\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimized the CUDA kernel by using shared memory with a larger tile size (64x64), warp-level tiling, and prefetching to improve memory bandwidth utilization and reduce shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE/WARPS_PER_BLOCK];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE/WARPS_PER_BLOCK];\n    \n    int warp_id = threadIdx.y / WARP_SIZE;\n    int lane_id = threadIdx.y % WARP_SIZE;\n    int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE/WARPS_PER_BLOCK) + lane_id;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Prefetch next tile\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[warp_id][lane_id][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        }\n        if ((tiled_k + lane_id) < K && col < N) {\n            Bs[warp_id][lane_id][threadIdx.x] = B[(tiled_k + lane_id) * N + col];\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product\n        for (int k = 0; k < TILE_SIZE/WARPS_PER_BLOCK; ++k) {\n            for (int w = 0; w < WARPS_PER_BLOCK; ++w) {\n                value += As[w][lane_id][k] * Bs[warp_id][k][threadIdx.x];\n            }\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, WARPS_PER_BLOCK * WARP_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimized the CUDA kernel by using shared memory for both input matrices with double buffering to overlap memory transfers with computation, and increased thread block size for better occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define BUFFER_NUM 2\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[BUFFER_NUM][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[BUFFER_NUM][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    int stage = 0;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load next tile into buffer\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[stage][threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[stage][threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute with previous buffer\n        int prev_stage = 1 - stage;\n        if (t > 0) {\n            for (int k = 0; k < TILE_SIZE; ++k) {\n                value += As[prev_stage][threadIdx.y][k] * Bs[prev_stage][k][threadIdx.x];\n            }\n        }\n        \n        stage = prev_stage;\n        __syncthreads();\n    }\n    \n    // Process last tile\n    int prev_stage = 1 - stage;\n    for (int k = 0; k < TILE_SIZE; ++k) {\n        value += As[prev_stage][threadIdx.y][k] * Bs[prev_stage][k][threadIdx.x];\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.084924793243408
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimized the CUDA kernel by using shared memory for tiling with increased tile size, warp-level tiling, and register blocking to reduce shared memory bank conflicts and improve memory access efficiency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE];\n    \n    int warp_id = threadIdx.y;\n    int lane_id = threadIdx.x;\n    \n    int row = blockIdx.y * (WARPS_PER_BLOCK * TILE_SIZE / THREADS_PER_WARP) + warp_id;\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float value[WARPS_PER_BLOCK] = {0.0f};\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tile from A into shared memory\n        if (row < M && (tiled_k + lane_id) < K) {\n            As[warp_id][lane_id] = A[row * K + tiled_k + lane_id];\n        } else {\n            As[warp_id][lane_id] = 0.0f;\n        }\n        \n        // Load tile from B into shared memory\n        if ((tiled_k + warp_id) < K && col < N) {\n            Bs[warp_id][lane_id] = B[(tiled_k + warp_id) * N + col];\n        } else {\n            Bs[warp_id][lane_id] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product with register blocking\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            value[warp_id] += As[warp_id][k] * Bs[k][lane_id];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value[warp_id];\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(THREADS_PER_WARP, WARPS_PER_BLOCK);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, \n                (M + (WARPS_PER_BLOCK * TILE_SIZE / THREADS_PER_WARP) - 1) / \n                (WARPS_PER_BLOCK * TILE_SIZE / THREADS_PER_WARP)));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimized the CUDA kernel by using shared memory for tiling with larger tile size (64x64), increased unroll factor (8), and register tiling for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            #pragma unroll\n            for (int u = 0; u < UNROLL_FACTOR; u++) {\n                value += As[threadIdx.y][k+u] * Bs[k+u][threadIdx.x];\n            }\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimize the CUDA matmul kernel by using larger tile sizes, increased thread block dimensions, and more aggressive loop unrolling to better utilize shared memory and increase parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n            value += As[threadIdx.y][k+4] * Bs[k+4][threadIdx.x];\n            value += As[threadIdx.y][k+5] * Bs[k+5][threadIdx.x];\n            value += As[threadIdx.y][k+6] * Bs[k+6][threadIdx.x];\n            value += As[threadIdx.y][k+7] * Bs[k+7][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimized the kernel by using larger shared memory tiles (64x64), increasing thread block size to 64x64, and using 8-way loop unrolling to further reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n            value += As[threadIdx.y][k+4] * Bs[k+4][threadIdx.x];\n            value += As[threadIdx.y][k+5] * Bs[k+5][threadIdx.x];\n            value += As[threadIdx.y][k+6] * Bs[k+6][threadIdx.x];\n            value += As[threadIdx.y][k+7] * Bs[k+7][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimized the CUDA kernel by using larger shared memory tiles (64x64), increased unroll factor to 8, and adjusted thread block dimensions for better occupancy on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n            value += As[threadIdx.y][k+4] * Bs[k+4][threadIdx.x];\n            value += As[threadIdx.y][k+5] * Bs[k+5][threadIdx.x];\n            value += As[threadIdx.y][k+6] * Bs[k+6][threadIdx.x];\n            value += As[threadIdx.y][k+7] * Bs[k+7][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimized the kernel by using register tiling with warp-level matrix multiplication and leveraging tensor cores for mixed-precision computation while maintaining final accumulation in full precision.}",
        "function": "#include <torch/extension.h>\n#include <cuda_fp16.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    \n    int row = blockIdx.y * TILE_SIZE * 2 + warp_id * TILE_SIZE;\n    int col = blockIdx.x * TILE_SIZE * 2 + (lane_id / 8) * TILE_SIZE;\n    \n    float accum[TILE_SIZE][TILE_SIZE] = {0.0f};\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        __half2 a_frag[TILE_SIZE/2];\n        __half2 b_frag[TILE_SIZE/2];\n        \n        // Load tiles using half2 for better memory efficiency\n        for (int i = 0; i < TILE_SIZE/2; ++i) {\n            int a_row = row + i*2;\n            int a_col = tiled_k + lane_id;\n            if (a_row < M && a_col < K) {\n                a_frag[i] = __floats2half2_rn(\n                    A[a_row * K + a_col],\n                    A[(a_row+1) * K + a_col]\n                );\n            } else {\n                a_frag[i] = __float2half2_rn(0.0f);\n            }\n            \n            int b_row = tiled_k + lane_id;\n            int b_col = col + i*2;\n            if (b_row < K && b_col < N) {\n                b_frag[i] = __floats2half2_rn(\n                    B[b_row * N + b_col],\n                    B[b_row * N + (b_col+1)]\n                );\n            } else {\n                b_frag[i] = __float2half2_rn(0.0f);\n            }\n        }\n        \n        // Warp-level matrix multiply using tensor cores\n        for (int i = 0; i < TILE_SIZE/2; ++i) {\n            for (int j = 0; j < TILE_SIZE/2; ++j) {\n                float2 tmp = __half22float2(__hmul2(a_frag[i], b_frag[j]));\n                accum[i*2][j*2] += tmp.x;\n                accum[i*2+1][j*2] += tmp.y;\n                accum[i*2][j*2+1] += tmp.x;\n                accum[i*2+1][j*2+1] += tmp.y;\n            }\n        }\n    }\n    \n    // Store results\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int out_row = row + i;\n            int out_col = col + j;\n            if (out_row < M && out_col < N) {\n                atomicAdd(&C[out_row * N + out_col], accum[i][j]);\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(256);\n    dim3 blocks((N + TILE_SIZE*2 - 1) / (TILE_SIZE*2), (M + TILE_SIZE*2 - 1) / (TILE_SIZE*2));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiply-accumulate operations with cooperative fetching and register tiling for improved throughput and reduced shared memory pressure.}",
        "function": "#include <torch/extension.h>\n\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n#define BLOCK_SIZE (WARPS_PER_BLOCK * THREADS_PER_WARP)\n#define TILE_K 8\n#define TILE_M 8\n#define TILE_N 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warpId = threadIdx.x / THREADS_PER_WARP;\n    const int laneId = threadIdx.x % THREADS_PER_WARP;\n    \n    const int globalRow = blockIdx.y * (WARPS_PER_BLOCK * TILE_M) + warpId * TILE_M;\n    const int globalCol = blockIdx.x * (TILE_N * THREADS_PER_WARP) + laneId * TILE_N;\n    \n    float regA[TILE_M][TILE_K];\n    float regB[TILE_K][TILE_N];\n    float regC[TILE_M][TILE_N] = {0.0f};\n    \n    for (int k = 0; k < K; k += TILE_K) {\n        // Cooperative fetching of tiles\n        for (int m = 0; m < TILE_M; ++m) {\n            for (int tk = 0; tk < TILE_K; ++tk) {\n                int row = globalRow + m;\n                int col = k + tk;\n                regA[m][tk] = (row < M && col < K) ? A[row * K + col] : 0.0f;\n            }\n        }\n        \n        for (int tk = 0; tk < TILE_K; ++tk) {\n            for (int n = 0; n < TILE_N; ++n) {\n                int row = k + tk;\n                int col = globalCol + n;\n                regB[tk][n] = (row < K && col < N) ? B[row * N + col] : 0.0f;\n            }\n        }\n        \n        // Warp-level matrix multiply-accumulate\n        for (int tk = 0; tk < TILE_K; ++tk) {\n            for (int m = 0; m < TILE_M; ++m) {\n                for (int n = 0; n < TILE_N; ++n) {\n                    regC[m][n] += regA[m][tk] * regB[tk][n];\n                }\n            }\n        }\n    }\n    \n    // Write results\n    for (int m = 0; m < TILE_M; ++m) {\n        for (int n = 0; n < TILE_N; ++n) {\n            int row = globalRow + m;\n            int col = globalCol + n;\n            if (row < M && col < N) {\n                C[row * N + col] = regC[m][n];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(BLOCK_SIZE);\n    dim3 blocks((N + TILE_N * THREADS_PER_WARP - 1) / (TILE_N * THREADS_PER_WARP),\n                (M + WARPS_PER_BLOCK * TILE_M - 1) / (WARPS_PER_BLOCK * TILE_M));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiply-accumulate operations with cooperative fetching and register tiling for improved memory efficiency and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_K 8\n#define TILE_M 8\n#define TILE_N 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_row = blockIdx.y * blockDim.y + threadIdx.y;\n    const int warp_col = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    if (warp_row >= M || warp_col >= N) return;\n    \n    float accum[TILE_M][TILE_N] = {0.0f};\n    \n    for (int k_base = 0; k_base < K; k_base += TILE_K) {\n        float a_frag[TILE_M][TILE_K];\n        float b_frag[TILE_K][TILE_N];\n        \n        // Cooperative fetching of A and B tiles\n        for (int m = 0; m < TILE_M; ++m) {\n            for (int k = 0; k < TILE_K; ++k) {\n                int row = warp_row + m;\n                int col = k_base + k;\n                a_frag[m][k] = (row < M && col < K) ? A[row * K + col] : 0.0f;\n            }\n        }\n        \n        for (int k = 0; k < TILE_K; ++k) {\n            for (int n = 0; n < TILE_N; ++n) {\n                int row = k_base + k;\n                int col = warp_col + n;\n                b_frag[k][n] = (row < K && col < N) ? B[row * N + col] : 0.0f;\n            }\n        }\n        \n        // Warp-level matrix multiply-accumulate\n        for (int m = 0; m < TILE_M; ++m) {\n            for (int n = 0; n < TILE_N; ++n) {\n                for (int k = 0; k < TILE_K; ++k) {\n                    accum[m][n] += a_frag[m][k] * b_frag[k][n];\n                }\n            }\n        }\n    }\n    \n    // Store results\n    for (int m = 0; m < TILE_M; ++m) {\n        for (int n = 0; n < TILE_N; ++n) {\n            int row = warp_row + m;\n            int col = warp_col + n;\n            if (row < M && col < N) {\n                C[row * N + col] = accum[m][n];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARP_SIZE, 1);\n    dim3 blocks((N + TILE_N - 1) / TILE_N, (M + TILE_M - 1) / TILE_M);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the CUDA kernel by using warp-level matrix multiplication with register tiling and cooperative fetching to maximize memory bandwidth utilization and minimize shared memory usage.}",
        "function": "#include <torch/extension.h>\n\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n#define THREADS_PER_BLOCK (WARPS_PER_BLOCK * WARP_SIZE)\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    const int warpId = threadIdx.x / WARP_SIZE;\n    const int laneId = threadIdx.x % WARP_SIZE;\n    \n    const int row = blockIdx.y * (WARPS_PER_BLOCK * TILE_SIZE) + warpId * TILE_SIZE;\n    const int col = blockIdx.x * (WARP_SIZE * TILE_SIZE) + laneId * TILE_SIZE;\n    \n    float regA[TILE_SIZE];\n    float regB[TILE_SIZE];\n    float regC[TILE_SIZE][TILE_SIZE] = {0.0f};\n    \n    for (int k = 0; k < K; k += TILE_SIZE) {\n        // Cooperative fetching of A tile\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            int a_row = row + i;\n            int a_col = k + laneId;\n            regA[i] = (a_row < M && a_col < K) ? A[a_row * K + a_col] : 0.0f;\n        }\n        \n        // Cooperative fetching of B tile\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int b_row = k + laneId;\n            int b_col = col + j;\n            regB[j] = (b_row < K && b_col < N) ? B[b_row * N + b_col] : 0.0f;\n        }\n        \n        // Warp-level matrix multiplication\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            for (int j = 0; j < TILE_SIZE; ++j) {\n                regC[i][j] += regA[i] * regB[j];\n            }\n        }\n    }\n    \n    // Store results\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int c_row = row + i;\n            int c_col = col + j;\n            if (c_row < M && c_col < N) {\n                C[c_row * N + c_col] = regC[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(THREADS_PER_BLOCK);\n    dim3 blocks((N + (WARP_SIZE * TILE_SIZE) - 1) / (WARP_SIZE * TILE_SIZE),\n                (M + (WARPS_PER_BLOCK * TILE_SIZE) - 1) / (WARPS_PER_BLOCK * TILE_SIZE));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{New implementation: Use a warp-level approach with each warp computing a 32x32 tile, leveraging warp-wide operations and register tiling to reduce shared memory usage while maintaining coalesced memory access.}",
        "function": "#include <torch/extension.h>\n\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    \n    const int row = blockIdx.y * TILE_SIZE + warp_id * (TILE_SIZE / WARPS_PER_BLOCK);\n    const int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        const int tiled_k = t * TILE_SIZE;\n        \n        float a_reg[TILE_SIZE / WARPS_PER_BLOCK];\n        float b_reg[TILE_SIZE / WARPS_PER_BLOCK];\n        \n        // Load tiles into registers\n        for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n            if (row + i < M && tiled_k + lane_id < K) {\n                a_reg[i] = A[(row + i) * K + tiled_k + lane_id];\n            } else {\n                a_reg[i] = 0.0f;\n            }\n            \n            if (col < N && tiled_k + warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + i < K) {\n                b_reg[i] = B[(tiled_k + warp_id * (TILE_SIZE / WARPS_PER_BLOCK) + i) * N + col];\n            } else {\n                b_reg[i] = 0.0f;\n            }\n        }\n        \n        // Compute partial products\n        for (int k = 0; k < TILE_SIZE / WARPS_PER_BLOCK; ++k) {\n            for (int i = 0; i < TILE_SIZE / WARPS_PER_BLOCK; ++i) {\n                value += a_reg[i] * b_reg[k];\n            }\n        }\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARP_SIZE * WARPS_PER_BLOCK, 1);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the CUDA matmul kernel by using a warp-level approach with register tiling and vectorized memory access to reduce shared memory usage and improve occupancy.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_id = threadIdx.x / WARP_SIZE;\n    int lane_id = threadIdx.x % WARP_SIZE;\n    \n    int row = blockIdx.y * TILE_SIZE + warp_id * 8;\n    int col = blockIdx.x * TILE_SIZE + lane_id;\n    \n    float value[8] = {0.0f};\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        float4 a_tile[2], b_tile;\n        \n        // Vectorized load for A\n        if (row < M && (tiled_k + lane_id * 4) < K) {\n            a_tile[0] = *reinterpret_cast<const float4*>(&A[row * K + tiled_k + lane_id * 4]);\n            a_tile[1] = *reinterpret_cast<const float4*>(&A[(row + 4) * K + tiled_k + lane_id * 4]);\n        }\n        \n        // Vectorized load for B\n        if ((tiled_k + lane_id * 4) < K && col < N) {\n            b_tile = *reinterpret_cast<const float4*>(&B[(tiled_k + lane_id * 4) * N + col]);\n        }\n        \n        // Compute partial products\n        for (int k = 0; k < 4; k++) {\n            for (int i = 0; i < 8; i++) {\n                value[i] += (i < 4 ? a_tile[0].x : a_tile[1].x) * b_tile.x;\n                if (k < 3) {\n                    a_tile[i/4].x = (i%4 == 0) ? a_tile[i/4].y : \n                                 (i%4 == 1) ? a_tile[i/4].z : a_tile[i/4].w;\n                    b_tile.x = (k == 0) ? b_tile.y : (k == 1) ? b_tile.z : b_tile.w;\n                }\n            }\n        }\n    }\n    \n    // Store results\n    if (row < M && col < N) {\n        for (int i = 0; i < 8; i++) {\n            if ((row + i) < M) {\n                C[(row + i) * N + col] = value[i];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(256);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the CUDA kernel by using a larger tile size (64x64) with double buffering in shared memory to overlap computation and memory transfers, while maintaining correctness through careful boundary checks.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    int stage = 0;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Preload next tile while computing current tile\n        int next_t = t + 1;\n        int next_tiled_k = next_t * TILE_SIZE;\n        \n        if (row < M && (next_tiled_k + threadIdx.x) < K) {\n            As[1-stage][threadIdx.y][threadIdx.x] = A[row * K + next_tiled_k + threadIdx.x];\n        } else if (next_t < (K + TILE_SIZE - 1) / TILE_SIZE) {\n            As[1-stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((next_tiled_k + threadIdx.y) < K && col < N) {\n            Bs[1-stage][threadIdx.y][threadIdx.x] = B[(next_tiled_k + threadIdx.y) * N + col];\n        } else if (next_t < (K + TILE_SIZE - 1) / TILE_SIZE) {\n            Bs[1-stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute current tile\n        if (t > 0) {\n            for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n                value += As[stage][threadIdx.y][k] * Bs[stage][k][threadIdx.x];\n                value += As[stage][threadIdx.y][k+1] * Bs[stage][k+1][threadIdx.x];\n                value += As[stage][threadIdx.y][k+2] * Bs[stage][k+2][threadIdx.x];\n                value += As[stage][threadIdx.y][k+3] * Bs[stage][k+3][threadIdx.x];\n            }\n        }\n        \n        stage = 1 - stage;\n        __syncthreads();\n    }\n    \n    // Compute last tile\n    for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n        value += As[stage][threadIdx.y][k] * Bs[stage][k][threadIdx.x];\n        value += As[stage][threadIdx.y][k+1] * Bs[stage][k+1][threadIdx.x];\n        value += As[stage][threadIdx.y][k+2] * Bs[stage][k+2][threadIdx.x];\n        value += As[stage][threadIdx.y][k+3] * Bs[stage][k+3][threadIdx.x];\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the matmul kernel by using a larger tile size (64x64) with double buffering in shared memory and more aggressive loop unrolling (factor of 8) to better utilize the RTX 4090's resources.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[2][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[2][TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float sum = 0.0f;\n    int stage = 0;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int A_col = tiledK + threadIdx.x;\n        int B_row = tiledK + threadIdx.y;\n        \n        // Double buffering: load next tile while computing current\n        if (row < M && A_col < K) {\n            As[1-stage][threadIdx.y][threadIdx.x] = A[row * K + A_col];\n        } else {\n            As[1-stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (B_row < K && col < N) {\n            Bs[1-stage][threadIdx.y][threadIdx.x] = B[B_row * N + col];\n        } else {\n            Bs[1-stage][threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute with current tile\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[stage][threadIdx.y][k] * Bs[stage][k][threadIdx.x];\n            sum += As[stage][threadIdx.y][k+1] * Bs[stage][k+1][threadIdx.x];\n            sum += As[stage][threadIdx.y][k+2] * Bs[stage][k+2][threadIdx.x];\n            sum += As[stage][threadIdx.y][k+3] * Bs[stage][k+3][threadIdx.x];\n            sum += As[stage][threadIdx.y][k+4] * Bs[stage][k+4][threadIdx.x];\n            sum += As[stage][threadIdx.y][k+5] * Bs[stage][k+5][threadIdx.x];\n            sum += As[stage][threadIdx.y][k+6] * Bs[stage][k+6][threadIdx.x];\n            sum += As[stage][threadIdx.y][k+7] * Bs[stage][k+7][threadIdx.x];\n        }\n        \n        stage = 1 - stage;\n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimized the CUDA kernel by using shared memory for tiling with increased tile size, warp-level matrix multiplication, and register blocking to reduce shared memory bank conflicts and improve memory access efficiency.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    \n    int warp_id = threadIdx.y / (WARP_SIZE / TILE_SIZE);\n    int lane_id = threadIdx.x;\n    int row = blockIdx.y * (TILE_SIZE * WARPS_PER_BLOCK) + warp_id * TILE_SIZE + threadIdx.y % (TILE_SIZE);\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float reg_A[WARPS_PER_BLOCK];\n    float reg_B[WARPS_PER_BLOCK];\n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tile from A and B into shared memory\n        if (row < M && (tiled_k + lane_id) < K) {\n            As[warp_id][threadIdx.y % TILE_SIZE][lane_id] = A[row * K + tiled_k + lane_id];\n        } else {\n            As[warp_id][threadIdx.y % TILE_SIZE][lane_id] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y % TILE_SIZE) < K && col < N) {\n            Bs[warp_id][threadIdx.y % TILE_SIZE][lane_id] = B[(tiled_k + threadIdx.y % TILE_SIZE) * N + col];\n        } else {\n            Bs[warp_id][threadIdx.y % TILE_SIZE][lane_id] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Warp-level matrix multiplication\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            reg_A[warp_id] = As[warp_id][threadIdx.y % TILE_SIZE][k];\n            reg_B[warp_id] = Bs[warp_id][k][lane_id];\n            value += reg_A[warp_id] * reg_B[warp_id];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, WARPS_PER_BLOCK * (TILE_SIZE / WARP_SIZE));\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimized the CUDA kernel by using shared memory for tiling with increased tile size, warp-level optimizations, and improved memory access patterns for better utilization of memory bandwidth.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Coalesced memory access for loading tiles\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Warp-level optimization with loop unrolling\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k++) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the CUDA matmul kernel by using register tiling, warp-level matrix operations, and increased occupancy with larger thread blocks while maintaining correctness.}",
        "function": "#include <torch/extension.h>\n\n#define BLOCK_SIZE 64\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n#define TILE_K 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warpId = threadIdx.x / WARP_SIZE;\n    int laneId = threadIdx.x % WARP_SIZE;\n    \n    int row = blockIdx.y * BLOCK_SIZE + warpId * 8;\n    int col = blockIdx.x * BLOCK_SIZE + laneId * 2;\n    \n    float accum[4][2] = {0.0f};\n    \n    for (int t = 0; t < K; t += TILE_K) {\n        float a_frag[4];\n        float b_frag[2];\n        \n        for (int k = 0; k < TILE_K; ++k) {\n            int k_idx = t + k;\n            \n            // Load A fragment\n            if (row + 3 < M && k_idx < K) {\n                a_frag[0] = A[(row) * K + k_idx];\n                a_frag[1] = A[(row+1) * K + k_idx];\n                a_frag[2] = A[(row+2) * K + k_idx];\n                a_frag[3] = A[(row+3) * K + k_idx];\n            }\n            \n            // Load B fragment\n            if (col + 1 < N && k_idx < K) {\n                b_frag[0] = B[k_idx * N + col];\n                b_frag[1] = B[k_idx * N + col+1];\n            }\n            \n            // Compute outer product\n            for (int i = 0; i < 4; ++i) {\n                for (int j = 0; j < 2; ++j) {\n                    accum[i][j] += a_frag[i] * b_frag[j];\n                }\n            }\n        }\n    }\n    \n    // Store results\n    if (row + 3 < M && col + 1 < N) {\n        for (int i = 0; i < 4; ++i) {\n            C[(row+i) * N + col] = accum[i][0];\n            C[(row+i) * N + col+1] = accum[i][1];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARP_SIZE * WARPS_PER_BLOCK, 1);\n    dim3 blocks((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (M + BLOCK_SIZE - 1) / BLOCK_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimize the CUDA kernel by using register tiling, warp-level operations, and increased thread block utilization to reduce shared memory bank conflicts and improve parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define WARPS_PER_BLOCK 4\n#define THREADS_PER_WARP 32\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    int warp_id = threadIdx.x / THREADS_PER_WARP;\n    int lane_id = threadIdx.x % THREADS_PER_WARP;\n    \n    int row = blockIdx.y * (TILE_SIZE * WARPS_PER_BLOCK) + warp_id * TILE_SIZE + lane_id;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x % TILE_SIZE;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        float a_reg = 0.0f, b_reg = 0.0f;\n        \n        if (row < M && (tiled_k + lane_id) < K) {\n            a_reg = A[row * K + tiled_k + lane_id];\n        }\n        \n        if (col < N && (tiled_k + lane_id) < K) {\n            b_reg = B[(tiled_k + lane_id) * N + col];\n        }\n        \n        for (int k = 0; k < TILE_SIZE; ++k) {\n            float a_val = __shfl_sync(0xffffffff, a_reg, k);\n            float b_val = __shfl_sync(0xffffffff, b_reg, k);\n            value += a_val * b_val;\n        }\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(THREADS_PER_WARP * WARPS_PER_BLOCK);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE * WARPS_PER_BLOCK - 1) / (TILE_SIZE * WARPS_PER_BLOCK));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the matmul kernel by using larger shared memory tiles (64x64), increased unroll factor (8), and register tiling to improve memory access efficiency and arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 8\n#define REG_TILE 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum[REG_TILE] = {0.0f};\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int A_col = tiledK + threadIdx.x;\n        int B_row = tiledK + threadIdx.y;\n\n        if (row < M && A_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            for (int r = 0; r < REG_TILE; r++) {\n                sum[r] += As[threadIdx.y][k+r] * Bs[k+r][threadIdx.x];\n            }\n        }\n\n        __syncthreads();\n    }\n\n    float final_sum = 0.0f;\n    for (int r = 0; r < REG_TILE; r++) {\n        final_sum += sum[r];\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = final_sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimized the kernel by using larger shared memory tiles (64x64), increasing thread block size to 64x64, and using 8-way loop unrolling to further reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n            value += As[threadIdx.y][k+4] * Bs[k+4][threadIdx.x];\n            value += As[threadIdx.y][k+5] * Bs[k+5][threadIdx.x];\n            value += As[threadIdx.y][k+6] * Bs[k+6][threadIdx.x];\n            value += As[threadIdx.y][k+7] * Bs[k+7][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimized the kernel by using larger shared memory tiles (64x64), increasing thread block size to 64x64, and using 8-way loop unrolling to further reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n            value += As[threadIdx.y][k+4] * Bs[k+4][threadIdx.x];\n            value += As[threadIdx.y][k+5] * Bs[k+5][threadIdx.x];\n            value += As[threadIdx.y][k+6] * Bs[k+6][threadIdx.x];\n            value += As[threadIdx.y][k+7] * Bs[k+7][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimized the CUDA kernel by using larger shared memory tiles (64x64), increased unroll factor (8), and adjusted thread block dimensions for better occupancy and memory throughput.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 8\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            #pragma unroll\n            for (int u = 0; u < UNROLL_FACTOR; u++) {\n                value += As[threadIdx.y][k+u] * Bs[k+u][threadIdx.x];\n            }\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the CUDA matmul kernel by using warp-level matrix multiplication with register tiling and cooperative fetching to maximize memory throughput and arithmetic intensity.}",
        "function": "#include <torch/extension.h>\n\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    const int warp_row = warp_id / (blockDim.x / WARP_SIZE);\n    const int warp_col = warp_id % (blockDim.x / WARP_SIZE);\n    \n    const int row = blockIdx.y * (WARPS_PER_BLOCK * TILE_SIZE) + warp_row * TILE_SIZE;\n    const int col = blockIdx.x * (WARPS_PER_BLOCK * TILE_SIZE) + warp_col * TILE_SIZE;\n    \n    float reg_A[TILE_SIZE];\n    float reg_B[TILE_SIZE];\n    float reg_C[TILE_SIZE][TILE_SIZE] = {0.0f};\n    \n    for (int t = 0; t < K; t += TILE_SIZE) {\n        // Cooperative loading of tiles into registers\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            int a_row = row + i;\n            int a_col = t + lane_id;\n            reg_A[i] = (a_row < M && a_col < K) ? A[a_row * K + a_col] : 0.0f;\n            \n            int b_row = t + lane_id;\n            int b_col = col + i;\n            reg_B[i] = (b_row < K && b_col < N) ? B[b_row * N + b_col] : 0.0f;\n        }\n        \n        // Warp-level matrix multiplication\n        for (int i = 0; i < TILE_SIZE; ++i) {\n            for (int j = 0; j < TILE_SIZE; ++j) {\n                reg_C[i][j] += reg_A[i] * reg_B[j];\n            }\n        }\n    }\n    \n    // Store results\n    for (int i = 0; i < TILE_SIZE; ++i) {\n        for (int j = 0; j < TILE_SIZE; ++j) {\n            int c_row = row + i;\n            int c_col = col + j;\n            if (c_row < M && c_col < N) {\n                C[c_row * N + c_col] = reg_C[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARPS_PER_BLOCK * WARP_SIZE);\n    dim3 blocks((N + (WARPS_PER_BLOCK * TILE_SIZE) - 1) / (WARPS_PER_BLOCK * TILE_SIZE),\n                (M + (WARPS_PER_BLOCK * TILE_SIZE) - 1) / (WARPS_PER_BLOCK * TILE_SIZE));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the CUDA kernel by using warp-level matrix multiplication with cooperative loading and vectorized memory access for improved throughput and reduced shared memory bank conflicts.}",
        "function": "#include <torch/extension.h>\n\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    const int lane_id = threadIdx.x % WARP_SIZE;\n    \n    const int row = blockIdx.y * WARPS_PER_BLOCK * TILE_SIZE + warp_id * TILE_SIZE + lane_id / (TILE_SIZE/2);\n    const int col = blockIdx.x * TILE_SIZE + (lane_id % (TILE_SIZE/2)) * 2;\n    \n    float4 value = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        const int tiled_k = t * TILE_SIZE;\n        float4 a_frag[2];\n        float4 b_frag[2];\n        \n        // Cooperative loading of tiles\n        if (row < M && (tiled_k + lane_id) < K) {\n            a_frag[0] = *reinterpret_cast<const float4*>(&A[row * K + tiled_k + lane_id]);\n            a_frag[1] = *reinterpret_cast<const float4*>(&A[row * K + tiled_k + lane_id + WARP_SIZE/2]);\n        }\n        \n        if ((tiled_k + lane_id) < K && col < N) {\n            b_frag[0] = *reinterpret_cast<const float4*>(&B[(tiled_k + lane_id) * N + col]);\n            b_frag[1] = *reinterpret_cast<const float4*>(&B[(tiled_k + lane_id + WARP_SIZE/2) * N + col]);\n        }\n        \n        // Warp-level matrix multiplication\n        for (int k = 0; k < 2; ++k) {\n            for (int l = 0; l < 2; ++l) {\n                value.x += a_frag[k].x * b_frag[l].x;\n                value.y += a_frag[k].x * b_frag[l].y;\n                value.z += a_frag[k].y * b_frag[l].x;\n                value.w += a_frag[k].y * b_frag[l].y;\n            }\n        }\n    }\n    \n    if (row < M && col < N) {\n        *reinterpret_cast<float4*>(&C[row * N + col]) = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARP_SIZE * WARPS_PER_BLOCK);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + WARPS_PER_BLOCK * TILE_SIZE - 1) / (WARPS_PER_BLOCK * TILE_SIZE));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimize the matmul kernel by using warp-level matrix multiplication with cooperative loading and reduced shared memory usage for better utilization of tensor cores on RTX 4090.}",
        "function": "#include <torch/extension.h>\n\n#define WARP_SIZE 32\n#define WARP_TILE_M 64\n#define WARP_TILE_N 64\n#define WARP_TILE_K 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    const int warp_row = blockIdx.y * WARP_TILE_M;\n    const int warp_col = blockIdx.x * WARP_TILE_N;\n    \n    const int lane_id = threadIdx.x % WARP_SIZE;\n    const int warp_id = threadIdx.x / WARP_SIZE;\n    \n    float accum[WARP_TILE_M/WARP_SIZE][WARP_TILE_N/WARP_SIZE] = {0.0f};\n    \n    for (int k_step = 0; k_step < K; k_step += WARP_TILE_K) {\n        float a_frag[WARP_TILE_M/WARP_SIZE][WARP_TILE_K];\n        float b_frag[WARP_TILE_K][WARP_TILE_N/WARP_SIZE];\n        \n        // Cooperative loading of A and B tiles\n        #pragma unroll\n        for (int i = 0; i < WARP_TILE_M/WARP_SIZE; ++i) {\n            int row = warp_row + i * WARP_SIZE + lane_id;\n            #pragma unroll\n            for (int k = 0; k < WARP_TILE_K; ++k) {\n                int col = k_step + k;\n                a_frag[i][k] = (row < M && col < K) ? A[row * K + col] : 0.0f;\n            }\n        }\n        \n        #pragma unroll\n        for (int k = 0; k < WARP_TILE_K; ++k) {\n            int row = k_step + k;\n            #pragma unroll\n            for (int j = 0; j < WARP_TILE_N/WARP_SIZE; ++j) {\n                int col = warp_col + j * WARP_SIZE + lane_id;\n                b_frag[k][j] = (row < K && col < N) ? B[row * N + col] : 0.0f;\n            }\n        }\n        \n        // Warp-level matrix multiplication\n        #pragma unroll\n        for (int i = 0; i < WARP_TILE_M/WARP_SIZE; ++i) {\n            #pragma unroll\n            for (int j = 0; j < WARP_TILE_N/WARP_SIZE; ++j) {\n                #pragma unroll\n                for (int k = 0; k < WARP_TILE_K; ++k) {\n                    accum[i][j] += a_frag[i][k] * b_frag[k][j];\n                }\n            }\n        }\n    }\n    \n    // Store results\n    #pragma unroll\n    for (int i = 0; i < WARP_TILE_M/WARP_SIZE; ++i) {\n        int row = warp_row + i * WARP_SIZE + lane_id;\n        #pragma unroll\n        for (int j = 0; j < WARP_TILE_N/WARP_SIZE; ++j) {\n            int col = warp_col + j * WARP_SIZE + lane_id;\n            if (row < M && col < N) {\n                C[row * N + col] = accum[i][j];\n            }\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(128);  // 4 warps per block\n    dim3 blocks((N + WARP_TILE_N - 1) / WARP_TILE_N, (M + WARP_TILE_M - 1) / WARP_TILE_M);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the matmul kernel by using warp-level matrix multiplication with cooperative loading and bank conflict-free shared memory access patterns.}",
        "function": "#include <torch/extension.h>\n\n#define WARPS_PER_BLOCK 4\n#define WARP_SIZE 32\n#define TILE_SIZE 16\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[WARPS_PER_BLOCK][TILE_SIZE][TILE_SIZE];\n    \n    int warp_id = threadIdx.y;\n    int lane_id = threadIdx.x;\n    \n    int row = blockIdx.y * (WARPS_PER_BLOCK * TILE_SIZE) + warp_id * TILE_SIZE + lane_id / (TILE_SIZE/2);\n    int col = blockIdx.x * TILE_SIZE + (lane_id % (TILE_SIZE/2)) * 2;\n    \n    float reg_a[TILE_SIZE] = {0.0f};\n    float reg_b[TILE_SIZE] = {0.0f};\n    float reg_c[2] = {0.0f};\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        if (row < M && tiled_k + lane_id < K) {\n            As[warp_id][lane_id / (TILE_SIZE/2)][lane_id % (TILE_SIZE/2)] = A[row * K + tiled_k + lane_id];\n        }\n        \n        if (col < N && tiled_k + lane_id < K) {\n            Bs[warp_id][lane_id / (TILE_SIZE/2)][lane_id % (TILE_SIZE/2)] = B[(tiled_k + lane_id) * N + col];\n        }\n        \n        __syncthreads();\n        \n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            reg_a[k] = As[warp_id][k][lane_id / (TILE_SIZE/2)];\n            reg_b[k] = Bs[warp_id][k][lane_id % (TILE_SIZE/2)];\n        }\n        \n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            reg_c[0] += reg_a[k] * reg_b[k];\n            reg_c[1] += reg_a[k] * reg_b[k + TILE_SIZE/2];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = reg_c[0];\n        if (col + 1 < N) {\n            C[row * N + col + 1] = reg_c[1];\n        }\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(WARP_SIZE, WARPS_PER_BLOCK);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + WARPS_PER_BLOCK * TILE_SIZE - 1) / (WARPS_PER_BLOCK * TILE_SIZE));\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the kernel by transposing matrix B for better memory coalescing, using shared memory tiling with a different tile size, and implementing a more aggressive loop unrolling strategy.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 64\n#define UNROLL_FACTOR 8\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tile from A and transposed tile from B into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.x][threadIdx.y] = B[(tiled_k + threadIdx.y) * N + col]; // Transposed load\n        } else {\n            Bs[threadIdx.x][threadIdx.y] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product with aggressive loop unrolling\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k]   * Bs[threadIdx.x][k];\n            value += As[threadIdx.y][k+1] * Bs[threadIdx.x][k+1];\n            value += As[threadIdx.y][k+2] * Bs[threadIdx.x][k+2];\n            value += As[threadIdx.y][k+3] * Bs[threadIdx.x][k+3];\n            value += As[threadIdx.y][k+4] * Bs[threadIdx.x][k+4];\n            value += As[threadIdx.y][k+5] * Bs[threadIdx.x][k+5];\n            value += As[threadIdx.y][k+6] * Bs[threadIdx.x][k+6];\n            value += As[threadIdx.y][k+7] * Bs[threadIdx.x][k+7];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": null
    }
]