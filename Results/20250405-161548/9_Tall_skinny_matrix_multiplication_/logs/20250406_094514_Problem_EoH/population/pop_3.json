[
    {
        "algorithm": "{Optimize the CUDA matmul kernel by using shared memory for tiling, loop unrolling, and increased thread block size to improve memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tiles into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product with loop unrolling\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.0080255508422855
    },
    {
        "algorithm": "{Optimize the CUDA kernel by using shared memory for tiling, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tiles into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product with loop unrolling\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.010006332397461
    },
    {
        "algorithm": "{Optimized the kernel by using shared memory for tiling, increasing thread block size, and unrolling the inner loop to reduce global memory accesses and improve memory coalescing.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tile from A and B into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if (col < N && (tiled_k + threadIdx.y) < K) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product with loop unrolling\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.011651229858399
    },
    {
        "algorithm": "{Optimized the CUDA kernel by using shared memory for tiling, loop unrolling, and increasing thread block size for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n    \n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    \n    float value = 0.0f;\n    \n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiled_k = t * TILE_SIZE;\n        \n        // Load tile from A and B into shared memory\n        if (row < M && (tiled_k + threadIdx.x) < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + tiled_k + threadIdx.x];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        if ((tiled_k + threadIdx.y) < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[(tiled_k + threadIdx.y) * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n        \n        __syncthreads();\n        \n        // Compute partial product with loop unrolling\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            value += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            value += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            value += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            value += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n        \n        __syncthreads();\n    }\n    \n    if (row < M && col < N) {\n        C[row * N + col] = value;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + threads.x - 1) / threads.x, (M + threads.y - 1) / threads.y);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.011702251434326
    },
    {
        "algorithm": "{Optimize the matmul kernel by using shared memory for tiling, loop unrolling, and increasing thread block size for better memory access patterns and parallelism.}",
        "function": "#include <torch/extension.h>\n\n#define TILE_SIZE 32\n#define UNROLL_FACTOR 4\n\n__global__ void matmul_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int K, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (K + TILE_SIZE - 1) / TILE_SIZE; ++t) {\n        int tiledK = t * TILE_SIZE;\n        int A_col = tiledK + threadIdx.x;\n        int B_row = tiledK + threadIdx.y;\n\n        if (row < M && A_col < K) {\n            As[threadIdx.y][threadIdx.x] = A[row * K + A_col];\n        } else {\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        if (B_row < K && col < N) {\n            Bs[threadIdx.y][threadIdx.x] = B[B_row * N + col];\n        } else {\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; k += UNROLL_FACTOR) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n            sum += As[threadIdx.y][k+1] * Bs[k+1][threadIdx.x];\n            sum += As[threadIdx.y][k+2] * Bs[k+2][threadIdx.x];\n            sum += As[threadIdx.y][k+3] * Bs[k+3][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < M && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\ntorch::Tensor matmul_cuda(torch::Tensor A, torch::Tensor B) {\n    auto M = A.size(0);\n    auto K = A.size(1);\n    auto N = B.size(1);\n\n    auto C = torch::zeros({M, N}, A.options());\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (M + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_kernel<<<blocks, threads>>>(A.data_ptr<float>(), B.data_ptr<float>(), C.data_ptr<float>(), M, K, N);\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &matmul_cuda, \"Matrix multiplication kernel\");\n}",
        "score": -4.027712011337281
    }
]