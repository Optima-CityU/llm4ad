[
    {
        "sample_order": 1,
        "algorithm": "{Optimize the sigmoid kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and maximizing memory throughput with proper alignment and vectorization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + __expf(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    constexpr int unroll_factor = 4;\n    int index = (blockIdx.x * blockDim.x + threadIdx.x) * unroll_factor;\n    #pragma unroll\n    for (int i = 0; i < unroll_factor; ++i) {\n        if (index + i < numel) {\n            output[index + i] = fast_sigmoid(input[index + i]);\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 2,
        "algorithm": "{Optimized the sigmoid kernel by using fast math operations while maintaining numerical precision, and improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return scalar_t(1) / (scalar_t(1) + exp(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (numel + threads - 1) / threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 3,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math approximations while maintaining accuracy, vectorizing memory accesses, and ensuring proper block/grid configuration for the RTX 4090 architecture.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    constexpr int vec_size = sizeof(float4) / sizeof(scalar_t);\n    int index = (blockIdx.x * blockDim.x + threadIdx.x) * vec_size;\n    if (index < numel) {\n        float4 in = *reinterpret_cast<const float4*>(&input[index]);\n        float4 out;\n        out.x = static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp(-in.x));\n        out.y = static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp(-in.y));\n        out.z = static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp(-in.z));\n        out.w = static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp(-in.w));\n        *reinterpret_cast<float4*>(&output[index]) = out;\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int vec_size = sizeof(float4) / sizeof(float);\n    const int blocks = (numel + threads * vec_size - 1) / (threads * vec_size);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 4,
        "algorithm": "{Optimized implementation using fast math approximation for exp() with bounds checking and improved memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t fast_exp(scalar_t x) {\n    return exp(x);\n}\n\ntemplate <>\n__device__ __forceinline__ half fast_exp(half x) {\n    return hexp(x);\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + fast_exp(-val));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 5,
        "algorithm": "{Optimize the sigmoid kernel by using fast math approximations while maintaining numerical precision, unrolling loops, and maximizing memory throughput with proper alignment and vectorization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x * 4;\n    if (index + 3 < numel) {\n        #pragma unroll\n        for (int i = 0; i < 4; ++i) {\n            const scalar_t x = input[index + i];\n            output[index + i] = __fdividef(1.0f, 1.0f + __expf(-x));\n        }\n    } else {\n        for (int i = 0; i < 4 && (index + i) < numel; ++i) {\n            const scalar_t x = input[index + i];\n            output[index + i] = __fdividef(1.0f, 1.0f + __expf(-x));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 6,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations, loop unrolling, and increased parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            scalar_t val = input[elem];\n            output[elem] = __fdividef(1.0f, 1.0f + __expf(-val));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 7,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                              scalar_t* __restrict__ output,\n                              int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 4\n    for (int i = index; i < numel; i += stride) {\n        const scalar_t x = input[i];\n        output[i] = __fdividef(1.0f, (1.0f + __expf(-x)));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (numel + threads - 1) / threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 8,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math approximations while maintaining accuracy, improving memory access patterns, and ensuring proper block/grid sizing for the RTX 4090 architecture.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads, 0, cudaStreamPerThread>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 9,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                              scalar_t* __restrict__ output,\n                              int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        const int idx = index + i * blockDim.x;\n        if (idx < numel) {\n            const scalar_t val = input[idx];\n            output[idx] = __fdividef(1.0f, 1.0f + __expf(-val));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 10,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math approximations while maintaining accuracy, unrolling loops, and ensuring proper memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + __expf(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 11,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations while maintaining numerical accuracy, improving memory access patterns, and ensuring proper block/grid sizing for the RTX 4090 architecture.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return scalar_t(1) / (scalar_t(1) + __expf(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = std::min((numel + threads - 1) / threads, 2048);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(), \n            output.data_ptr<scalar_t>(), \n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 12,
        "algorithm": "{Optimized sigmoid kernel using fast exponential approximation, loop unrolling, and increased occupancy with more threads per block.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t fast_exp(scalar_t x) {\n    x = 1.0f + x / 1024.0f;\n    x *= x; x *= x; x *= x; x *= x;\n    x *= x; x *= x; x *= x; x *= x;\n    x *= x; x *= x;\n    return x;\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            scalar_t x = input[elem];\n            scalar_t exp_val = fast_exp(-x);\n            output[elem] = static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp_val);\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 13,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations while maintaining numerical accuracy, unrolling loops, and maximizing memory throughput with proper alignment and vectorization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t x = input[i];\n        output[i] = __fdividef(1.0f, (1.0f + __expf(-x)));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 14,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations with precise computation, ensuring correctness while improving throughput via better memory access patterns and instruction-level parallelism.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = __fdividef(1.0f, 1.0f + __expf(-val));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 15,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        const int idx = index + i * blockDim.x;\n        if (idx < numel) {\n            const scalar_t val = input[idx];\n            output[idx] = __fdividef(1.0f, 1.0f + __expf(-val));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 16,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations while maintaining numerical accuracy, and improve memory access patterns with proper thread block configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <c10/cuda/CUDAMathCompat.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + c10::cuda::compat::exp(-x));\n}\n\ntemplate <>\n__forceinline__ __device__ half sigmoid(half x) {\n    return __float2half(1.0f / (1.0f + expf(-__half2float(x))));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                              scalar_t* __restrict__ output,\n                              int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 17,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations, better memory access patterns, and increased parallelism while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = __fdividef(1.0f, 1.0f + __expf(-val));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 18,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations, loop unrolling, and improved memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int64_t i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = __fdividef(1.0f, (1.0f + __expf(-val)));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (int)((numel + threads - 1) / threads));\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, cudaStreamPerThread>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 19,
        "algorithm": "{Optimized the sigmoid kernel by using fast math operations and improved memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + __expf(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 20,
        "algorithm": "{Optimized the sigmoid kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        const int idx = index + i * blockDim.x;\n        if (idx < numel) {\n            const scalar_t val = input[idx];\n            output[idx] = __fdividef(1.0f, (1.0f + __expf(-val)));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 21,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            scalar_t val = input[elem];\n            output[elem] = __fdividef(1.0f, 1.0f + __expf(-val));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 22,
        "algorithm": "{Optimized the sigmoid kernel by using fast math approximations while maintaining numerical accuracy through careful implementation of the exact sigmoid formula, and improved memory access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        scalar_t x = input[i];\n        output[i] = scalar_t(1) / (scalar_t(1) + exp(-x));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (numel + threads - 1) / threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 23,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations, loop unrolling, and increased occupancy while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            scalar_t val = input[elem];\n            output[elem] = __fdividef(1.0f, (1.0f + __expf(-val)));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 24,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations while maintaining numerical accuracy, and ensure proper memory coalescing with optimal block and thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + __expf(-x));\n}\n\ntemplate <>\n__forceinline__ __device__ half fast_sigmoid(half x) {\n    return __float2half(1.0f / (1.0f + __expf(-__half2float(x))));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads, 0, cudaStreamPerThread>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 25,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations while maintaining numerical accuracy, and ensure proper memory coalescing and thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + __expf(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 26,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations, loop unrolling, and increased occupancy while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            scalar_t val = input[elem];\n            output[elem] = __fdividef(1.0f, 1.0f + __expf(-val));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 27,
        "algorithm": "{Optimized the sigmoid kernel by using fast math operations while maintaining numerical accuracy, improved memory access patterns, and increased thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = __fdividef(1.0f, 1.0f + __expf(-val));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (numel + threads - 1) / threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 28,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations while maintaining numerical accuracy, and ensure proper memory coalescing and thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = scalar_t(1) / (scalar_t(1) + __expf(-val));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (numel + threads - 1) / threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 29,
        "algorithm": "{Optimized the sigmoid kernel by using fast math operations while maintaining numerical precision, improved memory access patterns, and increased occupancy with optimal block and thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp(-x));\n}\n\ntemplate <>\n__forceinline__ __device__ half sigmoid(half x) {\n    return __float2half(1.0f / (1.0f + __expf(-__half2float(x))));\n}\n\ntemplate <>\n__forceinline__ __device__ __nv_bfloat16 sigmoid(__nv_bfloat16 x) {\n    return __float2bfloat16(1.0f / (1.0f + __expf(-__bfloat162float(x))));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, \n        input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(), \n            output.data_ptr<scalar_t>(), \n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 30,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            scalar_t val = input[elem];\n            output[elem] = __fdividef(1.0f, (1.0f + __expf(-val)));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 31,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math approximations while maintaining numerical accuracy, improving memory access patterns, and ensuring proper block/grid sizing for the RTX 4090 architecture.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    const int grid_size = min(blocks, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t>\n            <<<grid_size, threads, 0, c10::cuda::getCurrentCUDAStream()>>>(\n                input.data_ptr<scalar_t>(), \n                output.data_ptr<scalar_t>(), \n                numel\n            );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 32,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math approximations while maintaining numerical accuracy, improving memory access patterns, and increasing occupancy with optimal block and thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp(-x));\n}\n\ntemplate <>\n__forceinline__ __device__ half fast_sigmoid(half x) {\n    return __float2half(1.0f / (1.0f + __expf(-__half2float(x))));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 33,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            scalar_t val = input[elem];\n            output[elem] = __fdividef(1.0f, 1.0f + __expf(-val));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 34,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math approximations while maintaining numerical accuracy, improving memory access patterns, and maximizing occupancy with optimal block and thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return scalar_t(1) / (scalar_t(1) + exp(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(), \n            output.data_ptr<scalar_t>(), \n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 35,
        "algorithm": "{Optimize the sigmoid kernel by using fast math approximations while maintaining numerical accuracy, unrolling loops, and maximizing memory throughput with proper memory alignment and access patterns.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + __expf(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        const int idx = index + i * blockDim.x;\n        if (idx < numel) {\n            output[idx] = fast_sigmoid(input[idx]);\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(), \n            output.data_ptr<scalar_t>(), \n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 36,
        "algorithm": "{Optimized the sigmoid CUDA kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 4\n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = scalar_t(1) / (scalar_t(1) + __expf(-val));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 37,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations while maintaining numerical precision, unrolling loops, and using optimal block/thread configurations for RTX 4090.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + __expf(-x));\n}\n\ntemplate <>\n__forceinline__ __device__ half fast_sigmoid(half x) {\n    return __float2half(1.0f / (1.0f + __expf(-__half2float(x))));\n}\n\ntemplate <typename scalar_t, int UNROLL_FACTOR=4>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    int index = blockIdx.x * blockDim.x * UNROLL_FACTOR + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < UNROLL_FACTOR; ++i) {\n        int current_idx = index + i * blockDim.x;\n        if (current_idx < numel) {\n            output[current_idx] = fast_sigmoid(input[current_idx]);\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads>>>(\n            input.data_ptr<scalar_t>(), \n            output.data_ptr<scalar_t>(), \n            numel\n        );\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 38,
        "algorithm": "{Optimized the sigmoid kernel by using fast math operations while maintaining numerical accuracy, improved memory access patterns, and increased thread utilization.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + __expf(-x));\n}\n\ntemplate <>\n__forceinline__ __device__ half fast_sigmoid(half x) {\n    return __float2half(1.0f / (1.0f + __expf(-__half2float(x))));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                              scalar_t* __restrict__ output,\n                              int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 39,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations while maintaining numerical accuracy, and ensure proper memory coalescing with optimal block and thread configuration.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = scalar_t(1) / (scalar_t(1) + exp(-val));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 40,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations while maintaining numerical accuracy, unrolling loops, and ensuring coalesced memory access.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = __fdividef(1.0f, (1.0f + __expf(-val)));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 41,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math approximations, loop unrolling, and increased parallelism while maintaining numerical correctness through careful implementation.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + exp(-x));\n}\n\ntemplate <>\n__device__ __forceinline__ half fast_sigmoid(half x) {\n    return __float2half(1.0f / (1.0f + expf(-__half2float(x))));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                              scalar_t* __restrict__ output,\n                              int64_t numel) {\n    const int stride = blockDim.x * gridDim.x;\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    \n    #pragma unroll 4\n    for (; index < numel; index += stride) {\n        output[index] = fast_sigmoid(input[index]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 42,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math approximations while maintaining numerical accuracy, improving memory access patterns, and ensuring proper kernel launch configuration for the RTX 4090 architecture.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + __expf(-x));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 43,
        "algorithm": "{Optimized the sigmoid kernel by using fast math operations, loop unrolling, and increased occupancy with optimal block/thread configuration while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            scalar_t x = input[elem];\n            output[elem] = __fdividef(1.0f, 1.0f + __expf(-x));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 44,
        "algorithm": "{Optimize the sigmoid kernel by using fast math approximations while maintaining numerical accuracy, and improve memory access patterns with vectorized loads/stores.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    constexpr int VEC_SIZE = 4;\n    int index = (blockIdx.x * blockDim.x + threadIdx.x) * VEC_SIZE;\n    \n    if (index < numel) {\n        #pragma unroll\n        for (int i = 0; i < VEC_SIZE && (index + i) < numel; ++i) {\n            scalar_t val = input[index + i];\n            output[index + i] = __fdividef(1.0f, (1.0f + __expf(-val)));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 45,
        "algorithm": "{Optimize the sigmoid kernel by using fast math operations while maintaining numerical precision, increasing thread block size, and minimizing memory access overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = __fdividef(1.0f, (1.0f + __expf(-val)));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 1024;\n    const int blocks = std::min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 46,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations while maintaining numerical accuracy, unrolling loops, and ensuring memory coalescing.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            scalar_t val = input[elem];\n            output[elem] = __fdividef(1.0f, 1.0f + __expf(-val));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 47,
        "algorithm": "{Optimized the sigmoid kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 4\n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = __fdividef(1.0f, (1.0f + __expf(-val)));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 48,
        "algorithm": "{Optimized sigmoid computation using fast exponential approximation and improved memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_exp(scalar_t x) {\n    x = 1.0 + x / 1024.0;\n    x *= x; x *= x; x *= x; x *= x;\n    x *= x; x *= x; x *= x; x *= x;\n    x *= x; x *= x;\n    return x;\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        scalar_t x = input[i];\n        scalar_t exp_val = exp(-abs(x));\n        scalar_t sigmoid = (x >= 0) ? (1.0 / (1.0 + exp_val)) : (exp_val / (1.0 + exp_val));\n        output[i] = sigmoid;\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (numel + threads - 1) / threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0, cudaStreamPerThread>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 49,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 4\n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = __fdividef(1.0f, 1.0f + __expf(-val));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads - 1) / threads;\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 50,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        const int idx = index + i * blockDim.x;\n        if (idx < numel) {\n            const scalar_t val = input[idx];\n            output[idx] = __fdividef(1.0f, 1.0f + __expf(-val));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 51,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 4\n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = __fdividef(1.0f, (1.0f + __expf(-val)));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 52,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    #pragma unroll 4\n    for (int i = index; i < numel; i += stride) {\n        const scalar_t val = input[i];\n        output[i] = __fdividef(1.0f, 1.0f + __expf(-val));\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min(65535, (numel + threads - 1) / threads);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 53,
        "algorithm": "{Optimized the sigmoid kernel by using fast math operations while maintaining numerical precision, increasing thread utilization, and minimizing memory access overhead.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n#include <cuda_bf16.h>\n\ntemplate <typename scalar_t>\n__forceinline__ __device__ scalar_t fast_sigmoid(scalar_t x) {\n    return static_cast<scalar_t>(1) / (static_cast<scalar_t>(1) + __expf(-x));\n}\n\ntemplate <>\n__forceinline__ __device__ half fast_sigmoid(half x) {\n    return __float2half(1.0f / (1.0f + __expf(-__half2float(x))));\n}\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_optimized(const scalar_t* __restrict__ input,\n                                        scalar_t* __restrict__ output,\n                                        int64_t numel) {\n    const int index = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    \n    for (int i = index; i < numel; i += stride) {\n        output[i] = fast_sigmoid(input[i]);\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = min((numel + threads - 1) / threads, 1024);\n    \n    AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, \n        input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel_optimized<scalar_t><<<blocks, threads, 0, cudaStreamPerThread>>>(\n            input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    },
    {
        "sample_order": 54,
        "algorithm": "{Optimize the sigmoid CUDA kernel by using fast math operations, loop unrolling, and better memory access patterns while maintaining numerical accuracy.}",
        "function": "#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               int64_t numel) {\n    const int index = blockIdx.x * blockDim.x * 4 + threadIdx.x;\n    #pragma unroll\n    for (int i = 0; i < 4; ++i) {\n        int elem = index + i * blockDim.x;\n        if (elem < numel) {\n            scalar_t val = input[elem];\n            output[elem] = __fdividef(1.0f, 1.0f + __expf(-val));\n        }\n    }\n}\n\ntorch::Tensor sigmoid_cuda(torch::Tensor input) {\n    input = input.contiguous();\n    auto output = torch::empty_like(input);\n    int64_t numel = input.numel();\n    \n    const int threads = 256;\n    const int blocks = (numel + threads * 4 - 1) / (threads * 4);\n    \n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"sigmoid_cuda\", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(input.data_ptr<scalar_t>(), output.data_ptr<scalar_t>(), numel);\n    }));\n    \n    return output;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &sigmoid_cuda, \"Apply Sigmoid activation (CUDA)\");\n}",
        "score": null
    }
]